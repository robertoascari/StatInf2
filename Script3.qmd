---
title: "Script 3 - MCMC - Gibbs sampling"
author: "Roberto Ascari"
format: html
editor: visual
toc: true
---

In this section, we show two Gibbs samplers: the first one aims to estimate the parameters $(\mu, \sigma^2)$ of a Normal likelihood, whereas the second one estimates the parameters of a linear regression model.

# Gibbs Sampling for the Mean and Variance of a Normal Distribution

We consider a scenario characterized where $Y| \mu, \sigma^2 \sim N(\mu, \sigma^2)$. Furthermore, we impose **prior independence**, meaning that $\pi(\mu, \sigma^2) = \pi(\mu)\pi(\sigma^2)$. Then, we specify the following priors:

- $\mu \sim N(\mu_0, \sigma^2_0)$;

- $\sigma^2 \sim Inv.Gamma(a_0, b_0)$.

In a simulation framework, we need to specify the true values of the parameters generating the data.

```{r}
#| echo: true
#| eval: true

mu.true <- 10
sigma2.true <- 5
n <- 100
```

Then, we can generate a sample...

```{r}
#| echo: true
#| eval: true

set.seed(42)
y <- rnorm(n, mu.true, sqrt(sigma2.true))
ybar <- mean(y); ybar
```

... and set hyperparameters:

```{r}
#| echo: true
#| eval: true
#| 
mu0 <- 0
sigma2.0 <- 1000

alpha <- 10
beta <- 10
```

The prior expectation is

```{r}
#| echo: true
#| eval: true

beta/(alpha-1)
```

and the prior variance is:

```{r}
#| echo: true
#| eval: true

(beta^2)/((alpha-1)*(alpha-2))
```

## Initialization:

We initialize each element by drawing a value from the corresponding prior:

```{r}
#| echo: true
#| eval: true

B <- 1000
mu.chain <- numeric(B)
sigma2.chain <- numeric(B)

set.seed(42)
mu.chain[1] <- rnorm(1, mu0, sqrt(sigma2.0))
sigma2.chain[1] <- 1/rgamma(1, alpha, rate = beta)
```

Then, we can implement the Gibbs sampling by generating each parameter from its full-conditional distribution. The full-conditionals are the following:

- $\mu| \sigma^2, \textbf{y} \sim N\left(\frac{n\sigma^2_0 \bar{y} + \sigma^2 \mu_0}{n\sigma^2_0 + \sigma^2}, \frac{\sigma^2 \sigma^2_0}{n\sigma^2_0 + \sigma^2}\right)$;

- $\sigma^2| \mu, \textbf{y} \sim Inv.Gamma\left(a_0 + \frac{n}{2}, b_0 + \frac{\sum_{i=1}^n (y_i - \mu)^2}{2}\right)$.

```{r}
#| echo: true
#| eval: true

set.seed(42)
for(b in 2:B){
  # Draw mu from the F.C.
  mu.chain[b] <-
    rnorm(1, 
          (n*ybar*sigma2.0 + sigma2.chain[b-1])/(n*sigma2.0+sigma2.chain[b-1]),
          sqrt((sigma2.chain[b-1]*sigma2.0)/(n*sigma2.0+sigma2.chain[b-1])))
  
  # Draw sigma2 from the F.C.
  sigma2.chain[b] <-
    1/rgamma(1, alpha + n/2,
             rate = beta + .5*sum((y-mu.chain[b])^2))
  
}
```

```{r}
#| echo: true
#| eval: true

plot(mu.chain, sigma2.chain, pch=20)
points(mu.true, sigma2.true, col="#D55E00", pch=20)
```

We can now generate some diagnostic plots.

```{r}
#| echo: true
#| eval: true

# Traceplots:
par(mfrow=c(2,1))
plot(mu.chain[-1], pch=20, type="l")
plot(sigma2.chain[-1], pch=20, type="l")
par(mfrow=c(1,1))
```

```{r}
#| echo: true
#| eval: true

mu.means <- numeric(B)
sigma2.means <- numeric(B)

mu.means[1] <- mu.chain[1]
sigma2.means[1] <- sigma2.chain[1]

for(b in 2:B){
  mu.means[b] <- mean(mu.chain[2:b])
  sigma2.means[b] <- mean(sigma2.chain[2:b])
}

plot(mu.means[-1], pch=20, type="l", ylim=c(9.2, 10.8))
abline(h=mu.true, col="#D55E00")
```


```{r}
#| echo: true
#| eval: true

plot(sigma2.means[-1], pch=20, type="l", ylim=c(3,7))
abline(h=sigma2.true, col="#D55E00")
```




```{r}
#| echo: true
#| eval: true

par(mfrow=c(2,1))
acf(mu.chain)
acf(sigma2.chain)
par(mfrow=c(1,1))
```

Once we have a chain for each parameter, we need to remove the warm-up (i.e., the part of the chains for which we cannot assume the convergence to the stationary distribution).

```{r}
#| echo: true
#| eval: true

warm_perc <- .5

mu.new <- mu.chain[round(B*warm_perc+1):B]
sigma2.new <- sigma2.chain[round(B*warm_perc+1):B]

par(mfrow=c(2,1))
plot(mu.new, pch=20, type="l", ylim=c(9.2, 10.8))
abline(h=mu.true, col="#D55E00")

plot(sigma2.new, pch=20, type="l", ylim=c(3,7))
abline(h=sigma2.true, col="#D55E00")

par(mfrow=c(1,1))
```

The new vectors can be used to compute estimates, CSs, and probabilities:

```{r}
#| echo: true
#| eval: true

mean(mu.new)
quantile(mu.new, probs = c(.025, .975))
```

```{r}
#| echo: true
#| eval: true

mean(sigma2.new)
quantile(sigma2.new, probs = c(.025, .975))
```

```{r}
#| echo: true
#| eval: true

mean(mu.new > 10)
```

```{r}
#| echo: true
#| eval: true

hist(mu.new, prob=T, xlab=expression(mu),
     main="Posterior distribution of mu")
lines(density(mu.new), col="#D55E00")
abline(v=mu.true, col="black", lty="dashed")
```

```{r}
#| echo: true
#| eval: true

hist(sigma2.new, prob=T, xlab=expression(sigma2),
     main="Posterior distribution of sigma2")
lines(density(sigma2.new), col="#D55E00")
abline(v=sigma2.true, col="black", lty="dashed")
```

```{r}
#| echo: true
#| eval: true

plot(mu.new, sigma2.new, pch = 20)
```

We can define a function to fit this Gibbs sampler more easily on new data.

```{r}
#| echo: true
#| eval: true


normal_GS <- function(y, B = 5000, 
                      mu0 = 0, sigma2.0 = 1000, 
                      alpha = 10, beta = 10, 
                      warm_perc = .5, seed=42){
  
  mu.chain <- numeric(B)
  sigma2.chain <- numeric(B)
  ybar <- mean(y)
  n <- length(y)
  
  # Initialization:
  set.seed(seed)
  mu.chain[1] <- rnorm(1, mu0, sqrt(sigma2.0))
  sigma2.chain[1] <- 1/rgamma(1, alpha, rate = beta)
  
  for(b in 2:B){
    # Draw mu from the F.C.
    mu.chain[b] <-
      rnorm(1,
            (n*ybar*sigma2.0 + sigma2.chain[b-1])/(n*sigma2.0+sigma2.chain[b-1]),
            sqrt((sigma2.chain[b-1]*sigma2.0)/(n*sigma2.0+sigma2.chain[b-1])))
    
    # Draw sigma2 from the F.C.
    sigma2.chain[b] <-
      1/rgamma(1, alpha + n/2,
               rate = beta + .5*sum((y-mu.chain[b])^2))
  }
  
  mu.new <- mu.chain[round(B*warm_perc+1):B]
  sigma2.new <- sigma2.chain[round(B*warm_perc+1):B]
  
  return(cbind(mu.chain = mu.new, sigma2.chain = sigma2.new))
}
```

## Gala dataset (I)

```{r}
#| echo: true
#| eval: true

library(faraway)
data(gala)
str(gala)
help(gala)
```

```{r}
#| echo: true
#| eval: true

y <- gala$Species
gala_GS <- normal_GS(y, B = 10000)

str(gala_GS)
head(gala_GS)
```

```{r}
#| echo: true
#| eval: true

plot(gala_GS, pch = 20)

hist(gala_GS[,1], prob = T, 
     main="Posterior distribution of mu")
lines(density(gala_GS[,1]), col="#D55E00")

hist(gala_GS[,2], prob = T, 
     main="Posterior distribution of sigma2")
lines(density(gala_GS[,2]), col="#D55E00")

colMeans(gala_GS)
t(apply(gala_GS, 2, function(x) quantile(x, probs=c(.025, .975))))
```

```{r}
#| echo: true
#| eval: true

mean(gala_GS[,1] > 70)
mean(gala_GS[,1] > 70 & gala_GS[,2] < 5500)
```

# Gibbs Sampling for the parameters of a Linear Regression Model

```{r}
#| echo: true
#| eval: true

rm(list=ls())

# True values:
beta <- c(-2,5,3)
sigma2 <- 6
```

```{r}
#| echo: true
#| eval: true

# Generating data:
n <- 100
set.seed(42)
X <- matrix(rnorm(2*n, 0, 50), ncol=2)
X <- cbind(rep(1,n), X)

y <- as.numeric(X%*%beta + rnorm(n, 0, sqrt(sigma2)))
```

Classical OLS/ML estimates:

```{r}
#| echo: true
#| eval: true

summ <- summary(lm(y~X[,2]+X[,3])); summ
```

```{r}
#| echo: true
#| eval: true

beta0 <- rep(0,3)
Sigma0 <- 100*diag(3)

a0 <- 10
b0 <- 10
```

```{r}
#| echo: true
#| eval: true

# Prior Expectation for sigma2
b0/(a0-1)
# Prior Variance for sigma2
(b0^2)/((a0-1)*(a0-2))
```

```{r}
#| echo: true
#| eval: true

B <- 5000
beta.chain <- matrix(NA, ncol=3, nrow=B)
sigma2.chain <- numeric(B)

beta.chain[1,] <- rep(0,3)
sigma2.chain[1] <- 1
```

Gibbs sampling:

```{r}
#| echo: true
#| eval: true
library(MASS)

for(b in 2:B){
  
  Sigma.n <- solve(solve(Sigma0) + (t(X)%*%X)/sigma2.chain[b-1])
  beta.n <- Sigma.n %*% ((solve(Sigma0)%*%beta0) + (t(X)%*%y)/sigma2.chain[b-1])
  
  beta.chain[b,] <- mvrnorm(n=1, mu=beta.n, Sigma=Sigma.n)
  
  sigma2.chain[b] <- 
    1/rgamma(1, a0 + .5*n,
             rate = b0 + 
               0.5*(t(y-X%*%beta.chain[b,])%*%(y-X%*%beta.chain[b,])))
}
```

Diagnostic plots:

```{r}
#| echo: true
#| eval: true

# Traceplots:
par(mfrow=c(2,2))
plot(beta.chain[,1], pch=20, type="l");abline(h=beta[1], col="#D55E00")
plot(beta.chain[,2], pch=20, type="l");abline(h=beta[2], col="#D55E00")
plot(beta.chain[,3], pch=20, type="l");abline(h=beta[3], col="#D55E00")
plot(sigma2.chain, pch=20, type="l");abline(h=sigma2, col="#D55E00")
par(mfrow=c(1,1))

par(mfrow=c(2,2))
plot(beta.chain[-1,1], pch=20, type="l");abline(h=beta[1], col="#D55E00")
plot(beta.chain[-1,2], pch=20, type="l");abline(h=beta[2], col="#D55E00")
plot(beta.chain[-1,3], pch=20, type="l");abline(h=beta[3], col="#D55E00")
plot(sigma2.chain[-1], pch=20, type="l");abline(h=sigma2, col="#D55E00")
par(mfrow=c(1,1))
```

```{r}
#| echo: true
#| eval: true

beta_mean <- matrix(NA, ncol = 3, nrow = nrow(beta.chain))
sigma2.means <- numeric(length(sigma2.chain))

beta_mean[1,] <- beta.chain[1,]
sigma2.means[1] <- sigma2.chain[1]

for(b in 2:nrow(beta.chain)){
  beta_mean[b,1] <- mean(beta.chain[2:b,1])
  beta_mean[b,2] <- mean(beta.chain[2:b,2])
  beta_mean[b,3] <- mean(beta.chain[2:b,3])
  sigma2.means[b] <- mean(sigma2.chain[2:b])
}

par(mfrow=c(2,2))
plot(beta_mean[-1,1], pch=20, type="l")
plot(beta_mean[-1,2], pch=20, type="l")
plot(beta_mean[-1,3], pch=20, type="l")
plot(sigma2.means[-1], pch=20, type="l")
par(mfrow=c(1,1))

acf(beta_mean[,1])
acf(beta_mean[,2])
acf(beta_mean[,3])
acf(sigma2.means)
```

```{r}
#| echo: true
#| eval: true

# Removing the warmu-up:
warm_perc <- .5

beta.new <- beta.chain[round(B*warm_perc+1):B,]
sigma2.new <- sigma2.chain[round(B*warm_perc+1):B]

# Computing estimates:
colMeans(beta.new)
mean(sigma2.new)
```

```{r}
#| echo: true
#| eval: true

# Histogram and kernel estimate:
par(mfrow=c(2,2))
hist(beta.new[,1], prob=T, xlab=expression(beta0),
     main="Posterior distribution of beta0")
lines(density(beta.new[,1]), col="#D55E00")
##############################################
hist(beta.new[,2], prob=T, xlab=expression(beta1),
     main="Posterior distribution of beta1")
lines(density(beta.new[,2]), col="#D55E00")
##############################################
hist(beta.new[,3], prob=T, xlab=expression(beta2),
     main="Posterior distribution of beta2")
lines(density(beta.new[,3]), col="#D55E00")
##############################################
hist(sigma2.new, prob=T, xlab=expression(sigma2),
     main="Posterior distribution of sigma2")
lines(density(sigma2.new), col="#D55E00")
par(mfrow=c(1,1))
```

```{r}
#| echo: true
#| eval: true

round(var(beta.new), 5)
round((2.483^2)*summ$cov.unscaled,5)
```

Defining a function:

```{r}
#| echo: true
#| eval: true

rm(list=ls())

LM_GS <- function(y, X, B = 5000, 
                  beta0 = rep(0, ncol(X)), 
                  Sigma0 = diag(ncol(X)), 
                  a0 = 10, b0 = 10, warm_perc = .5, seed=42){
  
  beta.chain <- matrix(NA, ncol=ncol(X), nrow=B)
  sigma2.chain <- numeric(B)
  n <- length(y)
  
  # Initialization:
  beta.chain[1,] <- rep(0, ncol(X))
  sigma2.chain[1] <- 1
  
  library(MASS)
  for(b in 2:B){
    Sigma.n <- solve(solve(Sigma0) + (t(X)%*%X)/sigma2.chain[b-1])
    beta.n <- Sigma.n %*% ((solve(Sigma0)%*%beta0) + (t(X)%*%y)/sigma2.chain[b-1])
    
    beta.chain[b,] <- mvrnorm(n=1, mu=beta.n, Sigma=Sigma.n)
    
    sigma2.chain[b] <- 
      1/rgamma(1, a0 + .5*n,
               rate = b0 + 
                 0.5*(t(y-X%*%beta.chain[b,])%*%(y-X%*%beta.chain[b,])))
  }
  
  beta.new <- beta.chain[round(B*warm_perc+1):B,]
  sigma2.new <- sigma2.chain[round(B*warm_perc+1):B]
  
  return(list(beta = beta.new, sigma2 = sigma2.new))
}
```

## Gala dataset (II)

```{r}
#| echo: true
#| eval: true

# Definition of y and X:
y <- gala$Species
X <- model.matrix(Species ~ ., data = gala[,-2])

# Fitting the model:
lm_gala <- LM_GS(y, X)
str(lm_gala)
```

```{r}
#| echo: true
#| eval: true

# Extracting the elements of the chain:
betas <- lm_gala$beta
colnames(betas) <- colnames(X)
sigma2 <- lm_gala$sigma2

colMeans(betas)
```

```{r}
#| echo: true
#| eval: true

t(apply(betas, 2, function(x) quantile(x, probs=c(.025, .975))))
```

```{r}
#| echo: true
#| eval: true

mean(sigma2)
```

```{r}
#| echo: true
#| eval: true

cov(betas)

```
