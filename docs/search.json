[
  {
    "objectID": "Script5.html",
    "href": "Script5.html",
    "title": "Script 5 - MCMC - Stan",
    "section": "",
    "text": "You can download the R script here.\nSome useful links for working with Stan:\n\nList of Language-Specific Stan Interfaces.\nRstan: an interface to Stan for R.\nRStan: Getting Started.\n\n\nLinear Regression\nIn this part, we are going to fit a simple linear regression model through Stan. Here you can download the .stan model.\nFirst of all, let’s compile the model and save it into an R object.\n\nLM_Model &lt;- rstan::stan_model(file=\"data/LM_Model.stan\")\n\nLM_Model\n\nS4 class stanmodel 'anon_model' coded as follows:\ndata{\n    int&lt;lower = 0&gt; n;  // number of obs\n    int&lt;lower = 0&gt; K;  // number of covariates (including the intercept)\n\n    vector[n] y;     // response\n    matrix[n, K] X;  // covariates\n  real a0;\n  real b0;\n    vector[K] beta0;\n    vector[K] s2_0;\n}\nparameters{\n    real&lt;lower = 0&gt; sigma2;\n    vector[K] beta;\n}\ntransformed parameters {\n    vector[n] mu;\n  mu = X * beta;\n}\nmodel{\n    // Prior:\n    sigma2 ~ inv_gamma(a0, b0);\n\n    for(k in 1:K){\n        beta[k] ~ normal(beta0[k], sqrt(s2_0[k]));\n    }\n    // Likelihood:\n    y ~ normal(mu, sqrt(sigma2));\n}\ngenerated quantities{\n\n    vector[n] log_lik;\n\n    for (j in 1:n){\n            log_lik[j] = normal_lpdf(y[j] | mu[j], sqrt(sigma2));\n    }\n} \n\n\nWe consider the Prestige data from the car package. Statistical units are occupations, for which we have:\n\neducation: average education of occupational incumbents, years, in 1971;\nincome: average income of incumbents, dollars, in 1971;\nwomen: percentage of incumbents who are women;\nprestige: Pineo-Porter prestige score for occupation, from a social survey conducted in the mid-1960s;\ncensus: canadian Census occupational code;\ntype: type of occupation (bc, Blue Collar; prof, Professional, Managerial, and Technical; wc, White Collar).\n\nLet’s transform the income variable so to reduce the asimmetry.\n\ndata(\"Prestige\")\n\nPrestige2 &lt;-\n  Prestige[complete.cases(Prestige),-5]\nPrestige2$income &lt;- log(Prestige2$income)\n\nIn order to fit the stan model, we need to pass some data: - y: the response vector;\n\nX: the design matrix;\nn: the sample size;\nK: the number of covariates (including the intercept term);\na0, b0, beta0, s2_0: hyperparameters.\n\n\ny &lt;- Prestige2$prestige\nmod &lt;- lm(prestige ~ ., data=Prestige2)\n\nX &lt;- model.matrix(mod)\nn &lt;- nrow(X)\nK &lt;- ncol(X)\n\nWe further create a list containing the objects to be passed to the stan model.\n\ndata.stan &lt;- list(\n  y = y,\n  X = X,\n  n = n,\n  K = K,\n  a0 = 10,\n  b0 = 10,\n  beta0 = rep(0, K),\n  s2_0 = rep(10, K)\n) \n\nWe can now use the rstan::sampling function to fit the model. We what two chains, each of length 10’000, and we want to discard the first 50% as warm-up.\n\nn.iter &lt;- 10000\nnchain &lt;- 2\n\nLM_stan &lt;- rstan::sampling(\n  object = LM_Model,\n  data = data.stan,\n  warmup = 0.5*n.iter, \n  iter = n.iter,\n  thin=1, chains = nchain,\n  refresh = n.iter/2     \n  #, pars=c(\"beta\", \"sigma2\")\n  )\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 5.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.52 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 1.157 seconds (Warm-up)\nChain 1:                1.418 seconds (Sampling)\nChain 1:                2.575 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1.131 seconds (Warm-up)\nChain 2:                1.311 seconds (Sampling)\nChain 2:                2.442 seconds (Total)\nChain 2: \n\n\n\nsumm &lt;- summary(LM_stan, pars = c(\"beta\", \"sigma2\"))$summary\n\nrownames(summ)[1:K] &lt;- colnames(X)\nround(summ, 3)\n\n              mean se_mean    sd   2.5%    25%    50%    75%  97.5%    n_eff\n(Intercept) -3.027   0.042 3.128 -9.125 -5.126 -3.035 -0.927  3.184 5506.887\neducation    4.306   0.007 0.455  3.407  4.000  4.309  4.614  5.202 4053.221\nincome       0.516   0.010 0.616 -0.703  0.106  0.515  0.930  1.730 4195.269\nwomen       -0.060   0.000 0.025 -0.110 -0.076 -0.060 -0.044 -0.012 6432.375\ntypeprof     6.088   0.034 2.259  1.613  4.567  6.087  7.604 10.480 4524.194\ntypewc      -2.795   0.023 1.772 -6.306 -3.972 -2.773 -1.573  0.607 5931.683\nsigma2      48.835   0.084 6.804 37.377 44.022 48.179 52.888 63.836 6553.391\n            Rhat\n(Intercept)    1\neducation      1\nincome         1\nwomen          1\ntypeprof       1\ntypewc         1\nsigma2         1\n\n\nTraceplots:\n\nrstan::traceplot(LM_stan, pars = c('beta', 'sigma2'), inc_warmup = TRUE)\n\n\n\n\n\n\n\nrstan::traceplot(LM_stan, pars = c('beta', 'sigma2'), inc_warmup = FALSE)\n\n\n\n\n\n\n\n\n\npost_chain &lt;- rstan::extract(LM_stan, c('beta', 'sigma2')) \nbetas &lt;- post_chain$beta\ncolnames(betas) &lt;- colnames(X)\n\nsigma2 &lt;- post_chain$sigma2\n\ndf_plot &lt;- \n  data.frame(value = c(\n    betas[,1], betas[,2], betas[,3],\n    betas[,4], betas[,5], betas[,6],\n    sigma2), \n    param = rep(c(colnames(X), 'sigma2'), \n                each = nrow(post_chain$beta)))\n\n\ndf_plot %&gt;% \n  ggplot(aes(value, fill = param)) + \n  geom_density() + \n  facet_wrap(~param, scales = 'free') + \n  theme_minimal() + \n  theme(legend.position=\"false\")\n\n\n\n\n\n\n\n\n\nacf(betas[,1])\n\n\n\n\n\n\n\nacf(betas[,2])\n\n\n\n\n\n\n\nacf(betas[,3])\n\n\n\n\n\n\n\nacf(betas[,4])\n\n\n\n\n\n\n\nacf(betas[,5])\n\n\n\n\n\n\n\nacf(betas[,5])\nacf(betas[,6])\n\n\n\n\n\n\n\nacf(sigma2)\n\n\n\n\n\n\n\n\nAs classical MCMCs, we can use the sampled values to approximate any quantity of interest. For example, we can approximate \\(\\mathbb{P}(\\beta_1 &gt; 4, \\beta_3 &gt; -0.5)\\):\n\nmean(betas[,2] &gt; 4 & betas[,4] &gt; -.5)\n\n[1] 0.7504\n\n\n\n\nLogistic Regression\nLet’s consider the logistic regression model to the cardiac dataset.\nThe Logistic_Model.stan model can be downloaded here.\n\nrm(list = ls())\ncardiac &lt;- read.csv(\"data/cardiac.csv\", header=T, sep=\";\")\n\ny &lt;- cardiac$Chd\nX &lt;- model.matrix(lm(Chd ~ ., data=cardiac))\nn &lt;- nrow(X)\nK &lt;- ncol(X)\n\n\nLogistic_Model &lt;-\n  rstan::stan_model(file=\"data/Logistic_Model.stan\")\n\nLogistic_Model\n\nS4 class stanmodel 'anon_model' coded as follows:\n///////////////////////// DATA /////////////////////////////////////\ndata {\n    int&lt;lower = 1&gt; n;       // number of data\n    int&lt;lower = 1&gt; K;       // number of covariates (including the intercept)\n    int&lt;lower = 0, upper = 1&gt; y[n];   // response vector\n    matrix[n, K] X;           // design matrix\n\n  vector[K] beta0;\n  vector[K] s2_0;\n}\n//////////////////// PARAMETERS /////////////////////////////////\nparameters {\n    vector[K] beta;        // regression coefficients\n}\ntransformed parameters {\n    vector[n] eta;\n    vector&lt;lower=0&gt;[n] Odds;\n    vector&lt;lower=0,upper=1&gt;[n] p;\n\n  eta = X * beta;\n\n  for(i in 1:n){\n    Odds[i] = exp(eta[i]);\n    p[i] = Odds[i]/(1+Odds[i]);\n  }\n\n}\n////////////////// MODEL ////////////////////////\nmodel {\n  // Prior\n    for (j in 1:K){\n        beta[j] ~ normal(beta0, sqrt(s2_0));\n    }\n\n  // Likelihood\n    for (s in 1:n){\n        y[s] ~ bernoulli(p[s]);\n    }\n} \n\n\n\ndata.stan_logis &lt;- list(\n  y = y,\n  X = X,\n  n = n,\n  K = K,\n  beta0 = rep(0, K),\n  s2_0 = rep(10, K)\n) \n\n\nn.iter &lt;- 10000\nnchain &lt;- 2\n\nLogistic_stan &lt;- rstan::sampling(\n  object = Logistic_Model,\n  data = data.stan_logis,\n  warmup = 0.5*n.iter, iter = n.iter,\n  thin = 1, chains = nchain,\n  refresh = n.iter/2\n)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 7.9e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.79 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 2.16 seconds (Warm-up)\nChain 1:                1.578 seconds (Sampling)\nChain 1:                3.738 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 3.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.32 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1.977 seconds (Warm-up)\nChain 2:                1.742 seconds (Sampling)\nChain 2:                3.719 seconds (Total)\nChain 2: \n\n\n\nsumm &lt;- summary(Logistic_stan, pars = \"beta\")$summary\nrownames(summ)[1:K] &lt;- colnames(X)\nround(summ, 3)\n\n              mean se_mean    sd   2.5%    25%    50%    75%  97.5%    n_eff\n(Intercept) -4.446   0.024 0.939 -6.323 -5.067 -4.431 -3.787 -2.704 1467.734\nAge          0.093   0.001 0.020  0.056  0.079  0.093  0.107  0.133 1446.189\n             Rhat\n(Intercept) 1.000\nAge         1.001\n\n\n\nrstan::traceplot(Logistic_stan, pars = \"beta\", inc_warmup = TRUE)\n\n\n\n\n\n\n\nrstan::traceplot(Logistic_stan, pars = \"beta\", inc_warmup = FALSE)\n\n\n\n\n\n\n\n\n\npost_chain &lt;- rstan::extract(Logistic_stan, \"beta\") \nbetas &lt;- post_chain$beta\ncolnames(betas) &lt;- colnames(X)\n\ndf_plot &lt;- \n  data.frame(value = c(betas[,1], betas[,2]), \n             param = rep(colnames(X), each = nrow(post_chain$beta)))\n\ndf_plot %&gt;% \n  ggplot(aes(value, fill = param)) + \n  geom_density() + \n  facet_wrap(~param, scales = 'free') + \n  theme_minimal() + \n  theme(legend.position=\"false\")\n\n\n\n\n\n\n\n\nWe can evaluate probabilities for each patients.\n\nprobs_stan &lt;- rstan::extract(Logistic_stan, \"p\")$p\nplot(cardiac$Age, colMeans(probs_stan), pch=20, type=\"l\")\n\n\n\n\n\n\n\nplot(cardiac$Age, colMeans(probs_stan), pch=20)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Inference II",
    "section": "",
    "text": "Statistical Inference II provides an introduction to Bayesian data analysis, covering prior and posterior distributions, classical one-parameter models, prior elicitation, posterior-based inference, MCMC methods, and Bayesian approaches to linear and generalized linear models.\n\nSyllabus\n\nIntroduction to Bayesian data analysis: prior and posterior distributions for inference.\nOne-parameter models: Binomial-Beta, Poisson-Gamma, Exponential-Gamma, and Normal-Normal.\nMethods for prior elicitation.\nInference based on the posterior distribution (point and interval estimates; hypotheses testing).\nSimulation-based inference: MCMC methods.\nLinear and generalized linear models from a Bayesian perspective.\n\n\n\nScripts\n\nThe Bernoulli-Beta model: Code.\nNormal approximation and Monte Carlo: Code.\nMCMC - Gibbs sampling: Code.\nMCMC - Metropolis Hasting: Code.\nMCMC - Stan: Code.\n\n\n\nTextbooks\n\nHoff, P. (2009). A first course in Bayesian Statistical Methods. Springer.\nGelman, A., Carlin, J., Stern, H., Dunson, D., Vehtari, A., and Rubin, D. (2013). Bayesian Data Analysis. Chapman & Hall/CRC Texts in Statistical Science.\nRobert, C. and Casella, G. (2004). Monte Carlo Statistical Methods. Springer."
  }
]