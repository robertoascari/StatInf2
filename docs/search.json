[
  {
    "objectID": "Script5.html",
    "href": "Script5.html",
    "title": "Script 1 - Bernoulli - Beta",
    "section": "",
    "text": "You can download the R script here.\nIn this section, we will explore the Bernoulli-Beta model applied to the cardiac dataset. Specifically, we assume that \\(Y_i | \\theta \\sim Bernoulli(\\theta)\\) and that \\(\\theta \\sim Beta(a, b)\\). With these choices, we showed that the posterior distribution is \\(\\theta | \\textbf{y} \\sim Beta(a + \\sum_{i=1}^n y_i, b + n - \\sum_{i=1}^n y_i)\\).\nWe will examine four different scenarios, each corresponding to a distinct choice of hyperparameters for the Beta prior.\n\ncardiac &lt;- read.table(\"data/cardiac.csv\", header=T, sep=\";\")\nstr(cardiac)\n\n'data.frame':   100 obs. of  2 variables:\n $ Age: int  20 23 24 25 25 26 26 28 28 29 ...\n $ Chd: int  0 0 0 0 1 0 0 0 0 0 ...\n\ny &lt;- cardiac$Chd\ntable(y)\n\ny\n 0  1 \n57 43 \n\nsum(y)\n\n[1] 43\n\nlength(y)\n\n[1] 100\n\n\nThese are thw four scenarios we consider: - A: \\(\\theta \\sim Beta(1,1)\\); - B: \\(\\theta \\sim Beta(10,10)\\); - C: \\(\\theta \\sim Beta(10,5)\\); - D: \\(\\theta \\sim Beta(5,10)\\).\n\npar(mfrow=c(2,2))\ncurve(dbeta(x, 1, 1), main=\"Scenario A\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 10, 10), main=\"Scenario B\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 10, 5), main=\"Scenario C\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 5, 10), main=\"Scenario D\", xlab=expression(theta), ylab=expression(pi(theta)))\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nWe further define the L_binom function, which returns the value of the likelihood function evaluated in a specific theta point.\n\nL_Binom &lt;- function(y, theta){\n  s &lt;- sum(y)\n  n &lt;- length(y)\n  L &lt;- theta^s * (1-theta)^(n-s)\n  return(L)\n}\n\nL_Binom(y, theta=0.001)\n\n[1] 9.445671e-130\n\n\nObviously, the likelihood function does not depend on the prior distribution we impose on \\(\\theta\\). In this case, the MLE corresponds to the sample mean (blue dashed line).\n\ncurve(L_Binom(y, theta=x), main=\"Likelihood\",\n      xlab=expression(theta), lwd=2)\nabline(v=mean(y), col=\"blue\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nScenario A:\nIn this scenario, the prior distribution coincides with a uniform distribution on the \\((0,1)\\) interval. Thus, the prior mean (red dashed line) is equal to 0.5.\n\na1 &lt;- 1; b1 &lt;- 1\n\ncurve(dbeta(x, a1, b1), main=\"Prior A\", ylim=c(0,1.4),\n      xlab=expression(theta), lwd=2)\nabline(v=a1/(a1+b1), col=\"red\", lty=\"dashed\")\n\n\n\n\n\n\n\n\nThe posterior distribution is $| Beta()\n\nn &lt;- length(y)\na1.post &lt;- a1 + sum(y)\nb1.post &lt;- b1 + n - sum(y)\n\ncurve(dbeta(x, a1.post, b1.post), main=\"Scenario A\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a1, b1), lty=\"dashed\", add=T, lwd=2)\nabline(v=a1/(a1+b1), col=\"red\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"green\", lty=\"dotted\", lwd=2)\nabline(v=a1.post/(a1.post+b1.post), col=\"blue\", lty=\"dashed\", lwd=2)\nlegend(.8,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"red\", \"green\", \"blue\"), lty=c(2,1,2,3,2), bty=\"n\")\n\n\n\n\n\n\n\n\n\n\nThe chosen prior distribution does not favor\n\n\nany value of theta.\n\n\nThus, we selected a non-informative prior.\n\n\nThe posterior distribution is centered\n\n\naround the MLE.\n\n\nScenario B:\na2 &lt;- 10; b2 &lt;- 10\n\n\nPrior distribution:\ncurve(dbeta(x, a2, b2), main=“Prior B”, xlab=expression(theta), lwd=2) abline(v=a2/(a2+b2), col=“red”, lty=“dashed”)\n\n\nLikelihood function:\ncurve(L_Binom(y, theta=x), main=“Likelihood”, xlab=expression(theta), lwd=2) abline(v=mean(y), col=“blue”, lty=“dashed”)\n\n\nPosterior distribution:\nn &lt;- length(y) a2.post &lt;- a2 + sum(y) b2.post &lt;- b2 + n - sum(y)\ncurve(dbeta(x, a2.post, b2.post), main=“Scenario B”, xlab=expression(theta), lwd=2) curve(dbeta(x, a2, b2), lty=“dashed”, add=T, lwd=2) abline(v=a2/(a2+b2), col=“red”, lty=“dashed”, lwd=2) abline(v=mean(y), col=“green”, lty=“dotted”, lwd=2) abline(v=a2.post/(a2.post+b2.post), col=“blue”, lty=“dashed”, lwd=2) legend(.8,8, c(“Prior”, “Posterior”, “Prior Mean”, “MLE”, “Post. Mean”), lwd=2, col=c(“black”, “black”, “red”, “green”, “blue”), lty=c(2,1,2,3,2), bty=“n”)\n\n\nNow we are still considering a symmetric prior\n\n\nwith prior mean = 0.5, but it does not\n\n\ngive same density/probability to each\n\n\nvalue/interval of theta.\n\n\nNow the posterior mean is a weighted average\n\n\nbetween prior mean and MLE.\n\n\nScenario C:\na3 &lt;- 10; b3 &lt;- 5\n\n\nPosterior distribution:\nn &lt;- length(y) a3.post &lt;- a3 + sum(y) b3.post &lt;- b3 + n - sum(y)\ncurve(dbeta(x, a3.post, b3.post), main=“Scenario C”, xlab=expression(theta), lwd=2) curve(dbeta(x, a3, b3), lty=“dashed”, add=T, lwd=2) abline(v=a3/(a3+b3), col=“red”, lty=“dashed”, lwd=2) abline(v=mean(y), col=“green”, lty=“dotted”, lwd=2) abline(v=a3.post/(a3.post+b3.post), col=“blue”, lty=“dashed”, lwd=2) legend(.8,8, c(“Prior”, “Posterior”, “Prior Mean”, “MLE”, “Post. Mean”), lwd=2, col=c(“black”, “black”, “red”, “green”, “blue”), lty=c(2,1,2,3,2), bty=“n”)\n\n\nScenario D:\na4 &lt;- 5; b4 &lt;- 10\n\n\nPosterior distribution:\nn &lt;- length(y) a4.post &lt;- a4 + sum(y) b4.post &lt;- b4 + n - sum(y)\ncurve(dbeta(x, a4.post, b4.post), main=“Scenario D”, xlab=expression(theta), lwd=2) curve(dbeta(x, a4, b4), lty=“dashed”, add=T, lwd=2) abline(v=a4/(a4+b4), col=“red”, lty=“dashed”, lwd=2) abline(v=mean(y), col=“green”, lty=“dotted”, lwd=2) abline(v=a4.post/(a4.post+b4.post), col=“blue”, lty=“dashed”, lwd=2) legend(.8,8, c(“Prior”, “Posterior”, “Prior Mean”, “MLE”, “Post. Mean”), lwd=2, col=c(“black”, “black”, “red”, “green”, “blue”), lty=c(2,1,2,3,2), bty=“n”)\n\n\nWe may summarize the (prior and) posterior distribution\n\n\nby some measures:\nBeta.Exp &lt;- function(a, b) a/(a+b) Beta.Var &lt;- function(a, b)(ab)/((a+b)^2(a+b+1)) Beta.Mode &lt;- function(a, b){ ifelse(a&gt;1 & b&gt;1, (a-1)/(a+b-2), NA) } Beta.Median &lt;- function(a, b) qbeta(.5, a, b)\nmeasures &lt;- matrix(NA, ncol=6, nrow=4) rownames(measures) &lt;- c(“A”, “B”, “C”, “D”) colnames(measures) &lt;- c(“a.post”, “b.post”, “Exp”, “Mode”, “Median”, “Var”)\nmeasures[,1] &lt;- c(a1.post,a2.post,a3.post,a4.post) measures[,2] &lt;- c(b1.post,b2.post,b3.post,b4.post) for(i in 1:4){ a &lt;- measures[i,1] b &lt;- measures[i,2] measures[i,3] &lt;- Beta.Exp(a, b) measures[i,4] &lt;- Beta.Mode(a, b) measures[i,5] &lt;- Beta.Median(a, b) measures[i,6] &lt;- Beta.Var(a, b) }\nround(measures,4)\n\n\nIn this simple scenario, even considering\n\n\ndifferent priors, we do not have posteriors\n\n\nleading to very different point estimates.\n\n\nThis is because the empirical evidence\n\n\n(i.e., our data) is much\n\n\nstronger than our prior belief.\n\n\nLet us consider an additional scenario where\n\n\nwe strongly believe that most people\n\n\ndo not have cardiovascular disease.\na5 &lt;- 100 b5 &lt;- 1000\n\n\nPrior distribution:\ncurve(dbeta(x, a5, b5), main=“Prior E”, xlab=expression(theta)) abline(v=a5/(a5+b5), col=“red”, lty=“dashed”)\n\n\nPosterior distribution:\nn &lt;- length(y) a5.post &lt;- a5 + sum(y) b5.post &lt;- b5 + n - sum(y)\ncurve(dbeta(x, a5.post, b5.post), main=“Scenario E”, xlab=expression(theta), lwd=2) curve(dbeta(x, a5, b5), lty=“dashed”, add=T, lwd=2) abline(v=a5/(a5+b5), col=“red”, lty=“dashed”, lwd=2) abline(v=mean(y), col=“green”, lty=“dotted”, lwd=2) abline(v=a5.post/(a5.post+b5.post), col=“blue”, lty=“dashed”, lwd=2) legend(.8,40, c(“Prior”, “Posterior”, “Prior Mean”, “MLE”, “Post. Mean”), lwd=2, col=c(“black”, “black”, “red”, “green”, “blue”), lty=c(2,1,2,3,2), bty=“n”)\nmeasures &lt;- matrix(NA, ncol=6, nrow=5) rownames(measures) &lt;- c(“A”, “B”, “C”, “D”, “E”) colnames(measures) &lt;- c(“a.post”, “b.post”, “Exp”, “Mode”, “Median”, “Var”)\nmeasures[,1] &lt;- c(a1.post,a2.post,a3.post,a4.post,a5.post) measures[,2] &lt;- c(b1.post,b2.post,b3.post,b4.post,b5.post) for(i in 1:5){ a &lt;- measures[i,1] b &lt;- measures[i,2] measures[i,3] &lt;- Beta.Exp(a, b) measures[i,4] &lt;- Beta.Mode(a, b) measures[i,5] &lt;- Beta.Median(a, b) measures[i,6] &lt;- Beta.Var(a, b) }\nround(measures,4)\n\n\nInterval Estimates:\n\n\nCredible Sets (CS).\n\n\nThe CS are very easy to compute. Indeed, we only have to compute\n\n\nquantiles of the posterior distribution.\n\n\nLet us consider scenario D:\na &lt;- a4.post b &lt;- b4.post\n\n\nScenario D:\ncurve(dbeta(x, a, b), main=“Scenario D”, xlab=expression(theta), lwd=2)\n\n\nCS of level 0.95\nCS &lt;- qbeta(c(0.025,0.975), a, b);CS abline(v=CS,lty=“dashed”, col=“blue”) legend(0.8,8,c(“Posterior”, “Credible Set”), lty=c(1,2), col=c(“black”, “blue”), bty=“n”)\n\n\nHighest posterior density (HPD):\n\n\nLet’s start by considering a random value for h\nh &lt;- 2 curve(dbeta(x, a, b), ylab=expression(paste(pi,“(”,theta,“|x)”)), xlab=expression(theta)) abline(h=h,lty=2)\n\n\nWe need to find the values of theta such that p(theta|y) = h\ntranslated &lt;- function(x, a, b) dbeta(x,a, b)-h # We decrease the density curve by h. # Now, the values we are looking for are such that translated = 0\ncurve(translated(x, a, b), add=T,lty=3,lwd=2) abline(h=0,lty=3)\nhpd1 &lt;- uniroot(translated,c(.2, .4),a,b)\\(root;hpd1\nhpd2 &lt;- uniroot(translated,c(.45, .5),a,b)\\)root;hpd2 integrate(dbeta, lower=hpd1, upper=hpd2, shape1=a, shape2=b)\n\n\nWe have a probability equal to 0.9152 –&gt; Decrease h\n\n\nIterative way:\nh.grid &lt;- seq(1, 2, by = 0.01)\nres &lt;- matrix(NA, ncol=4, nrow=length(h.grid)) colnames(res) &lt;- c(“HPD1”, “HPD2”, “level”, “h”)\nfor(i in 1:length(h.grid)){\ntranslated &lt;- function(x, a, b) dbeta(x,a, b)-h.grid[i]\nhpd1&lt;-uniroot(translated,c(.2,.4),a,b)\\(root\n  hpd2&lt;-uniroot(translated,c(.43,.55),a,b)\\)root\nI &lt;- integrate(dbeta, lower=hpd1, upper=hpd2, shape1=a, shape2=b)$value\niter &lt;- i res[i,] &lt;- c(hpd1, hpd2, I, h.grid[i]) if(I &lt;= 0.95) break }\nres[1:iter,] h.grid[iter]\nHPD &lt;- res[iter-1,-c(3,4)]; HPD CS\n\n\nGraphically:\ncurve(dbeta(x, a, b), main=“Scenario D”, xlab=expression(theta), lwd=2)\nabline(v=CS,lty=“dashed”, col=“blue”) abline(v=HPD,lty=“dashed”, col=“red”)\nlegend(0.8,8,c(“Posterior”, “Credible Set”, “HPD”), lty=c(1,2,2), col=c(“black”, “blue”, “red”), bty=“n”)\n\n\nHPD and CS are very similar, because the posterior is\n\n\nunimodal and almost symmetrical.\n\n\nHypothesis testing:\n\n\nLet consider H0: theta &lt;= 0.3 vs H1: theta &gt; 0.3\npbeta(.3, a4, b4) pbeta(.3, a4.post, b4.post)\nODDS.prior &lt;- pbeta(.3, a4, b4)/(1-pbeta(.3, a4, b4)); ODDS.prior ODDS.post &lt;- pbeta(.3, a4.post, b4.post)/(1-pbeta(.3, a4.post, b4.post)); ODDS.post\nBayes_Factor &lt;- ODDS.post/ODDS.prior; Bayes_Factor\n\n\nData do not support H0: by adding data, our confidence on\n\n\nH0 strongly decreases\n\n\nHOMEWORK: consider a sample of size n = 10 composed of\n\n\nBernoulli trials, for which we observed s = 3 successes.\n\n\nConsider that a priori theta ~ Beta(2, 8)\n\n\n1. Which is the posterior distribution of theta?\n\n\n2. Compute the MLE and the prior and posterior means.\n\n\nAre these results reasonable?\n\n\n3. Compute and compare the CS and the HPD.\n\n\n4. Let H0 : theta in (0.3, 0.5)\n\n\nH1 : theta in (0, 0.3) U (0.5, 1)\n\n\nWhich hypothesis can you support?"
  },
  {
    "objectID": "Script3.html",
    "href": "Script3.html",
    "title": "Script 3 - MCMC - Gibbs sampling",
    "section": "",
    "text": "In this section, we show two Gibbs samplers: the first one aims to estimate the parameters \\((\\mu, \\sigma^2)\\) of a Normal likelihood, whereas the second one estimates the parameters of a linear regression model."
  },
  {
    "objectID": "Script3.html#initialization",
    "href": "Script3.html#initialization",
    "title": "Script 3 - MCMC - Gibbs sampling",
    "section": "Initialization:",
    "text": "Initialization:\nWe initialize each element by drawing a value from the corresponding prior:\n\nB &lt;- 1000\nmu.chain &lt;- numeric(B)\nsigma2.chain &lt;- numeric(B)\n\nset.seed(42)\nmu.chain[1] &lt;- rnorm(1, mu0, sqrt(sigma2.0))\nsigma2.chain[1] &lt;- 1/rgamma(1, alpha, rate = beta)\n\nThen, we can implement the Gibbs sampling by generating each parameter from its full-conditional distribution. The full-conditionals are the following:\n\n\\(\\mu| \\sigma^2, \\textbf{y} \\sim N\\left(\\frac{n\\sigma^2_0 \\bar{y} + \\sigma^2 \\mu_0}{n\\sigma^2_0 + \\sigma^2}, \\frac{\\sigma^2 \\sigma^2_0}{n\\sigma^2_0 + \\sigma^2}\\right)\\);\n\\(\\sigma^2| \\mu, \\textbf{y} \\sim Inv.Gamma\\left(a_0 + \\frac{n}{2}, b_0 + \\frac{\\sum_{i=1}^n (y_i - \\mu)^2}{2}\\right)\\).\n\n\nset.seed(42)\nfor(b in 2:B){\n  # Draw mu from the F.C.\n  mu.chain[b] &lt;-\n    rnorm(1, \n          (n*ybar*sigma2.0 + sigma2.chain[b-1])/(n*sigma2.0+sigma2.chain[b-1]),\n          sqrt((sigma2.chain[b-1]*sigma2.0)/(n*sigma2.0+sigma2.chain[b-1])))\n  \n  # Draw sigma2 from the F.C.\n  sigma2.chain[b] &lt;-\n    1/rgamma(1, alpha + n/2,\n             rate = beta + .5*sum((y-mu.chain[b])^2))\n  \n}\n\n\nplot(mu.chain, sigma2.chain, pch=20)\npoints(mu.true, sigma2.true, col=\"#D55E00\", pch=20)\n\n\n\n\n\n\n\n\nWe can now generate some diagnostic plots.\n\n# Traceplots:\npar(mfrow=c(2,1))\nplot(mu.chain[-1], pch=20, type=\"l\")\nplot(sigma2.chain[-1], pch=20, type=\"l\")\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\n\nmu.means &lt;- numeric(B)\nsigma2.means &lt;- numeric(B)\n\nmu.means[1] &lt;- mu.chain[1]\nsigma2.means[1] &lt;- sigma2.chain[1]\n\nfor(b in 2:B){\n  mu.means[b] &lt;- mean(mu.chain[2:b])\n  sigma2.means[b] &lt;- mean(sigma2.chain[2:b])\n}\n\nplot(mu.means[-1], pch=20, type=\"l\", ylim=c(9.2, 10.8))\nabline(h=mu.true, col=\"#D55E00\")\n\n\n\n\n\n\n\n\n\nplot(sigma2.means[-1], pch=20, type=\"l\", ylim=c(3,7))\nabline(h=sigma2.true, col=\"#D55E00\")\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,1))\nacf(mu.chain)\nacf(sigma2.chain)\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nOnce we have a chain for each parameter, we need to remove the warm-up (i.e., the part of the chains for which we cannot assume the convergence to the stationary distribution).\n\nwarm_perc &lt;- .5\n\nmu.new &lt;- mu.chain[round(B*warm_perc+1):B]\nsigma2.new &lt;- sigma2.chain[round(B*warm_perc+1):B]\n\npar(mfrow=c(2,1))\nplot(mu.new, pch=20, type=\"l\", ylim=c(9.2, 10.8))\nabline(h=mu.true, col=\"#D55E00\")\n\nplot(sigma2.new, pch=20, type=\"l\", ylim=c(3,7))\nabline(h=sigma2.true, col=\"#D55E00\")\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nThe new vectors can be used to compute estimates, CSs, and probabilities:\n\nmean(mu.new)\n\n[1] 10.07781\n\nquantile(mu.new, probs = c(.025, .975))\n\n     2.5%     97.5% \n 9.660985 10.498000 \n\n\n\nmean(sigma2.new)\n\n[1] 4.710458\n\nquantile(sigma2.new, probs = c(.025, .975))\n\n    2.5%    97.5% \n3.694687 6.045622 \n\n\n\nmean(mu.new &gt; 10)\n\n[1] 0.648\n\n\n\nhist(mu.new, prob=T, xlab=expression(mu),\n     main=\"Posterior distribution of mu\")\nlines(density(mu.new), col=\"#D55E00\")\nabline(v=mu.true, col=\"black\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nhist(sigma2.new, prob=T, xlab=expression(sigma2),\n     main=\"Posterior distribution of sigma2\")\nlines(density(sigma2.new), col=\"#D55E00\")\nabline(v=sigma2.true, col=\"black\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nplot(mu.new, sigma2.new, pch = 20)\npoints(mu.true, sigma2.true, pch = 19, col = \"#D55E00\")\n\n\n\n\n\n\n\n\nWe can define a function to fit this Gibbs sampler more easily on new data.\n\nnormal_GS &lt;- function(y, B = 5000, \n                      mu0 = 0, sigma2.0 = 1000, \n                      alpha = 10, beta = 10, \n                      warm_perc = .5, seed=42){\n  \n  mu.chain &lt;- numeric(B)\n  sigma2.chain &lt;- numeric(B)\n  ybar &lt;- mean(y)\n  n &lt;- length(y)\n  \n  # Initialization:\n  set.seed(seed)\n  mu.chain[1] &lt;- rnorm(1, mu0, sqrt(sigma2.0))\n  sigma2.chain[1] &lt;- 1/rgamma(1, alpha, rate = beta)\n  \n  for(b in 2:B){\n    # Draw mu from the F.C.\n    mu.chain[b] &lt;-\n      rnorm(1,\n            (n*ybar*sigma2.0 + sigma2.chain[b-1])/(n*sigma2.0+sigma2.chain[b-1]),\n            sqrt((sigma2.chain[b-1]*sigma2.0)/(n*sigma2.0+sigma2.chain[b-1])))\n    \n    # Draw sigma2 from the F.C.\n    sigma2.chain[b] &lt;-\n      1/rgamma(1, alpha + n/2,\n               rate = beta + .5*sum((y-mu.chain[b])^2))\n  }\n  \n  mu.new &lt;- mu.chain[round(B*warm_perc+1):B]\n  sigma2.new &lt;- sigma2.chain[round(B*warm_perc+1):B]\n  \n  return(cbind(mu.chain = mu.new, sigma2.chain = sigma2.new))\n}"
  },
  {
    "objectID": "Script3.html#gala-dataset-i",
    "href": "Script3.html#gala-dataset-i",
    "title": "Script 3 - MCMC - Gibbs sampling",
    "section": "Gala dataset (I)",
    "text": "Gala dataset (I)\n\nlibrary(faraway)\n\nWarning in check_dep_version(): ABI version mismatch: \nlme4 was built with Matrix ABI version 1\nCurrent Matrix ABI version is 0\nPlease re-install lme4 from source or restore original 'Matrix' package\n\ndata(gala)\nstr(gala)\n\n'data.frame':   30 obs. of  7 variables:\n $ Species  : num  58 31 3 25 2 18 24 10 8 2 ...\n $ Endemics : num  23 21 3 9 1 11 0 7 4 2 ...\n $ Area     : num  25.09 1.24 0.21 0.1 0.05 ...\n $ Elevation: num  346 109 114 46 77 119 93 168 71 112 ...\n $ Nearest  : num  0.6 0.6 2.8 1.9 1.9 8 6 34.1 0.4 2.6 ...\n $ Scruz    : num  0.6 26.3 58.7 47.4 1.9 ...\n $ Adjacent : num  1.84 572.33 0.78 0.18 903.82 ...\n\nhelp(gala)\n\navvio in corso del server httpd per la guida ... fatto\n\n\n\ny &lt;- gala$Species\ngala_GS &lt;- normal_GS(y, B = 10000)\n\nstr(gala_GS)\n\n num [1:5000, 1:2] 92.7 44.4 32.8 24.4 103 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr [1:2] \"mu.chain\" \"sigma2.chain\"\n\nhead(gala_GS)\n\n      mu.chain sigma2.chain\n[1,]  92.71002     9974.745\n[2,]  44.36107     8010.302\n[3,]  32.81073     9962.350\n[4,]  24.37675    10741.804\n[5,] 103.04801     7786.621\n[6,]  63.96422     7512.084\n\n\n\nplot(gala_GS, pch = 20)\n\n\n\n\n\n\n\nhist(gala_GS[,1], prob = T, \n     main=\"Posterior distribution of mu\")\nlines(density(gala_GS[,1]), col=\"#D55E00\")\n\n\n\n\n\n\n\nhist(gala_GS[,2], prob = T, \n     main=\"Posterior distribution of sigma2\")\nlines(density(gala_GS[,2]), col=\"#D55E00\")\n\n\n\n\n\n\n\ncolMeans(gala_GS)\n\n    mu.chain sigma2.chain \n    67.34122   8294.29403 \n\nt(apply(gala_GS, 2, function(x) quantile(x, probs=c(.025, .975))))\n\n                   2.5%       97.5%\nmu.chain       37.53342    96.70556\nsigma2.chain 5465.86469 12619.92813\n\n\n\nmean(gala_GS[,1] &gt; 70)\n\n[1] 0.429\n\nmean(gala_GS[,1] &gt; 70 & gala_GS[,2] &lt; 5500)\n\n[1] 0.0174"
  },
  {
    "objectID": "Script3.html#gala-dataset-ii",
    "href": "Script3.html#gala-dataset-ii",
    "title": "Script 3 - MCMC - Gibbs sampling",
    "section": "Gala dataset (II)",
    "text": "Gala dataset (II)\n\ngala_lm &lt;- lm(Species ~ ., data = gala[,-2])\nsumm &lt;- summary(gala_lm); summ\n\n\nCall:\nlm(formula = Species ~ ., data = gala[, -2])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-111.679  -34.898   -7.862   33.460  182.584 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.068221  19.154198   0.369 0.715351    \nArea        -0.023938   0.022422  -1.068 0.296318    \nElevation    0.319465   0.053663   5.953 3.82e-06 ***\nNearest      0.009144   1.054136   0.009 0.993151    \nScruz       -0.240524   0.215402  -1.117 0.275208    \nAdjacent    -0.074805   0.017700  -4.226 0.000297 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 60.98 on 24 degrees of freedom\nMultiple R-squared:  0.7658,    Adjusted R-squared:  0.7171 \nF-statistic:  15.7 on 5 and 24 DF,  p-value: 6.838e-07\n\n# Definition of y and X:\ny &lt;- gala$Species\nX &lt;- model.matrix(Species ~ ., data = gala[,-2])\n\n# Fitting the model:\nlm_gala &lt;- LM_GS(y, X)\nstr(lm_gala)\n\nList of 2\n $ beta  : num [1:2500, 1:6] -0.685 0.788 -0.497 0.317 -0.501 ...\n $ sigma2: num [1:2500] 1677 2015 1942 1978 2534 ...\n\n\n\n# Extracting the elements of the chain:\nbetas &lt;- lm_gala$beta\ncolnames(betas) &lt;- colnames(X)\nsigma2 &lt;- lm_gala$sigma2\n\ncolMeans(betas)\n\n(Intercept)        Area   Elevation     Nearest       Scruz    Adjacent \n 0.02312137 -0.02602818  0.32983671 -0.00566936 -0.20657747 -0.07616102 \n\n\n\nt(apply(betas, 2, function(x) quantile(x, probs=c(.025, .975))))\n\n                   2.5%        97.5%\n(Intercept) -1.90212462  1.943814977\nArea        -0.05552613  0.003312421\nElevation    0.26572467  0.392750020\nNearest     -1.14952611  1.231034257\nScruz       -0.48552725  0.070630919\nAdjacent    -0.09987118 -0.051690537\n\n\n\nmean(sigma2)\n\n[1] 2049.94\n\n\n\nround(cov(betas), 5)\n\n            (Intercept)     Area Elevation  Nearest    Scruz Adjacent\n(Intercept)     0.97945  0.00058  -0.00232  0.02294 -0.00695  0.00054\nArea            0.00058  0.00023  -0.00038  0.00136  0.00035  0.00007\nElevation      -0.00232 -0.00038   0.00104 -0.00467 -0.00106 -0.00025\nNearest         0.02294  0.00136  -0.00467  0.38000 -0.05012  0.00196\nScruz          -0.00695  0.00035  -0.00106 -0.05012  0.01880 -0.00008\nAdjacent        0.00054  0.00007  -0.00025  0.00196 -0.00008  0.00016"
  },
  {
    "objectID": "Script1.html",
    "href": "Script1.html",
    "title": "Script 1 - Bernoulli - Beta",
    "section": "",
    "text": "In this section, we will explore the Bernoulli-Beta model applied to the cardiac dataset. Specifically, we assume that \\(Y_i | \\theta \\sim Bernoulli(\\theta)\\) and that \\(\\theta \\sim Beta(a, b)\\). With these choices, we showed that the posterior distribution is \\[\\theta | \\textbf{y} \\sim Beta\\left(a + \\sum_{i=1}^n y_i, b + n - \\sum_{i=1}^n y_i\\right).\\]\nWe will examine four different scenarios, each corresponding to a distinct choice of hyperparameters for the Beta prior.\n\ncardiac &lt;- read.table(\"data/cardiac.csv\", header=T, sep=\";\")\nstr(cardiac)\n\n'data.frame':   100 obs. of  2 variables:\n $ Age: int  20 23 24 25 25 26 26 28 28 29 ...\n $ Chd: int  0 0 0 0 1 0 0 0 0 0 ...\n\ny &lt;- cardiac$Chd\ntable(y)\n\ny\n 0  1 \n57 43 \n\nsum(y)\n\n[1] 43\n\n\n\ncardiac &lt;- read.table(\"data/cardiac.csv\", header=T, sep=\";\")\nstr(cardiac)\n\n'data.frame':   100 obs. of  2 variables:\n $ Age: int  20 23 24 25 25 26 26 28 28 29 ...\n $ Chd: int  0 0 0 0 1 0 0 0 0 0 ...\n\ny &lt;- cardiac$Chd\ntable(y)\n\ny\n 0  1 \n57 43 \n\nsum(y)\n\n[1] 43\n\nlength(y)\n\n[1] 100\n\n\n\nsum(y)\n\n[1] 43\n\n\n\nlength(y)\n\n[1] 100\n\n\nThese are the four scenarios we consider:\n\nA: \\(\\theta \\sim Beta(1,1)\\);\nB: \\(\\theta \\sim Beta(10,10)\\);\nC: \\(\\theta \\sim Beta(10,5)\\);\nD: \\(\\theta \\sim Beta(5,10)\\).\n\n\npar(mfrow=c(2,2))\ncurve(dbeta(x, 1, 1), main=\"Scenario A\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 10, 10), main=\"Scenario B\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 10, 5), main=\"Scenario C\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 5, 10), main=\"Scenario D\", xlab=expression(theta), ylab=expression(pi(theta)))\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nWe further define the L_binom function, which returns the value of the likelihood function evaluated in a specific theta point.\n\nL_Binom &lt;- function(y, theta){\n  s &lt;- sum(y)\n  n &lt;- length(y)\n  L &lt;- theta^s * (1-theta)^(n-s)\n  return(L)\n}\n\nL_Binom(y, theta=0.001)\n\n[1] 9.445671e-130\n\n\nObviously, the likelihood function does not depend on the prior distribution we impose on \\(\\theta\\). In this case, the MLE corresponds to the sample mean (blue dashed line).\n\ncurve(L_Binom(y, theta=x), main=\"Likelihood\",\n      xlab=expression(theta), lwd=2)\nabline(v=mean(y), col=\"#0072B2\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\n\nIn this scenario, the prior distribution coincides with a uniform distribution on the \\((0,1)\\) interval. Thus, the prior mean (red dashed line) is equal to 0.5.\n\na1 &lt;- 1; b1 &lt;- 1\n\ncurve(dbeta(x, a1, b1), main=\"Prior A\", ylim=c(0,1.4),\n      xlab=expression(theta), lwd=2)\nabline(v=a1/(a1+b1), col=\"#D55E00\", lty=\"dashed\")\n\n\n\n\n\n\n\n\nThe posterior distribution is \\(\\theta | \\textbf{y} \\sim Beta(44, 58)\\).\n\nn &lt;- length(y)\na1.post &lt;- a1 + sum(y)\nb1.post &lt;- b1 + n - sum(y)\n\ncurve(dbeta(x, a1.post, b1.post), main=\"Scenario A\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a1, b1), lty=\"dashed\", add=T, lwd=2)\nabline(v=a1/(a1+b1), col=\"#D55E00\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"#009E73\", lty=\"dotted\", lwd=2)\nabline(v=a1.post/(a1.post+b1.post), col=\"#0072B2\", lty=\"dashed\", lwd=2)\nlegend(.6,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"#D55E00\", \"#009E73\", \"#0072B2\"), lty=c(2,1,2,3,2), bty=\"n\")\n\n\n\n\n\n\n\n\nThe chosen prior distribution does not favor any particular value of \\(\\theta\\). Thus, we selected a non-informative prior. As a result, the posterior distribution is centered around the maximum likelihood estimate (MLE).\n\n\n\nIn this scenario, we use a prior distribution for \\(\\theta\\) that is still symmetric (meaning it treats values below and above 0.5 in the same way). However, unlike the uniform prior, it is not flat: it expresses a preference for values closer to 0.5.\n\na2 &lt;- 10; b2 &lt;- 10\n\ncurve(dbeta(x, a2, b2), main=\"Prior B\",\n      xlab=expression(theta), lwd=2)\nabline(v=a2/(a2+b2), col=\"#D55E00\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nn &lt;- length(y)\na2.post &lt;- a2 + sum(y)\nb2.post &lt;- b2 + n - sum(y)\n\ncurve(dbeta(x, a2.post, b2.post), main=\"Scenario B\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a2, b2), lty=\"dashed\", add=T, lwd=2)\nabline(v=a2/(a2+b2), col=\"#D55E00\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"#009E73\", lty=\"dotted\", lwd=2)\nabline(v=a2.post/(a2.post+b2.post), col=\"#0072B2\", lty=\"dashed\", lwd=2)\nlegend(.6,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"#D55E00\", \"#009E73\", \"#0072B2\"), lty=c(2,1,2,3,2), bty=\"n\")\n\n\n\n\n\n\n\n\nSince we are still considering a symmetric prior, the prior mean is equal to 0.5. However, unlike the uniform prior, it does not assign the same probability density to every value or interval of \\(\\theta\\). As a consequence, the posterior mean becomes a weighted average between the prior mean and the MLE.\n\n\n\nScenarios C and D consider asymmetric prior distributions.\n\na3 &lt;- 10; b3 &lt;- 5\n\ncurve(dbeta(x, a3, b3), main=\"Prior C\",\n      xlab=expression(theta), lwd=2)\nabline(v=a3/(a3+b3), col=\"#D55E00\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nn &lt;- length(y)\na3.post &lt;- a3 + sum(y)\nb3.post &lt;- b3 + n - sum(y)\n\ncurve(dbeta(x, a3.post, b3.post), main=\"Scenario C\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a3, b3), lty=\"dashed\", add=T, lwd=2)\nabline(v=a3/(a3+b3), col=\"#D55E00\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"#009E73\", lty=\"dotted\", lwd=2)\nabline(v=a3.post/(a3.post+b3.post), col=\"#0072B2\", lty=\"dashed\", lwd=2)\nlegend(.7,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"#D55E00\", \"#009E73\", \"#0072B2\"), lty=c(2,1,2,3,2), bty=\"n\")\n\n\n\n\n\n\n\n\n\na4 &lt;- 5; b4 &lt;- 10\n\ncurve(dbeta(x, a4, b4), main=\"Prior D\",\n      xlab=expression(theta), lwd=2)\nabline(v=a4/(a4+b4), col=\"#D55E00\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nn &lt;- length(y)\na4.post &lt;- a4 + sum(y)\nb4.post &lt;- b4 + n - sum(y)\n\ncurve(dbeta(x, a4.post, b4.post), main=\"Scenario D\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a4, b4), lty=\"dashed\", add=T, lwd=2)\nabline(v=a4/(a4+b4), col=\"#D55E00\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"#009E73\", lty=\"dotted\", lwd=2)\nabline(v=a4.post/(a4.post+b4.post), col=\"#0072B2\", lty=\"dashed\", lwd=2)\nlegend(.6,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"#D55E00\", \"#009E73\", \"#0072B2\"), lty=c(2,1,2,3,2), bty=\"n\")"
  },
  {
    "objectID": "Script1.html#scenario-a",
    "href": "Script1.html#scenario-a",
    "title": "Script 1 - Bernoulli - Beta",
    "section": "",
    "text": "In this scenario, the prior distribution coincides with a uniform distribution on the \\((0,1)\\) interval. Thus, the prior mean (red dashed line) is equal to 0.5.\n\na1 &lt;- 1; b1 &lt;- 1\n\ncurve(dbeta(x, a1, b1), main=\"Prior A\", ylim=c(0,1.4),\n      xlab=expression(theta), lwd=2)\nabline(v=a1/(a1+b1), col=\"#D55E00\", lty=\"dashed\")\n\n\n\n\n\n\n\n\nThe posterior distribution is \\(\\theta | \\textbf{y} \\sim Beta(44, 58)\\).\n\nn &lt;- length(y)\na1.post &lt;- a1 + sum(y)\nb1.post &lt;- b1 + n - sum(y)\n\ncurve(dbeta(x, a1.post, b1.post), main=\"Scenario A\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a1, b1), lty=\"dashed\", add=T, lwd=2)\nabline(v=a1/(a1+b1), col=\"#D55E00\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"#009E73\", lty=\"dotted\", lwd=2)\nabline(v=a1.post/(a1.post+b1.post), col=\"#0072B2\", lty=\"dashed\", lwd=2)\nlegend(.6,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"#D55E00\", \"#009E73\", \"#0072B2\"), lty=c(2,1,2,3,2), bty=\"n\")\n\n\n\n\n\n\n\n\nThe chosen prior distribution does not favor any particular value of \\(\\theta\\). Thus, we selected a non-informative prior. As a result, the posterior distribution is centered around the maximum likelihood estimate (MLE)."
  },
  {
    "objectID": "Script1.html#scenario-b",
    "href": "Script1.html#scenario-b",
    "title": "Script 1 - Bernoulli - Beta",
    "section": "",
    "text": "In this scenario, we use a prior distribution for \\(\\theta\\) that is still symmetric (meaning it treats values below and above 0.5 in the same way). However, unlike the uniform prior, it is not flat: it expresses a preference for values closer to 0.5.\n\na2 &lt;- 10; b2 &lt;- 10\n\ncurve(dbeta(x, a2, b2), main=\"Prior B\",\n      xlab=expression(theta), lwd=2)\nabline(v=a2/(a2+b2), col=\"#D55E00\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nn &lt;- length(y)\na2.post &lt;- a2 + sum(y)\nb2.post &lt;- b2 + n - sum(y)\n\ncurve(dbeta(x, a2.post, b2.post), main=\"Scenario B\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a2, b2), lty=\"dashed\", add=T, lwd=2)\nabline(v=a2/(a2+b2), col=\"#D55E00\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"#009E73\", lty=\"dotted\", lwd=2)\nabline(v=a2.post/(a2.post+b2.post), col=\"#0072B2\", lty=\"dashed\", lwd=2)\nlegend(.6,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"#D55E00\", \"#009E73\", \"#0072B2\"), lty=c(2,1,2,3,2), bty=\"n\")\n\n\n\n\n\n\n\n\nSince we are still considering a symmetric prior, the prior mean is equal to 0.5. However, unlike the uniform prior, it does not assign the same probability density to every value or interval of \\(\\theta\\). As a consequence, the posterior mean becomes a weighted average between the prior mean and the MLE."
  },
  {
    "objectID": "Script1.html#scenario-c-d",
    "href": "Script1.html#scenario-c-d",
    "title": "Script 1 - Bernoulli - Beta",
    "section": "",
    "text": "Scenarios C and D consider asymmetric prior distributions.\n\na3 &lt;- 10; b3 &lt;- 5\n\ncurve(dbeta(x, a3, b3), main=\"Prior C\",\n      xlab=expression(theta), lwd=2)\nabline(v=a3/(a3+b3), col=\"#D55E00\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nn &lt;- length(y)\na3.post &lt;- a3 + sum(y)\nb3.post &lt;- b3 + n - sum(y)\n\ncurve(dbeta(x, a3.post, b3.post), main=\"Scenario C\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a3, b3), lty=\"dashed\", add=T, lwd=2)\nabline(v=a3/(a3+b3), col=\"#D55E00\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"#009E73\", lty=\"dotted\", lwd=2)\nabline(v=a3.post/(a3.post+b3.post), col=\"#0072B2\", lty=\"dashed\", lwd=2)\nlegend(.7,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"#D55E00\", \"#009E73\", \"#0072B2\"), lty=c(2,1,2,3,2), bty=\"n\")\n\n\n\n\n\n\n\n\n\na4 &lt;- 5; b4 &lt;- 10\n\ncurve(dbeta(x, a4, b4), main=\"Prior D\",\n      xlab=expression(theta), lwd=2)\nabline(v=a4/(a4+b4), col=\"#D55E00\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nn &lt;- length(y)\na4.post &lt;- a4 + sum(y)\nb4.post &lt;- b4 + n - sum(y)\n\ncurve(dbeta(x, a4.post, b4.post), main=\"Scenario D\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a4, b4), lty=\"dashed\", add=T, lwd=2)\nabline(v=a4/(a4+b4), col=\"#D55E00\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"#009E73\", lty=\"dotted\", lwd=2)\nabline(v=a4.post/(a4.post+b4.post), col=\"#0072B2\", lty=\"dashed\", lwd=2)\nlegend(.6,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"#D55E00\", \"#009E73\", \"#0072B2\"), lty=c(2,1,2,3,2), bty=\"n\")"
  },
  {
    "objectID": "Script1.html#credible-sets-cs.",
    "href": "Script1.html#credible-sets-cs.",
    "title": "Script 1 - Bernoulli - Beta",
    "section": "Credible Sets (CS).",
    "text": "Credible Sets (CS).\nThe CS are very easy to compute in this scenario. Indeed, it is sufficient to calculate the appropriate quantiles of the posterior distribution.\nLet us consider scenario D:\n\na &lt;- a4.post\nb &lt;- b4.post\n\ncurve(dbeta(x, a, b), main=\"Scenario D\",\n      xlab=expression(theta), lwd=2)\n\n# CS of level 0.95\nCS &lt;- qbeta(c(0.025,0.975), a, b); CS\n\n[1] 0.3291889 0.5083162\n\nabline(v=CS,lty=\"dashed\", col=\"#0072B2\")\nlegend(0.8,8,c(\"Posterior\", \"Credible Set\"),\n       lty=c(1,2), col=c(\"black\", \"#0072B2\"), bty=\"n\")"
  },
  {
    "objectID": "Script1.html#highest-posterior-density-hpd",
    "href": "Script1.html#highest-posterior-density-hpd",
    "title": "Script 1 - Bernoulli - Beta",
    "section": "Highest posterior density (HPD):",
    "text": "Highest posterior density (HPD):\nLet’s start by considering an initial value for \\(h\\).\n\nh &lt;- 2\ncurve(dbeta(x, a, b), \n      ylab=expression(paste(pi,\"(\",theta,\"|x)\")),\n      xlab=expression(theta))\nabline(h=h,lty=2)\n\n\n\n\n\n\n\n\nWe need to find the values of theta such that \\(P(\\theta|\\textbf{y}) = h\\).\nTo do this, we progressively lower the posterior density curve by an amount \\(h\\).\n\ncurve(dbeta(x, a, b), \n      ylab=expression(paste(pi,\"(\",theta,\"|x)\")),\n      xlab=expression(theta))\nabline(h=h,lty=2)\n\ntranslated &lt;- function(x, a, b) dbeta(x,a, b) - h \n\ncurve(translated(x, a, b), add = T, lty = 3, lwd = 2)\nabline(h = 0, lty = 3)\n\n\n\n\n\n\n\n\nThe values we are looking for are those where the translated curve —that is, the original posterior density minus \\(h\\)— becomes zero.\n\nhpd1 &lt;- uniroot(translated,c(.2, .4),a,b)$root; hpd1\n\n[1] 0.3384793\n\nhpd2 &lt;- uniroot(translated,c(.45, .5),a,b)$root; hpd2\n\n[1] 0.4962774\n\nintegrate(dbeta, lower=hpd1, upper=hpd2, shape1=a, shape2=b)\n\n0.9152466 with absolute error &lt; 2.5e-14\n\n\nWe have a probability equal to 0.9152. Thus, we have to select a smaller \\(h\\). Iteratively:\n\nh.grid &lt;- seq(1, 2, by = 0.01)   \n\nres &lt;- matrix(NA, ncol=4, nrow=length(h.grid))\ncolnames(res) &lt;- c(\"HPD1\", \"HPD2\", \"level\", \"h\")\n\nfor(i in 1:length(h.grid)){        \n  translated &lt;- function(x, a, b) dbeta(x,a, b) - h.grid[i] \n  \n  hpd1&lt;-uniroot(translated,c(.2,.4),a,b)$root\n  hpd2&lt;-uniroot(translated,c(.43,.55),a,b)$root\n  \n  I &lt;- integrate(dbeta, lower=hpd1, upper=hpd2, shape1=a, shape2=b)$value\n  \n  iter &lt;- i\n  res[i,] &lt;- c(hpd1, hpd2, I, h.grid[i])\n  if(I &lt;= 0.95) break\n}\n\nres[1:iter,]\n\n           HPD1      HPD2     level    h\n [1,] 0.3225742 0.5134903 0.9635447 1.00\n [2,] 0.3228413 0.5132645 0.9630493 1.01\n [3,] 0.3230458 0.5130403 0.9626140 1.02\n [4,] 0.3232488 0.5128178 0.9621776 1.03\n [5,] 0.3234504 0.5125970 0.9617401 1.04\n [6,] 0.3236505 0.5123778 0.9613016 1.05\n [7,] 0.3238493 0.5121602 0.9608621 1.06\n [8,] 0.3240466 0.5119441 0.9604215 1.07\n [9,] 0.3242426 0.5117296 0.9599800 1.08\n[10,] 0.3244373 0.5115165 0.9595373 1.09\n[11,] 0.3246306 0.5113050 0.9590937 1.10\n[12,] 0.3248227 0.5110948 0.9586491 1.11\n[13,] 0.3250135 0.5108865 0.9582038 1.12\n[14,] 0.3252030 0.5106809 0.9577589 1.13\n[15,] 0.3253914 0.5104765 0.9573130 1.14\n[16,] 0.3255785 0.5102735 0.9568661 1.15\n[17,] 0.3257644 0.5100717 0.9564182 1.16\n[18,] 0.3259492 0.5098712 0.9559692 1.17\n[19,] 0.3261329 0.5096720 0.9555191 1.18\n[20,] 0.3263154 0.5094740 0.9550681 1.19\n[21,] 0.3264968 0.5092772 0.9546160 1.20\n[22,] 0.3266772 0.5090816 0.9541629 1.21\n[23,] 0.3268564 0.5088872 0.9537088 1.22\n[24,] 0.3270346 0.5086939 0.9532537 1.23\n[25,] 0.3272118 0.5085017 0.9527976 1.24\n[26,] 0.3273880 0.5083107 0.9523404 1.25\n[27,] 0.3275631 0.5081208 0.9518822 1.26\n[28,] 0.3277373 0.5079320 0.9514231 1.27\n[29,] 0.3279104 0.5077442 0.9509629 1.28\n[30,] 0.3280827 0.5075575 0.9505017 1.29\n[31,] 0.3282539 0.5073718 0.9500395 1.30\n[32,] 0.3284243 0.5071872 0.9495763 1.31\n\nh.grid[iter]\n\n[1] 1.31\n\nHPD &lt;- res[iter-1,-c(3,4)]; HPD\n\n     HPD1      HPD2 \n0.3282539 0.5073718 \n\nCS\n\n[1] 0.3291889 0.5083162\n\n\nWe can compare the two interval estimates in a graphical way:\n\ncurve(dbeta(x, a, b), main=\"Scenario D\",\n      xlab=expression(theta), lwd=2)\n\nabline(v=CS,lty=\"dashed\", col=\"#0072B2\")\nabline(v=HPD,lty=\"dashed\", col=\"#D55E00\")\n\nlegend(0.6,8,c(\"Posterior\", \"Credible Set\", \"HPD\"),\n       lty=c(1,2,2), col=c(\"black\", \"#0072B2\", \"#D55E00\"), bty=\"n\")\n\n\n\n\n\n\n\n\nHPD and CS are very similar, since the posterior is unimodal and almost symmetrical."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Inference II",
    "section": "",
    "text": "Statistical Inference II provides an introduction to Bayesian data analysis, covering prior and posterior distributions, classical one-parameter models, prior elicitation, posterior-based inference, MCMC methods, and Bayesian approaches to linear and generalized linear models."
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Statistical Inference II",
    "section": "Syllabus",
    "text": "Syllabus\n\nIntroduction to Bayesian data analysis: prior and posterior distributions for inference.\nOne-parameter models: Binomial-Beta, Poisson-Gamma, Exponential-Gamma, and Normal-Normal.\nMethods for prior elicitation.\nInference based on the posterior distribution (point and interval estimates; hypotheses testing).\nSimulation-based inference: MCMC methods.\nLinear and generalized linear models from a Bayesian perspective."
  },
  {
    "objectID": "index.html#textbooks",
    "href": "index.html#textbooks",
    "title": "Statistical Inference II",
    "section": "Textbooks",
    "text": "Textbooks\n\nHoff, P. (2009). A first course in Bayesian Statistical Methods. Springer.\nGelman, A., Carlin, J., Stern, H., Dunson, D., Vehtari, A., and Rubin, D. (2013). Bayesian Data Analysis. Chapman & Hall/CRC Texts in Statistical Science.\nRobert, C. and Casella, G. (2004). Monte Carlo Statistical Methods. Springer."
  },
  {
    "objectID": "Script2.html",
    "href": "Script2.html",
    "title": "Script 2 - Normal & Monte Carlo Approximations",
    "section": "",
    "text": "Normal Approximation for the Bernoulli-Beta posterior:\nLet us consider results of a general election, where party A collected 150’000 votes out of the 240’000 total votes.\n\nsum_y &lt;- 150000\nn &lt;- 240000\n\nWe still consider the Bernoulli-Beta model, meaning that \\(Y_i | \\theta \\sim Bernoulli(\\theta)\\) and \\(\\theta \\sim Beta(a, b)\\). Thus, \\[\\theta | \\textbf{y} \\sim Beta\\left(a + \\sum_{i=1}^n y_i, b + n - \\sum_{i=1}^n y_i\\right).\\]\nAs for the choice of the hyperparameters \\(a\\) and \\(b\\), we consider results of a previous general election. In that election, party A collected 49’000 votes out of 270’000 total votes:\n\na &lt;- 49000\nb &lt;- 270000\n\nThe Normal approximation requires the evaluation of the posterior mode. Let’s definte the mode of a Beta distribution.\n\nBeta.Mode &lt;- function(a, b){\n  ifelse(a&gt;1 & b&gt;1, (a-1)/(a+b-2), NA)\n}\n\nThe updated hyperparameters and the posterior mode \\(\\tilde{\\theta}\\) are:\n\na.post &lt;- a + sum_y; a.post\n\n[1] 199000\n\nb.post &lt;- b + n - sum_y; b.post\n\n[1] 360000\n\n\n\npost.mode &lt;- Beta.Mode(a.post, b.post); post.mode\n\n[1] 0.3559923\n\n\nFinally, the variance of the approximated normal distribution, namely the inverse of \\[\\left. -\\frac{\\partial^2 \\log \\pi(\\theta | \\textbf{y})}{\\partial \\theta^2} \\right|_{\\theta = \\tilde{\\theta}}.\\]\n\nsigma_approx &lt;- ((a.post-1)/(post.mode^2) + (b.post-1)/(1-post.mode)^2)^(-1)\n\nsigma_approx\n\n[1] 4.101299e-07\n\n\nLet’s compare the true posterior distribution with its Normal approximation:\n\ncurve(dbeta(x, a.post, b.post), xlim=c(.35,.36), lty = \"dashed\")\ncurve(dnorm(x, post.mode, sqrt(sigma_approx)), add=T, col=\"#D55E00\", lty = \"dotted\")\n\n\n\n\n\n\n\n\nThe Normal approximation is quite useful in computing HPDs, since they coincide with CSs due to the simmetry of the Normal distribution.\nIterative way for computing the true HPD:\n\nh.grid &lt;- seq(100, 105, by = 0.001)\n\nres &lt;- matrix(NA, ncol=4, nrow=length(h.grid))\ncolnames(res) &lt;- c(\"HPD1\", \"HPD2\", \"level\", \"h\")\n\nfor(i in 1:length(h.grid)){\n  translated &lt;- function(x, a, b) dbeta(x,a, b)-h.grid[i]\n\n  hpd1&lt;-uniroot(translated,c(.34,post.mode-.0005),a.post,b.post)$root\n  hpd2&lt;-uniroot(translated,c(post.mode+.0005,.36),a.post,b.post)$root\n\n  I &lt;- integrate(dbeta, lower=hpd1, upper=hpd2, shape1=a.post, shape2=b.post)$value\n\n  iter &lt;- i\n  res[i,] &lt;- c(hpd1, hpd2, I, h.grid[i])\n  if(I &lt;= 0.95) break\n}\n\nres[iter,]\n\n       HPD1        HPD2       level           h \n  0.3547393   0.3572496   0.9499997 102.0680000 \n\nHPD &lt;- res[iter-1,-c(3,4)]; HPD\n\n     HPD1      HPD2 \n0.3547393 0.3572496 \n\n\nHPD by taking advantage of the Normal approximation:\n\nqnorm(c(.025,.975), post.mode, sqrt(sigma_approx))\n\n[1] 0.3547371 0.3572475\n\n\n\n\nMonte Carlo\nWhen it is not possible to compute some quantity of interest directly from the posterior distribution, but it is possible to generate (pseudo)random samples from it, we can use Monte Carlo (MC) approximation methods.\nLet us consider the following example.\nA biostatistician aims to compare two treatments: - 1) a new vaccine for COVID-19; - 2) a placebo.\nHe/She collects a sample of 200 patients and randomly assigns them into two groups of equal size.\n\nn1 &lt;- 100\nn2 &lt;- 100\n\nThe first group receives the new vaccine, while the second group receives the placebo.\nAfter the treatments, the biostatistician records the number of deaths in each group, denoted by \\(Y_1\\) and \\(Y_2\\), respectively.\n\\(Y_1|\\theta_1 \\sim Bernoulli(\\theta_1)\\)\n\\(Y_2| \\theta_2 \\sim Bernoulli(\\theta_2)\\)\n\\(\\theta_j\\) represents the probability of death in group \\(j\\).\nThe biostatistician chooses to use non-informative priors for the parameters.\n\na1 &lt;- 1\nb1 &lt;- 1\n\na2 &lt;- 1\nb2 &lt;- 1\n\nOnce the priors have been selected, he/she looks at the data…\n\ns1 &lt;- 20\ns2 &lt;- 80\n\n… and updates the prior hyperparameters to obtain the posterior distributions.\n\na1.post &lt;- a1 + s1\nb1.post &lt;- b1 + n1 - s1\n\na2.post &lt;- a2 + s2\nb2.post &lt;- b2 + n2 - s2\n\n\ncurve(dbeta(x ,a1.post,b1.post), 0,1, lty=1, ylim=c(0,10),\n      xlab=expression(theta), ylab=expression(pi), lwd=2)\ncurve(dbeta(x , a2.post, b2.post), add=T, lty=3, lwd=2)\nlegend(0.4,10,c(\"Posterior 1\",\"Posterior 2\"),lty=c(1,3),cex=0.8,lwd=2,bty=\"n\")\n\n\n\n\n\n\n\n\nThese posterior distributions suggest that \\(\\theta_1\\) and \\(\\theta_2\\) are concentrated around different values.\nThe biostatistician’s goal is to study the log odds ratio \\[\\eta = \\log \\frac{\\frac{\\theta_1}{1-\\theta_1}}{\\frac{\\theta_2}{1-\\theta_2}}.\\] It is important to note that while we have obtained the posterior distributions of \\(\\theta_1\\) and \\(\\theta_2\\), we have not directly computed the posterior distribution of \\(\\eta\\).\n\nB &lt;- 100000\n\nset.seed(42)\ntheta1 &lt;- rbeta(B, a1.post, b1.post)\ntheta2 &lt;- rbeta(B, a2.post, b2.post)\n\neta &lt;- log(theta1/(1-theta1)*(1-theta2)/theta2)\n\n# Posterior Mean:\nmean(eta)\n\n[1] -2.735187\n\n\n\n# Posterior Variance:\nvar(eta)\n\n[1] 0.1222466\n\n\n\n# CS\nCS &lt;- quantile(eta, p=c(.025,.975)); CS\n\n     2.5%     97.5% \n-3.433526 -2.066745 \n\n\n\n# Histogram and kernel estimate:\n\nhist(eta, prob=T, xlab=expression(eta), main=\"Posterior distribution of log OR\")\nlines(density(eta), col=\"#D55E00\")\n\n\n\n\n\n\n\n\nUsing Monte Carlo sampling, the biostatistician estimates that, with 95% probability, the log odds ratio falls within the interval (-3.433526, -2.0667447).\nThis indicates that the odds of death in the “Vaccine” group are significantly lower than those in the “Placebo” group."
  },
  {
    "objectID": "Script4.html",
    "href": "Script4.html",
    "title": "Script 4 - MCMC - Metropolis Hasting",
    "section": "",
    "text": "Let’s suppose that the posterior is a Gamma(alpha.post, beta.post) and that we cannot compute the normalizing constant (i.e., we know only the kernel).\nWe choose a an Exponential distribution with mean equal to the previous value of the chain as the proposal distribution.\nrm(list = ls())\n\nB &lt;- 1000\nwarmup &lt;- 0.6\n\n# Prior hyperparameters:\na &lt;- 10\nb &lt;- 3\n# Generating data:\nset.seed(42)\nn &lt;- 15\nsum_yi &lt;- sum(rpois(n, 3))\nWe initialize the MCMC with an initial value:\nk &lt;- 0\ntheta &lt;- numeric(B)\ntheta.mean &lt;- numeric(B)\n\ntheta[1] &lt;- 1\ntheta.mean[1] &lt;- theta[1]\nNow, let’s implement the Metropolis Hasting algorithm. The main step here is to compute the probability \\(\\alpha\\left(\\theta^{(b)}, \\theta^*\\right)\\), which has the following expression:\n\\[\\begin{equation*}\n    \\alpha\\left(\\theta^{(b)}, \\theta^*\\right) = = \\begin{cases}\n  \\min\\left(1, \\frac{\\pi(\\theta^*|\\textbf{y})q(\\theta^{(b)}|\\theta^*)}{\\pi(\\theta^{(b)}|\\textbf{y}q(\\theta^{*}|\\theta^{(b)})} \\right)  & \\text{ if } {\\pi(\\theta^{(b)}|\\textbf{y}q(\\theta^{*}|\\theta^{(b)})} \\neq 0 \\\\\n  1 & \\text{ otherwise}\n          \\end{cases}.\n\\end{equation*}\\]\nfor(bb in 2:B){\n  theta_star &lt;- rexp(1, rate=1/theta[bb-1]) \n  \n  alpha_prob &lt;- \n    (theta_star/theta[bb-1])^(a+sum_yi-2)*\n    exp(-(b+n)*(theta_star-theta[bb-1]))*\n    exp(-theta[bb-1]/theta_star + theta_star/theta[bb-1])\n  \n  alpha_prob &lt;- min(alpha_prob, 1)\n  \n  # Acceptance ?\n  if(runif(1) &lt;= alpha_prob){\n    theta[bb] &lt;- theta_star\n    k &lt;- k + 1\n  } else {\n    theta[bb] &lt;- theta[bb-1]\n  }\n  \n  theta.mean[bb] &lt;- mean(theta[2:bb])\n}\n# Acceptance rate:\nk/B\n\n[1] 0.131\nTraceplots:\npar(mfrow=c(1,2))\nplot(theta, type=\"l\", col=\"gray\")\npoints(theta.mean, type=\"l\")\nacf(theta)\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\nWe remove the warm-up period…\nm &lt;- warmup*B\ntheta.new &lt;- theta[m:B]\n… and compare the real and simulated posterior:\nhist(theta.new,prob=T,xlab=expression(theta))\ncurve(dgamma(x, a+sum_yi, rate=b+n),add=T,lwd=2)\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\nThe comparison suggests that the simulated posterior is not a good approximation of the true theoretical one. As an additional red-flag, we note that the acceptance rate is quite small. Thus, to obtain a reliable sample from the stationary distribution, we have to increase the length of the chain and include a thin period:\nB &lt;- 50000\nwarmup &lt;- 0.6\n\nk &lt;- 0\ntheta &lt;- numeric(B)\ntheta.mean &lt;- numeric(B)\n\ntheta[1] &lt;- 1\ntheta.mean[1] &lt;- theta[1]\n\nfor(bb in 2:B){\n  theta_star &lt;- rexp(1, rate=1/theta[bb-1]) \n  \n  alpha_prob &lt;- \n    (theta_star/theta[bb-1])^(a+sum_yi-2)*\n    exp(-(b+n)*(theta_star-theta[bb-1]))*\n    exp(-theta[bb-1]/theta_star + theta_star/theta[bb-1])\n  \n  alpha_prob &lt;- min(alpha_prob,1)\n  \n  if(runif(1) &lt;= alpha_prob){\n    theta[bb] &lt;- theta_star\n    k &lt;- k + 1\n  } else {\n    theta[bb] &lt;- theta[bb-1]\n  }\n  \n  theta.mean[bb] &lt;- mean(theta[2:bb])\n}\nAcceptance rate:\nk/B\n\n[1] 0.1421\npar(mfrow=c(1,2))\nplot(theta, type=\"l\", col=\"gray\")\npoints(theta.mean, type=\"l\")\nacf(theta)\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n# Removing the warm up period:\nm &lt;- round(warmup*B)\nthin &lt;- 10\ntheta.new &lt;- theta[seq(m, B, thin)]\n\n# Comparing the real and simulated posterior:\nhist(theta.new,prob=T,xlab=expression(theta))\ncurve(dgamma(x, a+sum_yi, rate=b+n),add=T,lwd=2)\n\n\n\n\n\n\n\npar(mfrow=c(1,1))"
  },
  {
    "objectID": "Script4.html#homework",
    "href": "Script4.html#homework",
    "title": "Script 4 - MCMC - Metropolis Hasting",
    "section": "HOMEWORK:",
    "text": "HOMEWORK:\nConsider the same scenario of the latter example. Implement a Metropolis-Hasting algorithm by considering an independent MH generating the candidate theta_star from an exponential distribution with mean \\(X\\). Consider different values for \\(X\\) to investigate whether it affects the acceptance rate."
  }
]