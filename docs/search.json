[
  {
    "objectID": "Script5.html",
    "href": "Script5.html",
    "title": "Script 1 - Bernoulli - Beta",
    "section": "",
    "text": "You can download the R script here.\nIn this section, we will explore the Bernoulli-Beta model applied to the cardiac dataset. Specifically, we assume that \\(Y_i | \\theta \\sim Bernoulli(\\theta)\\) and that \\(\\theta \\sim Beta(a, b)\\). With these choices, we showed that the posterior distribution is \\(\\theta | \\textbf{y} \\sim Beta(a + \\sum_{i=1}^n y_i, b + n - \\sum_{i=1}^n y_i)\\).\nWe will examine four different scenarios, each corresponding to a distinct choice of hyperparameters for the Beta prior.\n\ncardiac &lt;- read.table(\"data/cardiac.csv\", header=T, sep=\";\")\nstr(cardiac)\n\n'data.frame':   100 obs. of  2 variables:\n $ Age: int  20 23 24 25 25 26 26 28 28 29 ...\n $ Chd: int  0 0 0 0 1 0 0 0 0 0 ...\n\ny &lt;- cardiac$Chd\ntable(y)\n\ny\n 0  1 \n57 43 \n\nsum(y)\n\n[1] 43\n\nlength(y)\n\n[1] 100\n\n\nThese are thw four scenarios we consider: - A: \\(\\theta \\sim Beta(1,1)\\); - B: \\(\\theta \\sim Beta(10,10)\\); - C: \\(\\theta \\sim Beta(10,5)\\); - D: \\(\\theta \\sim Beta(5,10)\\).\n\npar(mfrow=c(2,2))\ncurve(dbeta(x, 1, 1), main=\"Scenario A\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 10, 10), main=\"Scenario B\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 10, 5), main=\"Scenario C\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 5, 10), main=\"Scenario D\", xlab=expression(theta), ylab=expression(pi(theta)))\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nWe further define the L_binom function, which returns the value of the likelihood function evaluated in a specific theta point.\n\nL_Binom &lt;- function(y, theta){\n  s &lt;- sum(y)\n  n &lt;- length(y)\n  L &lt;- theta^s * (1-theta)^(n-s)\n  return(L)\n}\n\nL_Binom(y, theta=0.001)\n\n[1] 9.445671e-130\n\n\nObviously, the likelihood function does not depend on the prior distribution we impose on \\(\\theta\\). In this case, the MLE corresponds to the sample mean (blue dashed line).\n\ncurve(L_Binom(y, theta=x), main=\"Likelihood\",\n      xlab=expression(theta), lwd=2)\nabline(v=mean(y), col=\"blue\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nScenario A:\nIn this scenario, the prior distribution coincides with a uniform distribution on the \\((0,1)\\) interval. Thus, the prior mean (red dashed line) is equal to 0.5.\n\na1 &lt;- 1; b1 &lt;- 1\n\ncurve(dbeta(x, a1, b1), main=\"Prior A\", ylim=c(0,1.4),\n      xlab=expression(theta), lwd=2)\nabline(v=a1/(a1+b1), col=\"red\", lty=\"dashed\")\n\n\n\n\n\n\n\n\nThe posterior distribution is $| Beta()\n\nn &lt;- length(y)\na1.post &lt;- a1 + sum(y)\nb1.post &lt;- b1 + n - sum(y)\n\ncurve(dbeta(x, a1.post, b1.post), main=\"Scenario A\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a1, b1), lty=\"dashed\", add=T, lwd=2)\nabline(v=a1/(a1+b1), col=\"red\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"green\", lty=\"dotted\", lwd=2)\nabline(v=a1.post/(a1.post+b1.post), col=\"blue\", lty=\"dashed\", lwd=2)\nlegend(.8,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"red\", \"green\", \"blue\"), lty=c(2,1,2,3,2), bty=\"n\")\n\n\n\n\n\n\n\n\n\n\nThe chosen prior distribution does not favor\n\n\nany value of theta.\n\n\nThus, we selected a non-informative prior.\n\n\nThe posterior distribution is centered\n\n\naround the MLE.\n\n\nScenario B:\na2 &lt;- 10; b2 &lt;- 10\n\n\nPrior distribution:\ncurve(dbeta(x, a2, b2), main=“Prior B”, xlab=expression(theta), lwd=2) abline(v=a2/(a2+b2), col=“red”, lty=“dashed”)\n\n\nLikelihood function:\ncurve(L_Binom(y, theta=x), main=“Likelihood”, xlab=expression(theta), lwd=2) abline(v=mean(y), col=“blue”, lty=“dashed”)\n\n\nPosterior distribution:\nn &lt;- length(y) a2.post &lt;- a2 + sum(y) b2.post &lt;- b2 + n - sum(y)\ncurve(dbeta(x, a2.post, b2.post), main=“Scenario B”, xlab=expression(theta), lwd=2) curve(dbeta(x, a2, b2), lty=“dashed”, add=T, lwd=2) abline(v=a2/(a2+b2), col=“red”, lty=“dashed”, lwd=2) abline(v=mean(y), col=“green”, lty=“dotted”, lwd=2) abline(v=a2.post/(a2.post+b2.post), col=“blue”, lty=“dashed”, lwd=2) legend(.8,8, c(“Prior”, “Posterior”, “Prior Mean”, “MLE”, “Post. Mean”), lwd=2, col=c(“black”, “black”, “red”, “green”, “blue”), lty=c(2,1,2,3,2), bty=“n”)\n\n\nNow we are still considering a symmetric prior\n\n\nwith prior mean = 0.5, but it does not\n\n\ngive same density/probability to each\n\n\nvalue/interval of theta.\n\n\nNow the posterior mean is a weighted average\n\n\nbetween prior mean and MLE.\n\n\nScenario C:\na3 &lt;- 10; b3 &lt;- 5\n\n\nPosterior distribution:\nn &lt;- length(y) a3.post &lt;- a3 + sum(y) b3.post &lt;- b3 + n - sum(y)\ncurve(dbeta(x, a3.post, b3.post), main=“Scenario C”, xlab=expression(theta), lwd=2) curve(dbeta(x, a3, b3), lty=“dashed”, add=T, lwd=2) abline(v=a3/(a3+b3), col=“red”, lty=“dashed”, lwd=2) abline(v=mean(y), col=“green”, lty=“dotted”, lwd=2) abline(v=a3.post/(a3.post+b3.post), col=“blue”, lty=“dashed”, lwd=2) legend(.8,8, c(“Prior”, “Posterior”, “Prior Mean”, “MLE”, “Post. Mean”), lwd=2, col=c(“black”, “black”, “red”, “green”, “blue”), lty=c(2,1,2,3,2), bty=“n”)\n\n\nScenario D:\na4 &lt;- 5; b4 &lt;- 10\n\n\nPosterior distribution:\nn &lt;- length(y) a4.post &lt;- a4 + sum(y) b4.post &lt;- b4 + n - sum(y)\ncurve(dbeta(x, a4.post, b4.post), main=“Scenario D”, xlab=expression(theta), lwd=2) curve(dbeta(x, a4, b4), lty=“dashed”, add=T, lwd=2) abline(v=a4/(a4+b4), col=“red”, lty=“dashed”, lwd=2) abline(v=mean(y), col=“green”, lty=“dotted”, lwd=2) abline(v=a4.post/(a4.post+b4.post), col=“blue”, lty=“dashed”, lwd=2) legend(.8,8, c(“Prior”, “Posterior”, “Prior Mean”, “MLE”, “Post. Mean”), lwd=2, col=c(“black”, “black”, “red”, “green”, “blue”), lty=c(2,1,2,3,2), bty=“n”)\n\n\nWe may summarize the (prior and) posterior distribution\n\n\nby some measures:\nBeta.Exp &lt;- function(a, b) a/(a+b) Beta.Var &lt;- function(a, b)(ab)/((a+b)^2(a+b+1)) Beta.Mode &lt;- function(a, b){ ifelse(a&gt;1 & b&gt;1, (a-1)/(a+b-2), NA) } Beta.Median &lt;- function(a, b) qbeta(.5, a, b)\nmeasures &lt;- matrix(NA, ncol=6, nrow=4) rownames(measures) &lt;- c(“A”, “B”, “C”, “D”) colnames(measures) &lt;- c(“a.post”, “b.post”, “Exp”, “Mode”, “Median”, “Var”)\nmeasures[,1] &lt;- c(a1.post,a2.post,a3.post,a4.post) measures[,2] &lt;- c(b1.post,b2.post,b3.post,b4.post) for(i in 1:4){ a &lt;- measures[i,1] b &lt;- measures[i,2] measures[i,3] &lt;- Beta.Exp(a, b) measures[i,4] &lt;- Beta.Mode(a, b) measures[i,5] &lt;- Beta.Median(a, b) measures[i,6] &lt;- Beta.Var(a, b) }\nround(measures,4)\n\n\nIn this simple scenario, even considering\n\n\ndifferent priors, we do not have posteriors\n\n\nleading to very different point estimates.\n\n\nThis is because the empirical evidence\n\n\n(i.e., our data) is much\n\n\nstronger than our prior belief.\n\n\nLet us consider an additional scenario where\n\n\nwe strongly believe that most people\n\n\ndo not have cardiovascular disease.\na5 &lt;- 100 b5 &lt;- 1000\n\n\nPrior distribution:\ncurve(dbeta(x, a5, b5), main=“Prior E”, xlab=expression(theta)) abline(v=a5/(a5+b5), col=“red”, lty=“dashed”)\n\n\nPosterior distribution:\nn &lt;- length(y) a5.post &lt;- a5 + sum(y) b5.post &lt;- b5 + n - sum(y)\ncurve(dbeta(x, a5.post, b5.post), main=“Scenario E”, xlab=expression(theta), lwd=2) curve(dbeta(x, a5, b5), lty=“dashed”, add=T, lwd=2) abline(v=a5/(a5+b5), col=“red”, lty=“dashed”, lwd=2) abline(v=mean(y), col=“green”, lty=“dotted”, lwd=2) abline(v=a5.post/(a5.post+b5.post), col=“blue”, lty=“dashed”, lwd=2) legend(.8,40, c(“Prior”, “Posterior”, “Prior Mean”, “MLE”, “Post. Mean”), lwd=2, col=c(“black”, “black”, “red”, “green”, “blue”), lty=c(2,1,2,3,2), bty=“n”)\nmeasures &lt;- matrix(NA, ncol=6, nrow=5) rownames(measures) &lt;- c(“A”, “B”, “C”, “D”, “E”) colnames(measures) &lt;- c(“a.post”, “b.post”, “Exp”, “Mode”, “Median”, “Var”)\nmeasures[,1] &lt;- c(a1.post,a2.post,a3.post,a4.post,a5.post) measures[,2] &lt;- c(b1.post,b2.post,b3.post,b4.post,b5.post) for(i in 1:5){ a &lt;- measures[i,1] b &lt;- measures[i,2] measures[i,3] &lt;- Beta.Exp(a, b) measures[i,4] &lt;- Beta.Mode(a, b) measures[i,5] &lt;- Beta.Median(a, b) measures[i,6] &lt;- Beta.Var(a, b) }\nround(measures,4)\n\n\nInterval Estimates:\n\n\nCredible Sets (CS).\n\n\nThe CS are very easy to compute. Indeed, we only have to compute\n\n\nquantiles of the posterior distribution.\n\n\nLet us consider scenario D:\na &lt;- a4.post b &lt;- b4.post\n\n\nScenario D:\ncurve(dbeta(x, a, b), main=“Scenario D”, xlab=expression(theta), lwd=2)\n\n\nCS of level 0.95\nCS &lt;- qbeta(c(0.025,0.975), a, b);CS abline(v=CS,lty=“dashed”, col=“blue”) legend(0.8,8,c(“Posterior”, “Credible Set”), lty=c(1,2), col=c(“black”, “blue”), bty=“n”)\n\n\nHighest posterior density (HPD):\n\n\nLet’s start by considering a random value for h\nh &lt;- 2 curve(dbeta(x, a, b), ylab=expression(paste(pi,“(”,theta,“|x)”)), xlab=expression(theta)) abline(h=h,lty=2)\n\n\nWe need to find the values of theta such that p(theta|y) = h\ntranslated &lt;- function(x, a, b) dbeta(x,a, b)-h # We decrease the density curve by h. # Now, the values we are looking for are such that translated = 0\ncurve(translated(x, a, b), add=T,lty=3,lwd=2) abline(h=0,lty=3)\nhpd1 &lt;- uniroot(translated,c(.2, .4),a,b)\\(root;hpd1\nhpd2 &lt;- uniroot(translated,c(.45, .5),a,b)\\)root;hpd2 integrate(dbeta, lower=hpd1, upper=hpd2, shape1=a, shape2=b)\n\n\nWe have a probability equal to 0.9152 –&gt; Decrease h\n\n\nIterative way:\nh.grid &lt;- seq(1, 2, by = 0.01)\nres &lt;- matrix(NA, ncol=4, nrow=length(h.grid)) colnames(res) &lt;- c(“HPD1”, “HPD2”, “level”, “h”)\nfor(i in 1:length(h.grid)){\ntranslated &lt;- function(x, a, b) dbeta(x,a, b)-h.grid[i]\nhpd1&lt;-uniroot(translated,c(.2,.4),a,b)\\(root\n  hpd2&lt;-uniroot(translated,c(.43,.55),a,b)\\)root\nI &lt;- integrate(dbeta, lower=hpd1, upper=hpd2, shape1=a, shape2=b)$value\niter &lt;- i res[i,] &lt;- c(hpd1, hpd2, I, h.grid[i]) if(I &lt;= 0.95) break }\nres[1:iter,] h.grid[iter]\nHPD &lt;- res[iter-1,-c(3,4)]; HPD CS\n\n\nGraphically:\ncurve(dbeta(x, a, b), main=“Scenario D”, xlab=expression(theta), lwd=2)\nabline(v=CS,lty=“dashed”, col=“blue”) abline(v=HPD,lty=“dashed”, col=“red”)\nlegend(0.8,8,c(“Posterior”, “Credible Set”, “HPD”), lty=c(1,2,2), col=c(“black”, “blue”, “red”), bty=“n”)\n\n\nHPD and CS are very similar, because the posterior is\n\n\nunimodal and almost symmetrical.\n\n\nHypothesis testing:\n\n\nLet consider H0: theta &lt;= 0.3 vs H1: theta &gt; 0.3\npbeta(.3, a4, b4) pbeta(.3, a4.post, b4.post)\nODDS.prior &lt;- pbeta(.3, a4, b4)/(1-pbeta(.3, a4, b4)); ODDS.prior ODDS.post &lt;- pbeta(.3, a4.post, b4.post)/(1-pbeta(.3, a4.post, b4.post)); ODDS.post\nBayes_Factor &lt;- ODDS.post/ODDS.prior; Bayes_Factor\n\n\nData do not support H0: by adding data, our confidence on\n\n\nH0 strongly decreases\n\n\nHOMEWORK: consider a sample of size n = 10 composed of\n\n\nBernoulli trials, for which we observed s = 3 successes.\n\n\nConsider that a priori theta ~ Beta(2, 8)\n\n\n1. Which is the posterior distribution of theta?\n\n\n2. Compute the MLE and the prior and posterior means.\n\n\nAre these results reasonable?\n\n\n3. Compute and compare the CS and the HPD.\n\n\n4. Let H0 : theta in (0.3, 0.5)\n\n\nH1 : theta in (0, 0.3) U (0.5, 1)\n\n\nWhich hypothesis can you support?"
  },
  {
    "objectID": "Script3.html",
    "href": "Script3.html",
    "title": "Script 1 - Bernoulli - Beta",
    "section": "",
    "text": "You can download the R script here.\nIn this section, we will explore the Bernoulli-Beta model applied to the cardiac dataset. Specifically, we assume that \\(Y_i | \\theta \\sim Bernoulli(\\theta)\\) and that \\(\\theta \\sim Beta(a, b)\\). With these choices, we showed that the posterior distribution is \\(\\theta | \\textbf{y} \\sim Beta(a + \\sum_{i=1}^n y_i, b + n - \\sum_{i=1}^n y_i)\\).\nWe will examine four different scenarios, each corresponding to a distinct choice of hyperparameters for the Beta prior.\n\ncardiac &lt;- read.table(\"data/cardiac.csv\", header=T, sep=\";\")\nstr(cardiac)\n\n'data.frame':   100 obs. of  2 variables:\n $ Age: int  20 23 24 25 25 26 26 28 28 29 ...\n $ Chd: int  0 0 0 0 1 0 0 0 0 0 ...\n\ny &lt;- cardiac$Chd\ntable(y)\n\ny\n 0  1 \n57 43 \n\nsum(y)\n\n[1] 43\n\nlength(y)\n\n[1] 100\n\n\nThese are thw four scenarios we consider: - A: \\(\\theta \\sim Beta(1,1)\\); - B: \\(\\theta \\sim Beta(10,10)\\); - C: \\(\\theta \\sim Beta(10,5)\\); - D: \\(\\theta \\sim Beta(5,10)\\).\n\npar(mfrow=c(2,2))\ncurve(dbeta(x, 1, 1), main=\"Scenario A\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 10, 10), main=\"Scenario B\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 10, 5), main=\"Scenario C\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 5, 10), main=\"Scenario D\", xlab=expression(theta), ylab=expression(pi(theta)))\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nWe further define the L_binom function, which returns the value of the likelihood function evaluated in a specific theta point.\n\nL_Binom &lt;- function(y, theta){\n  s &lt;- sum(y)\n  n &lt;- length(y)\n  L &lt;- theta^s * (1-theta)^(n-s)\n  return(L)\n}\n\nL_Binom(y, theta=0.001)\n\n[1] 9.445671e-130\n\n\nObviously, the likelihood function does not depend on the prior distribution we impose on \\(\\theta\\). In this case, the MLE corresponds to the sample mean (blue dashed line).\n\ncurve(L_Binom(y, theta=x), main=\"Likelihood\",\n      xlab=expression(theta), lwd=2)\nabline(v=mean(y), col=\"blue\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nScenario A:\nIn this scenario, the prior distribution coincides with a uniform distribution on the \\((0,1)\\) interval. Thus, the prior mean (red dashed line) is equal to 0.5.\n\na1 &lt;- 1; b1 &lt;- 1\n\ncurve(dbeta(x, a1, b1), main=\"Prior A\", ylim=c(0,1.4),\n      xlab=expression(theta), lwd=2)\nabline(v=a1/(a1+b1), col=\"red\", lty=\"dashed\")\n\n\n\n\n\n\n\n\nThe posterior distribution is $| Beta()\n\nn &lt;- length(y)\na1.post &lt;- a1 + sum(y)\nb1.post &lt;- b1 + n - sum(y)\n\ncurve(dbeta(x, a1.post, b1.post), main=\"Scenario A\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a1, b1), lty=\"dashed\", add=T, lwd=2)\nabline(v=a1/(a1+b1), col=\"red\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"green\", lty=\"dotted\", lwd=2)\nabline(v=a1.post/(a1.post+b1.post), col=\"blue\", lty=\"dashed\", lwd=2)\nlegend(.8,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"red\", \"green\", \"blue\"), lty=c(2,1,2,3,2), bty=\"n\")\n\n\n\n\n\n\n\n\n\n\nThe chosen prior distribution does not favor\n\n\nany value of theta.\n\n\nThus, we selected a non-informative prior.\n\n\nThe posterior distribution is centered\n\n\naround the MLE.\n\n\nScenario B:\na2 &lt;- 10; b2 &lt;- 10\n\n\nPrior distribution:\ncurve(dbeta(x, a2, b2), main=“Prior B”, xlab=expression(theta), lwd=2) abline(v=a2/(a2+b2), col=“red”, lty=“dashed”)\n\n\nLikelihood function:\ncurve(L_Binom(y, theta=x), main=“Likelihood”, xlab=expression(theta), lwd=2) abline(v=mean(y), col=“blue”, lty=“dashed”)\n\n\nPosterior distribution:\nn &lt;- length(y) a2.post &lt;- a2 + sum(y) b2.post &lt;- b2 + n - sum(y)\ncurve(dbeta(x, a2.post, b2.post), main=“Scenario B”, xlab=expression(theta), lwd=2) curve(dbeta(x, a2, b2), lty=“dashed”, add=T, lwd=2) abline(v=a2/(a2+b2), col=“red”, lty=“dashed”, lwd=2) abline(v=mean(y), col=“green”, lty=“dotted”, lwd=2) abline(v=a2.post/(a2.post+b2.post), col=“blue”, lty=“dashed”, lwd=2) legend(.8,8, c(“Prior”, “Posterior”, “Prior Mean”, “MLE”, “Post. Mean”), lwd=2, col=c(“black”, “black”, “red”, “green”, “blue”), lty=c(2,1,2,3,2), bty=“n”)\n\n\nNow we are still considering a symmetric prior\n\n\nwith prior mean = 0.5, but it does not\n\n\ngive same density/probability to each\n\n\nvalue/interval of theta.\n\n\nNow the posterior mean is a weighted average\n\n\nbetween prior mean and MLE.\n\n\nScenario C:\na3 &lt;- 10; b3 &lt;- 5\n\n\nPosterior distribution:\nn &lt;- length(y) a3.post &lt;- a3 + sum(y) b3.post &lt;- b3 + n - sum(y)\ncurve(dbeta(x, a3.post, b3.post), main=“Scenario C”, xlab=expression(theta), lwd=2) curve(dbeta(x, a3, b3), lty=“dashed”, add=T, lwd=2) abline(v=a3/(a3+b3), col=“red”, lty=“dashed”, lwd=2) abline(v=mean(y), col=“green”, lty=“dotted”, lwd=2) abline(v=a3.post/(a3.post+b3.post), col=“blue”, lty=“dashed”, lwd=2) legend(.8,8, c(“Prior”, “Posterior”, “Prior Mean”, “MLE”, “Post. Mean”), lwd=2, col=c(“black”, “black”, “red”, “green”, “blue”), lty=c(2,1,2,3,2), bty=“n”)\n\n\nScenario D:\na4 &lt;- 5; b4 &lt;- 10\n\n\nPosterior distribution:\nn &lt;- length(y) a4.post &lt;- a4 + sum(y) b4.post &lt;- b4 + n - sum(y)\ncurve(dbeta(x, a4.post, b4.post), main=“Scenario D”, xlab=expression(theta), lwd=2) curve(dbeta(x, a4, b4), lty=“dashed”, add=T, lwd=2) abline(v=a4/(a4+b4), col=“red”, lty=“dashed”, lwd=2) abline(v=mean(y), col=“green”, lty=“dotted”, lwd=2) abline(v=a4.post/(a4.post+b4.post), col=“blue”, lty=“dashed”, lwd=2) legend(.8,8, c(“Prior”, “Posterior”, “Prior Mean”, “MLE”, “Post. Mean”), lwd=2, col=c(“black”, “black”, “red”, “green”, “blue”), lty=c(2,1,2,3,2), bty=“n”)\n\n\nWe may summarize the (prior and) posterior distribution\n\n\nby some measures:\nBeta.Exp &lt;- function(a, b) a/(a+b) Beta.Var &lt;- function(a, b)(ab)/((a+b)^2(a+b+1)) Beta.Mode &lt;- function(a, b){ ifelse(a&gt;1 & b&gt;1, (a-1)/(a+b-2), NA) } Beta.Median &lt;- function(a, b) qbeta(.5, a, b)\nmeasures &lt;- matrix(NA, ncol=6, nrow=4) rownames(measures) &lt;- c(“A”, “B”, “C”, “D”) colnames(measures) &lt;- c(“a.post”, “b.post”, “Exp”, “Mode”, “Median”, “Var”)\nmeasures[,1] &lt;- c(a1.post,a2.post,a3.post,a4.post) measures[,2] &lt;- c(b1.post,b2.post,b3.post,b4.post) for(i in 1:4){ a &lt;- measures[i,1] b &lt;- measures[i,2] measures[i,3] &lt;- Beta.Exp(a, b) measures[i,4] &lt;- Beta.Mode(a, b) measures[i,5] &lt;- Beta.Median(a, b) measures[i,6] &lt;- Beta.Var(a, b) }\nround(measures,4)\n\n\nIn this simple scenario, even considering\n\n\ndifferent priors, we do not have posteriors\n\n\nleading to very different point estimates.\n\n\nThis is because the empirical evidence\n\n\n(i.e., our data) is much\n\n\nstronger than our prior belief.\n\n\nLet us consider an additional scenario where\n\n\nwe strongly believe that most people\n\n\ndo not have cardiovascular disease.\na5 &lt;- 100 b5 &lt;- 1000\n\n\nPrior distribution:\ncurve(dbeta(x, a5, b5), main=“Prior E”, xlab=expression(theta)) abline(v=a5/(a5+b5), col=“red”, lty=“dashed”)\n\n\nPosterior distribution:\nn &lt;- length(y) a5.post &lt;- a5 + sum(y) b5.post &lt;- b5 + n - sum(y)\ncurve(dbeta(x, a5.post, b5.post), main=“Scenario E”, xlab=expression(theta), lwd=2) curve(dbeta(x, a5, b5), lty=“dashed”, add=T, lwd=2) abline(v=a5/(a5+b5), col=“red”, lty=“dashed”, lwd=2) abline(v=mean(y), col=“green”, lty=“dotted”, lwd=2) abline(v=a5.post/(a5.post+b5.post), col=“blue”, lty=“dashed”, lwd=2) legend(.8,40, c(“Prior”, “Posterior”, “Prior Mean”, “MLE”, “Post. Mean”), lwd=2, col=c(“black”, “black”, “red”, “green”, “blue”), lty=c(2,1,2,3,2), bty=“n”)\nmeasures &lt;- matrix(NA, ncol=6, nrow=5) rownames(measures) &lt;- c(“A”, “B”, “C”, “D”, “E”) colnames(measures) &lt;- c(“a.post”, “b.post”, “Exp”, “Mode”, “Median”, “Var”)\nmeasures[,1] &lt;- c(a1.post,a2.post,a3.post,a4.post,a5.post) measures[,2] &lt;- c(b1.post,b2.post,b3.post,b4.post,b5.post) for(i in 1:5){ a &lt;- measures[i,1] b &lt;- measures[i,2] measures[i,3] &lt;- Beta.Exp(a, b) measures[i,4] &lt;- Beta.Mode(a, b) measures[i,5] &lt;- Beta.Median(a, b) measures[i,6] &lt;- Beta.Var(a, b) }\nround(measures,4)\n\n\nInterval Estimates:\n\n\nCredible Sets (CS).\n\n\nThe CS are very easy to compute. Indeed, we only have to compute\n\n\nquantiles of the posterior distribution.\n\n\nLet us consider scenario D:\na &lt;- a4.post b &lt;- b4.post\n\n\nScenario D:\ncurve(dbeta(x, a, b), main=“Scenario D”, xlab=expression(theta), lwd=2)\n\n\nCS of level 0.95\nCS &lt;- qbeta(c(0.025,0.975), a, b);CS abline(v=CS,lty=“dashed”, col=“blue”) legend(0.8,8,c(“Posterior”, “Credible Set”), lty=c(1,2), col=c(“black”, “blue”), bty=“n”)\n\n\nHighest posterior density (HPD):\n\n\nLet’s start by considering a random value for h\nh &lt;- 2 curve(dbeta(x, a, b), ylab=expression(paste(pi,“(”,theta,“|x)”)), xlab=expression(theta)) abline(h=h,lty=2)\n\n\nWe need to find the values of theta such that p(theta|y) = h\ntranslated &lt;- function(x, a, b) dbeta(x,a, b)-h # We decrease the density curve by h. # Now, the values we are looking for are such that translated = 0\ncurve(translated(x, a, b), add=T,lty=3,lwd=2) abline(h=0,lty=3)\nhpd1 &lt;- uniroot(translated,c(.2, .4),a,b)\\(root;hpd1\nhpd2 &lt;- uniroot(translated,c(.45, .5),a,b)\\)root;hpd2 integrate(dbeta, lower=hpd1, upper=hpd2, shape1=a, shape2=b)\n\n\nWe have a probability equal to 0.9152 –&gt; Decrease h\n\n\nIterative way:\nh.grid &lt;- seq(1, 2, by = 0.01)\nres &lt;- matrix(NA, ncol=4, nrow=length(h.grid)) colnames(res) &lt;- c(“HPD1”, “HPD2”, “level”, “h”)\nfor(i in 1:length(h.grid)){\ntranslated &lt;- function(x, a, b) dbeta(x,a, b)-h.grid[i]\nhpd1&lt;-uniroot(translated,c(.2,.4),a,b)\\(root\n  hpd2&lt;-uniroot(translated,c(.43,.55),a,b)\\)root\nI &lt;- integrate(dbeta, lower=hpd1, upper=hpd2, shape1=a, shape2=b)$value\niter &lt;- i res[i,] &lt;- c(hpd1, hpd2, I, h.grid[i]) if(I &lt;= 0.95) break }\nres[1:iter,] h.grid[iter]\nHPD &lt;- res[iter-1,-c(3,4)]; HPD CS\n\n\nGraphically:\ncurve(dbeta(x, a, b), main=“Scenario D”, xlab=expression(theta), lwd=2)\nabline(v=CS,lty=“dashed”, col=“blue”) abline(v=HPD,lty=“dashed”, col=“red”)\nlegend(0.8,8,c(“Posterior”, “Credible Set”, “HPD”), lty=c(1,2,2), col=c(“black”, “blue”, “red”), bty=“n”)\n\n\nHPD and CS are very similar, because the posterior is\n\n\nunimodal and almost symmetrical.\n\n\nHypothesis testing:\n\n\nLet consider H0: theta &lt;= 0.3 vs H1: theta &gt; 0.3\npbeta(.3, a4, b4) pbeta(.3, a4.post, b4.post)\nODDS.prior &lt;- pbeta(.3, a4, b4)/(1-pbeta(.3, a4, b4)); ODDS.prior ODDS.post &lt;- pbeta(.3, a4.post, b4.post)/(1-pbeta(.3, a4.post, b4.post)); ODDS.post\nBayes_Factor &lt;- ODDS.post/ODDS.prior; Bayes_Factor\n\n\nData do not support H0: by adding data, our confidence on\n\n\nH0 strongly decreases\n\n\nHOMEWORK: consider a sample of size n = 10 composed of\n\n\nBernoulli trials, for which we observed s = 3 successes.\n\n\nConsider that a priori theta ~ Beta(2, 8)\n\n\n1. Which is the posterior distribution of theta?\n\n\n2. Compute the MLE and the prior and posterior means.\n\n\nAre these results reasonable?\n\n\n3. Compute and compare the CS and the HPD.\n\n\n4. Let H0 : theta in (0.3, 0.5)\n\n\nH1 : theta in (0, 0.3) U (0.5, 1)\n\n\nWhich hypothesis can you support?"
  },
  {
    "objectID": "Script1.html",
    "href": "Script1.html",
    "title": "Script 1 - Bernoulli - Beta",
    "section": "",
    "text": "In this section, we will explore the Bernoulli-Beta model applied to the cardiac dataset. Specifically, we assume that \\(Y_i | \\theta \\sim Bernoulli(\\theta)\\) and that \\(\\theta \\sim Beta(a, b)\\). With these choices, we showed that the posterior distribution is \\[\\theta | \\textbf{y} \\sim Beta\\left(a + \\sum_{i=1}^n y_i, b + n - \\sum_{i=1}^n y_i\\right).\\]\nWe will examine four different scenarios, each corresponding to a distinct choice of hyperparameters for the Beta prior.\ncardiac &lt;- read.table(\"data/cardiac.csv\", header=T, sep=\";\")\nstr(cardiac)\n\n'data.frame':   100 obs. of  2 variables:\n $ Age: int  20 23 24 25 25 26 26 28 28 29 ...\n $ Chd: int  0 0 0 0 1 0 0 0 0 0 ...\n\ny &lt;- cardiac$Chd\ntable(y)\n\ny\n 0  1 \n57 43 \n\nsum(y)\n\n[1] 43\ncardiac &lt;- read.table(\"data/cardiac.csv\", header=T, sep=\";\")\nstr(cardiac)\n\n'data.frame':   100 obs. of  2 variables:\n $ Age: int  20 23 24 25 25 26 26 28 28 29 ...\n $ Chd: int  0 0 0 0 1 0 0 0 0 0 ...\n\ny &lt;- cardiac$Chd\ntable(y)\n\ny\n 0  1 \n57 43 \n\nsum(y)\n\n[1] 43\n\nlength(y)\n\n[1] 100\nsum(y)\n\n[1] 43\nlength(y)\n\n[1] 100\nThese are the four scenarios we consider: - A: \\(\\theta \\sim Beta(1,1)\\); - B: \\(\\theta \\sim Beta(10,10)\\); - C: \\(\\theta \\sim Beta(10,5)\\); - D: \\(\\theta \\sim Beta(5,10)\\).\npar(mfrow=c(2,2))\ncurve(dbeta(x, 1, 1), main=\"Scenario A\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 10, 10), main=\"Scenario B\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 10, 5), main=\"Scenario C\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 5, 10), main=\"Scenario D\", xlab=expression(theta), ylab=expression(pi(theta)))\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\nWe further define the L_binom function, which returns the value of the likelihood function evaluated in a specific theta point.\nL_Binom &lt;- function(y, theta){\n  s &lt;- sum(y)\n  n &lt;- length(y)\n  L &lt;- theta^s * (1-theta)^(n-s)\n  return(L)\n}\n\nL_Binom(y, theta=0.001)\n\n[1] 9.445671e-130\nObviously, the likelihood function does not depend on the prior distribution we impose on \\(\\theta\\). In this case, the MLE corresponds to the sample mean (blue dashed line).\ncurve(L_Binom(y, theta=x), main=\"Likelihood\",\n      xlab=expression(theta), lwd=2)\nabline(v=mean(y), col=\"blue\", lty=\"dashed\")"
  },
  {
    "objectID": "Script1.html#interval-estimates",
    "href": "Script1.html#interval-estimates",
    "title": "Script 1 - Bernoulli - Beta",
    "section": "Interval Estimates:",
    "text": "Interval Estimates:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Inference II",
    "section": "",
    "text": "Statistical Inference II provides an introduction to Bayesian data analysis, covering prior and posterior distributions, classical one-parameter models, prior elicitation, posterior-based inference, MCMC methods, and Bayesian approaches to linear and generalized linear models."
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Statistical Inference II",
    "section": "Syllabus",
    "text": "Syllabus\n\nIntroduction to Bayesian data analysis: prior and posterior distributions for inference.\nOne-parameter models: Binomial-Beta, Poisson-Gamma, Exponential-Gamma, and Normal-Normal.\nMethods for prior elicitation.\nInference based on the posterior distribution (point and interval estimates; hypotheses testing).\nSimulation-based inference: MCMC methods.\nLinear and generalized linear models from a Bayesian perspective."
  },
  {
    "objectID": "index.html#textbooks",
    "href": "index.html#textbooks",
    "title": "Statistical Inference II",
    "section": "Textbooks",
    "text": "Textbooks\n\nHoff, P. (2009). A first course in Bayesian Statistical Methods. Springer.\nGelman, A., Carlin, J., Stern, H., Dunson, D., Vehtari, A., and Rubin, D. (2013). Bayesian Data Analysis. Chapman & Hall/CRC Texts in Statistical Science.\nRobert, C. and Casella, G. (2004). Monte Carlo Statistical Methods. Springer."
  },
  {
    "objectID": "Script2.html",
    "href": "Script2.html",
    "title": "Script 2 - Normal Approximation & Monte Carlo",
    "section": "",
    "text": "Normal Approximation for the Bernoulli-Beta posterior:\nLet us consider results of a general election, where party A collected 150’000 votes out of the 240’000 total votes.\n\nsum_y &lt;- 150000\nn &lt;- 240000\n\nWe still consider the Bernoulli-Beta model, meaning that \\(Y_i | \\theta \\sim Bernoulli(\\theta)\\) and \\(\\theta \\sim Beta(a, b)\\). Thus, \\[\\theta | \\textbf{y} \\sim Beta\\left(a + \\sum_{i=1}^n y_i, b + n - \\sum_{i=1}^n y_i\\right).\\]\nAs for the choice of the hyperparameters \\(a\\) and \\(b\\), we consider results of a previous general election. In that election, party A collected 49’000 votes out of 270’000 total votes:\n\na &lt;- 49000\nb &lt;- 270000\n\nThe Normal approximation requires the evaluation of the posterior mode. Let’s definte the mode of a Beta distribution.\n\nBeta.Mode &lt;- function(a, b){\n  ifelse(a&gt;1 & b&gt;1, (a-1)/(a+b-2), NA)\n}\n\nThe updated hyperparameters and the posterior mode \\(\\tilde{\\theta}\\) are:\n\na.post &lt;- a + sum_y; a.post\n\n[1] 199000\n\nb.post &lt;- b + n - sum_y; b.post\n\n[1] 360000\n\n\n\npost.mode &lt;- Beta.Mode(a.post, b.post); post.mode\n\n[1] 0.3559923\n\n\nFinally, the variance of the approximated normal distribution, namely the inverse of \\(\\left. -\\frac{\\partial^2 \\log \\pi(\\theta | \\textbf{y})}{\\partial \\theta^2} \\right|_{\\theta = \\tilde{\\theta}}\\).\n\nsigma_approx &lt;- ((a.post-1)/(post.mode^2) + (b.post-1)/(1-post.mode)^2)^(-1)\n\nsigma_approx\n\n[1] 4.101299e-07\n\n\nLet’s compare the true posterior distribution with its Normal approximation:\n\ncurve(dbeta(x, a.post, b.post), xlim=c(.35,.36), lty = \"dashed\")\ncurve(dnorm(x, post.mode, sqrt(sigma_approx)), add=T, col=\"red\", lty = \"dotted\")\n\n\n\n\n\n\n\n\nThe Normal approximation is quite useful in computing HPDs, since they coincide with CSs due to the simmetry of the Normal distribution.\nIterative way for computing the true HPD:\n\nh.grid &lt;- seq(100, 105, by = 0.001)\n\nres &lt;- matrix(NA, ncol=4, nrow=length(h.grid))\ncolnames(res) &lt;- c(\"HPD1\", \"HPD2\", \"level\", \"h\")\n\nfor(i in 1:length(h.grid)){\n  translated &lt;- function(x, a, b) dbeta(x,a, b)-h.grid[i]\n\n  hpd1&lt;-uniroot(translated,c(.34,post.mode-.0005),a.post,b.post)$root\n  hpd2&lt;-uniroot(translated,c(post.mode+.0005,.36),a.post,b.post)$root\n\n  I &lt;- integrate(dbeta, lower=hpd1, upper=hpd2, shape1=a.post, shape2=b.post)$value\n\n  iter &lt;- i\n  res[i,] &lt;- c(hpd1, hpd2, I, h.grid[i])\n  if(I &lt;= 0.95) break\n}\n\nres[iter,]\n\n       HPD1        HPD2       level           h \n  0.3547393   0.3572496   0.9499997 102.0680000 \n\nHPD &lt;- res[iter-1,-c(3,4)]; HPD\n\n     HPD1      HPD2 \n0.3547393 0.3572496 \n\n\nHPD by taking advantage of the Normal approximation:\n\nqnorm(c(.025,.975), post.mode, sqrt(sigma_approx))\n\n[1] 0.3547371 0.3572475\n\n\n\n\nMonte Carlo\nWhen it is not possible to compute some quantity of interest directly from the posterior distribution, but it is possible to generate (pseudo)random samples from it, we can use Monte Carlo (MC) approximation methods.\nLet us consider the following example.\nA biostatistician aims to compare two treatments: - 1) a new vaccine for COVID-19; - 2) a placebo.\nHe/She collects a sample of 200 patients and randomly assigns them into two groups of equal size.\n\nn1 &lt;- 100\nn2 &lt;- 100\n\nThe first group receives the new vaccine, while the second group receives the placebo.\nAfter the treatments, the biostatistician records the number of deaths in each group, denoted by \\(Y_1\\) and \\(Y_2\\), respectively.\n\\(Y_1|\\theta_1 \\sim Bernoulli(\\theta_1)\\)\n\\(Y_2| \\theta_2 \\sim Bernoulli(\\theta_2)\\)\n\\(\\theta_j\\) represents the probability of death in group \\(j\\).\nThe biostatistician chooses to use non-informative priors for the parameters.\n\na1 &lt;- 1\nb1 &lt;- 1\n\na2 &lt;- 1\nb2 &lt;- 1\n\nOnce the priors have been selected, he/she looks at the data…\n\ns1 &lt;- 20\ns2 &lt;- 80\n\n… and updates the prior hyperparameters to obtain the posterior distributions.\n\na1.post &lt;- a1 + s1\nb1.post &lt;- b1 + n1 - s1\n\na2.post &lt;- a2 + s2\nb2.post &lt;- b2 + n2 - s2\n\n\ncurve(dbeta(x ,a1.post,b1.post), 0,1, lty=1, ylim=c(0,10),\n      xlab=expression(theta), ylab=expression(pi), lwd=2)\ncurve(dbeta(x , a2.post, b2.post), add=T, lty=3, lwd=2)\nlegend(0.4,10,c(\"Posterior 1\",\"Posterior 2\"),lty=c(1,3),cex=0.8,lwd=2,bty=\"n\")\n\n\n\n\n\n\n\n\nThese posterior distributions suggest that \\(\\theta_1\\) and \\(\\theta_2\\) are concentrated around different values.\nThe biostatistician’s goal is to study the log odds ratio \\[\\eta = \\log \\frac{\\frac{\\theta_1}{1-\\theta_1}}{\\frac{\\theta_2}{1-\\theta_2}}.\\] It is important to note that while we have obtained the posterior distributions of \\(\\theta_1\\) and \\(\\theta_2\\), we have not directly computed the posterior distribution of \\(\\eta\\).\n\nB &lt;- 100000\n\nset.seed(42)\ntheta1 &lt;- rbeta(B, a1.post, b1.post)\ntheta2 &lt;- rbeta(B, a2.post, b2.post)\n\neta &lt;- log(theta1/(1-theta1)*(1-theta2)/theta2)\n\n# Posterior Mean:\nmean(eta)\n\n[1] -2.735187\n\n\n\n# Posterior Variance:\nvar(eta)\n\n[1] 0.1222466\n\n\n\n# CS\nCS &lt;- quantile(eta, p=c(.025,.975)); CS\n\n     2.5%     97.5% \n-3.433526 -2.066745 \n\n\n\n# Histogram and kernel estimate:\n\nhist(eta, prob=T, xlab=expression(eta), main=\"Posterior distribution of log OR\")\nlines(density(eta), col=\"red\")\n\n\n\n\n\n\n\n\nUsing Monte Carlo sampling, the biostatistician estimates that, with 95% probability, the log odds ratio falls within the interval (-3.433526, -2.0667447).\nThis indicates that the odds of death in the “Vaccine” group are significantly lower than those in the “Placebo” group."
  },
  {
    "objectID": "Script4.html",
    "href": "Script4.html",
    "title": "Script 1 - Bernoulli - Beta",
    "section": "",
    "text": "You can download the R script here.\nIn this section, we will explore the Bernoulli-Beta model applied to the cardiac dataset. Specifically, we assume that \\(Y_i | \\theta \\sim Bernoulli(\\theta)\\) and that \\(\\theta \\sim Beta(a, b)\\). With these choices, we showed that the posterior distribution is \\(\\theta | \\textbf{y} \\sim Beta(a + \\sum_{i=1}^n y_i, b + n - \\sum_{i=1}^n y_i)\\).\nWe will examine four different scenarios, each corresponding to a distinct choice of hyperparameters for the Beta prior.\n\ncardiac &lt;- read.table(\"data/cardiac.csv\", header=T, sep=\";\")\nstr(cardiac)\n\n'data.frame':   100 obs. of  2 variables:\n $ Age: int  20 23 24 25 25 26 26 28 28 29 ...\n $ Chd: int  0 0 0 0 1 0 0 0 0 0 ...\n\ny &lt;- cardiac$Chd\ntable(y)\n\ny\n 0  1 \n57 43 \n\nsum(y)\n\n[1] 43\n\nlength(y)\n\n[1] 100\n\n\nThese are thw four scenarios we consider: - A: \\(\\theta \\sim Beta(1,1)\\); - B: \\(\\theta \\sim Beta(10,10)\\); - C: \\(\\theta \\sim Beta(10,5)\\); - D: \\(\\theta \\sim Beta(5,10)\\).\n\npar(mfrow=c(2,2))\ncurve(dbeta(x, 1, 1), main=\"Scenario A\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 10, 10), main=\"Scenario B\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 10, 5), main=\"Scenario C\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 5, 10), main=\"Scenario D\", xlab=expression(theta), ylab=expression(pi(theta)))\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nWe further define the L_binom function, which returns the value of the likelihood function evaluated in a specific theta point.\n\nL_Binom &lt;- function(y, theta){\n  s &lt;- sum(y)\n  n &lt;- length(y)\n  L &lt;- theta^s * (1-theta)^(n-s)\n  return(L)\n}\n\nL_Binom(y, theta=0.001)\n\n[1] 9.445671e-130\n\n\nObviously, the likelihood function does not depend on the prior distribution we impose on \\(\\theta\\). In this case, the MLE corresponds to the sample mean (blue dashed line).\n\ncurve(L_Binom(y, theta=x), main=\"Likelihood\",\n      xlab=expression(theta), lwd=2)\nabline(v=mean(y), col=\"blue\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nScenario A:\nIn this scenario, the prior distribution coincides with a uniform distribution on the \\((0,1)\\) interval. Thus, the prior mean (red dashed line) is equal to 0.5.\n\na1 &lt;- 1; b1 &lt;- 1\n\ncurve(dbeta(x, a1, b1), main=\"Prior A\", ylim=c(0,1.4),\n      xlab=expression(theta), lwd=2)\nabline(v=a1/(a1+b1), col=\"red\", lty=\"dashed\")\n\n\n\n\n\n\n\n\nThe posterior distribution is $| Beta()\n\nn &lt;- length(y)\na1.post &lt;- a1 + sum(y)\nb1.post &lt;- b1 + n - sum(y)\n\ncurve(dbeta(x, a1.post, b1.post), main=\"Scenario A\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a1, b1), lty=\"dashed\", add=T, lwd=2)\nabline(v=a1/(a1+b1), col=\"red\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"green\", lty=\"dotted\", lwd=2)\nabline(v=a1.post/(a1.post+b1.post), col=\"blue\", lty=\"dashed\", lwd=2)\nlegend(.8,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"red\", \"green\", \"blue\"), lty=c(2,1,2,3,2), bty=\"n\")\n\n\n\n\n\n\n\n\n\n\nThe chosen prior distribution does not favor\n\n\nany value of theta.\n\n\nThus, we selected a non-informative prior.\n\n\nThe posterior distribution is centered\n\n\naround the MLE.\n\n\nScenario B:\na2 &lt;- 10; b2 &lt;- 10\n\n\nPrior distribution:\ncurve(dbeta(x, a2, b2), main=“Prior B”, xlab=expression(theta), lwd=2) abline(v=a2/(a2+b2), col=“red”, lty=“dashed”)\n\n\nLikelihood function:\ncurve(L_Binom(y, theta=x), main=“Likelihood”, xlab=expression(theta), lwd=2) abline(v=mean(y), col=“blue”, lty=“dashed”)\n\n\nPosterior distribution:\nn &lt;- length(y) a2.post &lt;- a2 + sum(y) b2.post &lt;- b2 + n - sum(y)\ncurve(dbeta(x, a2.post, b2.post), main=“Scenario B”, xlab=expression(theta), lwd=2) curve(dbeta(x, a2, b2), lty=“dashed”, add=T, lwd=2) abline(v=a2/(a2+b2), col=“red”, lty=“dashed”, lwd=2) abline(v=mean(y), col=“green”, lty=“dotted”, lwd=2) abline(v=a2.post/(a2.post+b2.post), col=“blue”, lty=“dashed”, lwd=2) legend(.8,8, c(“Prior”, “Posterior”, “Prior Mean”, “MLE”, “Post. Mean”), lwd=2, col=c(“black”, “black”, “red”, “green”, “blue”), lty=c(2,1,2,3,2), bty=“n”)\n\n\nNow we are still considering a symmetric prior\n\n\nwith prior mean = 0.5, but it does not\n\n\ngive same density/probability to each\n\n\nvalue/interval of theta.\n\n\nNow the posterior mean is a weighted average\n\n\nbetween prior mean and MLE.\n\n\nScenario C:\na3 &lt;- 10; b3 &lt;- 5\n\n\nPosterior distribution:\nn &lt;- length(y) a3.post &lt;- a3 + sum(y) b3.post &lt;- b3 + n - sum(y)\ncurve(dbeta(x, a3.post, b3.post), main=“Scenario C”, xlab=expression(theta), lwd=2) curve(dbeta(x, a3, b3), lty=“dashed”, add=T, lwd=2) abline(v=a3/(a3+b3), col=“red”, lty=“dashed”, lwd=2) abline(v=mean(y), col=“green”, lty=“dotted”, lwd=2) abline(v=a3.post/(a3.post+b3.post), col=“blue”, lty=“dashed”, lwd=2) legend(.8,8, c(“Prior”, “Posterior”, “Prior Mean”, “MLE”, “Post. Mean”), lwd=2, col=c(“black”, “black”, “red”, “green”, “blue”), lty=c(2,1,2,3,2), bty=“n”)\n\n\nScenario D:\na4 &lt;- 5; b4 &lt;- 10\n\n\nPosterior distribution:\nn &lt;- length(y) a4.post &lt;- a4 + sum(y) b4.post &lt;- b4 + n - sum(y)\ncurve(dbeta(x, a4.post, b4.post), main=“Scenario D”, xlab=expression(theta), lwd=2) curve(dbeta(x, a4, b4), lty=“dashed”, add=T, lwd=2) abline(v=a4/(a4+b4), col=“red”, lty=“dashed”, lwd=2) abline(v=mean(y), col=“green”, lty=“dotted”, lwd=2) abline(v=a4.post/(a4.post+b4.post), col=“blue”, lty=“dashed”, lwd=2) legend(.8,8, c(“Prior”, “Posterior”, “Prior Mean”, “MLE”, “Post. Mean”), lwd=2, col=c(“black”, “black”, “red”, “green”, “blue”), lty=c(2,1,2,3,2), bty=“n”)\n\n\nWe may summarize the (prior and) posterior distribution\n\n\nby some measures:\nBeta.Exp &lt;- function(a, b) a/(a+b) Beta.Var &lt;- function(a, b)(ab)/((a+b)^2(a+b+1)) Beta.Mode &lt;- function(a, b){ ifelse(a&gt;1 & b&gt;1, (a-1)/(a+b-2), NA) } Beta.Median &lt;- function(a, b) qbeta(.5, a, b)\nmeasures &lt;- matrix(NA, ncol=6, nrow=4) rownames(measures) &lt;- c(“A”, “B”, “C”, “D”) colnames(measures) &lt;- c(“a.post”, “b.post”, “Exp”, “Mode”, “Median”, “Var”)\nmeasures[,1] &lt;- c(a1.post,a2.post,a3.post,a4.post) measures[,2] &lt;- c(b1.post,b2.post,b3.post,b4.post) for(i in 1:4){ a &lt;- measures[i,1] b &lt;- measures[i,2] measures[i,3] &lt;- Beta.Exp(a, b) measures[i,4] &lt;- Beta.Mode(a, b) measures[i,5] &lt;- Beta.Median(a, b) measures[i,6] &lt;- Beta.Var(a, b) }\nround(measures,4)\n\n\nIn this simple scenario, even considering\n\n\ndifferent priors, we do not have posteriors\n\n\nleading to very different point estimates.\n\n\nThis is because the empirical evidence\n\n\n(i.e., our data) is much\n\n\nstronger than our prior belief.\n\n\nLet us consider an additional scenario where\n\n\nwe strongly believe that most people\n\n\ndo not have cardiovascular disease.\na5 &lt;- 100 b5 &lt;- 1000\n\n\nPrior distribution:\ncurve(dbeta(x, a5, b5), main=“Prior E”, xlab=expression(theta)) abline(v=a5/(a5+b5), col=“red”, lty=“dashed”)\n\n\nPosterior distribution:\nn &lt;- length(y) a5.post &lt;- a5 + sum(y) b5.post &lt;- b5 + n - sum(y)\ncurve(dbeta(x, a5.post, b5.post), main=“Scenario E”, xlab=expression(theta), lwd=2) curve(dbeta(x, a5, b5), lty=“dashed”, add=T, lwd=2) abline(v=a5/(a5+b5), col=“red”, lty=“dashed”, lwd=2) abline(v=mean(y), col=“green”, lty=“dotted”, lwd=2) abline(v=a5.post/(a5.post+b5.post), col=“blue”, lty=“dashed”, lwd=2) legend(.8,40, c(“Prior”, “Posterior”, “Prior Mean”, “MLE”, “Post. Mean”), lwd=2, col=c(“black”, “black”, “red”, “green”, “blue”), lty=c(2,1,2,3,2), bty=“n”)\nmeasures &lt;- matrix(NA, ncol=6, nrow=5) rownames(measures) &lt;- c(“A”, “B”, “C”, “D”, “E”) colnames(measures) &lt;- c(“a.post”, “b.post”, “Exp”, “Mode”, “Median”, “Var”)\nmeasures[,1] &lt;- c(a1.post,a2.post,a3.post,a4.post,a5.post) measures[,2] &lt;- c(b1.post,b2.post,b3.post,b4.post,b5.post) for(i in 1:5){ a &lt;- measures[i,1] b &lt;- measures[i,2] measures[i,3] &lt;- Beta.Exp(a, b) measures[i,4] &lt;- Beta.Mode(a, b) measures[i,5] &lt;- Beta.Median(a, b) measures[i,6] &lt;- Beta.Var(a, b) }\nround(measures,4)\n\n\nInterval Estimates:\n\n\nCredible Sets (CS).\n\n\nThe CS are very easy to compute. Indeed, we only have to compute\n\n\nquantiles of the posterior distribution.\n\n\nLet us consider scenario D:\na &lt;- a4.post b &lt;- b4.post\n\n\nScenario D:\ncurve(dbeta(x, a, b), main=“Scenario D”, xlab=expression(theta), lwd=2)\n\n\nCS of level 0.95\nCS &lt;- qbeta(c(0.025,0.975), a, b);CS abline(v=CS,lty=“dashed”, col=“blue”) legend(0.8,8,c(“Posterior”, “Credible Set”), lty=c(1,2), col=c(“black”, “blue”), bty=“n”)\n\n\nHighest posterior density (HPD):\n\n\nLet’s start by considering a random value for h\nh &lt;- 2 curve(dbeta(x, a, b), ylab=expression(paste(pi,“(”,theta,“|x)”)), xlab=expression(theta)) abline(h=h,lty=2)\n\n\nWe need to find the values of theta such that p(theta|y) = h\ntranslated &lt;- function(x, a, b) dbeta(x,a, b)-h # We decrease the density curve by h. # Now, the values we are looking for are such that translated = 0\ncurve(translated(x, a, b), add=T,lty=3,lwd=2) abline(h=0,lty=3)\nhpd1 &lt;- uniroot(translated,c(.2, .4),a,b)\\(root;hpd1\nhpd2 &lt;- uniroot(translated,c(.45, .5),a,b)\\)root;hpd2 integrate(dbeta, lower=hpd1, upper=hpd2, shape1=a, shape2=b)\n\n\nWe have a probability equal to 0.9152 –&gt; Decrease h\n\n\nIterative way:\nh.grid &lt;- seq(1, 2, by = 0.01)\nres &lt;- matrix(NA, ncol=4, nrow=length(h.grid)) colnames(res) &lt;- c(“HPD1”, “HPD2”, “level”, “h”)\nfor(i in 1:length(h.grid)){\ntranslated &lt;- function(x, a, b) dbeta(x,a, b)-h.grid[i]\nhpd1&lt;-uniroot(translated,c(.2,.4),a,b)\\(root\n  hpd2&lt;-uniroot(translated,c(.43,.55),a,b)\\)root\nI &lt;- integrate(dbeta, lower=hpd1, upper=hpd2, shape1=a, shape2=b)$value\niter &lt;- i res[i,] &lt;- c(hpd1, hpd2, I, h.grid[i]) if(I &lt;= 0.95) break }\nres[1:iter,] h.grid[iter]\nHPD &lt;- res[iter-1,-c(3,4)]; HPD CS\n\n\nGraphically:\ncurve(dbeta(x, a, b), main=“Scenario D”, xlab=expression(theta), lwd=2)\nabline(v=CS,lty=“dashed”, col=“blue”) abline(v=HPD,lty=“dashed”, col=“red”)\nlegend(0.8,8,c(“Posterior”, “Credible Set”, “HPD”), lty=c(1,2,2), col=c(“black”, “blue”, “red”), bty=“n”)\n\n\nHPD and CS are very similar, because the posterior is\n\n\nunimodal and almost symmetrical.\n\n\nHypothesis testing:\n\n\nLet consider H0: theta &lt;= 0.3 vs H1: theta &gt; 0.3\npbeta(.3, a4, b4) pbeta(.3, a4.post, b4.post)\nODDS.prior &lt;- pbeta(.3, a4, b4)/(1-pbeta(.3, a4, b4)); ODDS.prior ODDS.post &lt;- pbeta(.3, a4.post, b4.post)/(1-pbeta(.3, a4.post, b4.post)); ODDS.post\nBayes_Factor &lt;- ODDS.post/ODDS.prior; Bayes_Factor\n\n\nData do not support H0: by adding data, our confidence on\n\n\nH0 strongly decreases\n\n\nHOMEWORK: consider a sample of size n = 10 composed of\n\n\nBernoulli trials, for which we observed s = 3 successes.\n\n\nConsider that a priori theta ~ Beta(2, 8)\n\n\n1. Which is the posterior distribution of theta?\n\n\n2. Compute the MLE and the prior and posterior means.\n\n\nAre these results reasonable?\n\n\n3. Compute and compare the CS and the HPD.\n\n\n4. Let H0 : theta in (0.3, 0.5)\n\n\nH1 : theta in (0, 0.3) U (0.5, 1)\n\n\nWhich hypothesis can you support?"
  }
]