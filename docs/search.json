[
  {
    "objectID": "Script5.html",
    "href": "Script5.html",
    "title": "Script 5 - MCMC - Stan",
    "section": "",
    "text": "You can download the R script here.\nSome useful links for working with Stan:\n\nList of Language-Specific Stan Interfaces.\nRstan: an interface to Stan for R.\nRStan: Getting Started.\n\n\nLM\nIn this part, we are going to fit a simple linear regression model through Stan. Here you can download the .stan model.\nFirst of all, let’s compile the model and save it into an R object.\n\nLM_Model &lt;- rstan::stan_model(file=\"data/LM_Model.stan\")\n\nLM_Model\n\nS4 class stanmodel 'anon_model' coded as follows:\ndata{\n    int&lt;lower = 0&gt; n;  // number of obs\n    int&lt;lower = 0&gt; K;  // number of covariates (including the intercept)\n\n    vector[n] y;     // response\n    matrix[n, K] X;  // covariates\n  real a0;\n  real b0;\n    vector[K] beta0;\n    vector[K] s2_0;\n}\nparameters{\n    real&lt;lower = 0&gt; sigma2;\n    vector[K] beta;\n}\ntransformed parameters {\n    vector[n] mu;\n  mu = X * beta;\n}\nmodel{\n    // Prior:\n    sigma2 ~ inv_gamma(a0, b0);\n\n    for(k in 1:K){\n        beta[k] ~ normal(beta0[k], sqrt(s2_0[k]));\n    }\n    // Likelihood:\n    y ~ normal(mu, sqrt(sigma2));\n}\ngenerated quantities{\n\n    vector[n] log_lik;\n\n    for (j in 1:n){\n            log_lik[j] = normal_lpdf(y[j] | mu[j], sqrt(sigma2));\n    }\n} \n\n\nWe consider the Prestige data from the car package. Statistical units are occupations, for which we have:\n\neducation: average education of occupational incumbents, years, in 1971;\nincome: average income of incumbents, dollars, in 1971;\nwomen: percentage of incumbents who are women;\nprestige: Pineo-Porter prestige score for occupation, from a social survey conducted in the mid-1960s;\ncensus: canadian Census occupational code;\ntype: type of occupation (bc, Blue Collar; prof, Professional, Managerial, and Technical; wc, White Collar).\n\nLet’s transform the income variable so to reduce the asimmetry.\n\ndata(\"Prestige\")\n\nPrestige2 &lt;-\n  Prestige[complete.cases(Prestige),-5]\nPrestige2$income &lt;- log(Prestige2$income)\n\nIn order to fit the stan model, we need to pass some data: - y: the response vector;\n\nX: the design matrix;\nn: the sample size;\nK: the number of covariates (including the intercept term);\na0, b0, beta0, s2_0: hyperparameters.\n\n\ny &lt;- Prestige2$prestige\nmod &lt;- lm(prestige ~ ., data=Prestige2)\n\nX &lt;- model.matrix(mod)\nn &lt;- nrow(X)\nK &lt;- ncol(X)\n\nWe further create a list containing the objects to be passed to the stan model.\n\ndata.stan &lt;- list(\n  y = y,\n  X = X,\n  n = n,\n  K = K,\n  a0 = 10,\n  b0 = 10,\n  beta0 = rep(0, K),\n  s2_0 = rep(10, K)\n) \n\nWe can now use the rstan::sampling function to fit the model. We what two chains, each of length 10’000, and we want to discard the first 50% as warm-up.\n\nn.iter &lt;- 10000\nnchain &lt;- 2\n\nLM_stan &lt;- rstan::sampling(\n  object = LM_Model,\n  data = data.stan,\n  warmup = 0.5*n.iter, \n  iter = n.iter,\n  thin=1, chains = nchain,\n  refresh = n.iter/2     \n  #, pars=c(\"beta\", \"sigma2\")\n  )\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 5.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.52 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 1.209 seconds (Warm-up)\nChain 1:                1.461 seconds (Sampling)\nChain 1:                2.67 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.3e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1.209 seconds (Warm-up)\nChain 2:                1.319 seconds (Sampling)\nChain 2:                2.528 seconds (Total)\nChain 2: \n\n\n\nsumm &lt;- summary(LM_stan, pars = c(\"beta\", \"sigma2\"))$summary\n\nrownames(summ)[1:K] &lt;- colnames(X)\nround(summ, 3)\n\n              mean se_mean    sd   2.5%    25%    50%    75%  97.5%    n_eff\n(Intercept) -2.906   0.041 3.125 -9.205 -5.000 -2.902 -0.799  3.176 5840.231\neducation    4.308   0.007 0.460  3.404  3.997  4.299  4.617  5.230 4035.989\nincome       0.500   0.010 0.620 -0.735  0.085  0.506  0.919  1.712 3963.465\nwomen       -0.060   0.000 0.025 -0.109 -0.076 -0.060 -0.043 -0.012 6956.177\ntypeprof     6.092   0.034 2.264  1.540  4.588  6.139  7.609 10.477 4500.305\ntypewc      -2.826   0.023 1.791 -6.349 -4.023 -2.810 -1.606  0.646 6073.456\nsigma2      48.831   0.080 6.717 37.374 44.049 48.280 52.981 63.435 6968.333\n            Rhat\n(Intercept)    1\neducation      1\nincome         1\nwomen          1\ntypeprof       1\ntypewc         1\nsigma2         1\n\n\nTraceplots:\n\nrstan::traceplot(LM_stan, pars = c('beta', 'sigma2'), inc_warmup = TRUE)\n\n\n\n\n\n\n\nrstan::traceplot(LM_stan, pars = c('beta', 'sigma2'), inc_warmup = FALSE)\n\n\n\n\n\n\n\n\n\npost_chain &lt;- rstan::extract(LM_stan, c('beta', 'sigma2')) \nbetas &lt;- post_chain$beta\ncolnames(betas) &lt;- colnames(X)\n\nsigma2 &lt;- post_chain$sigma2\n\ndf_plot &lt;- \n  data.frame(value = c(\n    betas[,1], betas[,2], betas[,3],\n    betas[,4], betas[,5], betas[,6],\n    sigma2), \n    param = rep(c(colnames(X), 'sigma2'), \n                each = nrow(post_chain$beta)))\n\n\ndf_plot %&gt;% \n  ggplot(aes(value, fill = param)) + \n  geom_density() + \n  facet_wrap(~param, scales = 'free') + \n  theme_minimal() + \n  theme(legend.position=\"false\")\n\n\n\n\n\n\n\n\n\nacf(betas[,1])\n\n\n\n\n\n\n\nacf(betas[,2])\n\n\n\n\n\n\n\nacf(betas[,3])\n\n\n\n\n\n\n\nacf(betas[,4])\n\n\n\n\n\n\n\nacf(betas[,5])\n\n\n\n\n\n\n\nacf(betas[,5])\nacf(betas[,6])\n\n\n\n\n\n\n\nacf(sigma2)\n\n\n\n\n\n\n\n\nAs classical MCMCs, we can use the sampled values to approximate any quantity of interest. For example, we can approximate \\(\\mathbb{P}(\\beta_1 &gt; 4, \\beta_3 &gt; -0.5)\\):\n\nmean(betas[,2] &gt; 4 & betas[,4] &gt; -.5)\n\n[1] 0.7475\n\n\n\n\nLogistic Regression\nLet’s consider the logistic regression model to the cardiac dataset.\nThe Logistic_Model.stan model can be downloaded here.\n\nrm(list = ls())\ncardiac &lt;- read.csv(\"data/cardiac.csv\", header=T, sep=\";\")\n\ny &lt;- cardiac$Chd\nX &lt;- model.matrix(lm(Chd ~ ., data=cardiac))\nn &lt;- nrow(X)\nK &lt;- ncol(X)\n\n\nLogistic_Model &lt;-\n  rstan::stan_model(file=\"data/Logistic_Model.stan\")\n\nLogistic_Model\n\nS4 class stanmodel 'anon_model' coded as follows:\n///////////////////////// DATA /////////////////////////////////////\ndata {\n    int&lt;lower = 1&gt; n;       // number of data\n    int&lt;lower = 1&gt; K;       // number of covariates (including the intercept)\n    int&lt;lower = 0, upper = 1&gt; y[n];   // response vector\n    matrix[n, K] X;           // design matrix\n\n  vector[K] beta0;\n  vector[K] s2_0;\n}\n//////////////////// PARAMETERS /////////////////////////////////\nparameters {\n    vector[K] beta;        // regression coefficients\n}\ntransformed parameters {\n    vector[n] eta;\n    vector&lt;lower=0&gt;[n] Odds;\n    vector&lt;lower=0,upper=1&gt;[n] p;\n\n  eta = X * beta;\n\n  for(i in 1:n){\n    Odds[i] = exp(eta[i]);\n    p[i] = Odds[i]/(1+Odds[i]);\n  }\n\n}\n////////////////// MODEL ////////////////////////\nmodel {\n  // Prior\n    for (j in 1:K){\n        beta[j] ~ normal(beta0, sqrt(s2_0));\n    }\n\n  // Likelihood\n    for (s in 1:n){\n        y[s] ~ bernoulli(p[s]);\n    }\n} \n\n\n\ndata.stan_logis &lt;- list(\n  y = y,\n  X = X,\n  n = n,\n  K = K,\n  beta0 = rep(0, K),\n  s2_0 = rep(10, K)\n) \n\n\nn.iter &lt;- 10000\nnchain &lt;- 2\n\nLogistic_stan &lt;- rstan::sampling(\n  object = Logistic_Model,\n  data = data.stan_logis,\n  warmup = 0.5*n.iter, iter = n.iter,\n  thin = 1, chains = nchain,\n  refresh = n.iter/2\n)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 8.9e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.89 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 1: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 1: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 1: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 2.355 seconds (Warm-up)\nChain 1:                1.891 seconds (Sampling)\nChain 1:                4.246 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 3.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.32 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 10000 [  0%]  (Warmup)\nChain 2: Iteration: 5000 / 10000 [ 50%]  (Warmup)\nChain 2: Iteration: 5001 / 10000 [ 50%]  (Sampling)\nChain 2: Iteration: 10000 / 10000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1.973 seconds (Warm-up)\nChain 2:                2.168 seconds (Sampling)\nChain 2:                4.141 seconds (Total)\nChain 2: \n\n\n\nsumm &lt;- summary(Logistic_stan, pars = \"beta\")$summary\nrownames(summ)[1:K] &lt;- colnames(X)\nround(summ, 3)\n\n              mean se_mean    sd   2.5%    25%    50%    75%  97.5%    n_eff\n(Intercept) -4.447   0.024 0.923 -6.258 -5.084 -4.429 -3.810 -2.706 1501.027\nAge          0.093   0.001 0.020  0.056  0.079  0.093  0.107  0.133 1482.148\n            Rhat\n(Intercept)    1\nAge            1\n\n\n\nrstan::traceplot(Logistic_stan, pars = \"beta\", inc_warmup = TRUE)\n\n\n\n\n\n\n\nrstan::traceplot(Logistic_stan, pars = \"beta\", inc_warmup = FALSE)\n\n\n\n\n\n\n\n\n\npost_chain &lt;- rstan::extract(Logistic_stan, \"beta\") \nbetas &lt;- post_chain$beta\ncolnames(betas) &lt;- colnames(X)\n\ndf_plot &lt;- \n  data.frame(value = c(betas[,1], betas[,2]), \n             param = rep(colnames(X), each = nrow(post_chain$beta)))\n\ndf_plot %&gt;% \n  ggplot(aes(value, fill = param)) + \n  geom_density() + \n  facet_wrap(~param, scales = 'free') + \n  theme_minimal() + \n  theme(legend.position=\"false\")\n\n\n\n\n\n\n\n\nWe can evaluate probabilities for each patients.\n\nprobs_stan &lt;- rstan::extract(Logistic_stan, \"p\")$p\nplot(cardiac$Age, colMeans(probs_stan), pch=20, type=\"l\")\n\n\n\n\n\n\n\nplot(cardiac$Age, colMeans(probs_stan), pch=20)"
  },
  {
    "objectID": "Script3.html",
    "href": "Script3.html",
    "title": "Script 3 - MCMC - Gibbs sampling",
    "section": "",
    "text": "In this section, we show two Gibbs samplers: the first one aims to estimate the parameters \\((\\mu, \\sigma^2)\\) of a Normal likelihood, whereas the second one estimates the parameters of a linear regression model."
  },
  {
    "objectID": "Script3.html#initialization",
    "href": "Script3.html#initialization",
    "title": "Script 3 - MCMC - Gibbs sampling",
    "section": "Initialization:",
    "text": "Initialization:\nWe initialize each element by drawing a value from the corresponding prior:\n\nB &lt;- 1000\nmu.chain &lt;- numeric(B)\nsigma2.chain &lt;- numeric(B)\n\nset.seed(42)\nmu.chain[1] &lt;- rnorm(1, mu0, sqrt(sigma2.0))\nsigma2.chain[1] &lt;- 1/rgamma(1, alpha, rate = beta)\n\nThen, we can implement the Gibbs sampling by generating each parameter from its full-conditional distribution. The full-conditionals are the following:\n\n\\(\\mu| \\sigma^2, \\textbf{y} \\sim N\\left(\\frac{n\\sigma^2_0 \\bar{y} + \\sigma^2 \\mu_0}{n\\sigma^2_0 + \\sigma^2}, \\frac{\\sigma^2 \\sigma^2_0}{n\\sigma^2_0 + \\sigma^2}\\right)\\);\n\\(\\sigma^2| \\mu, \\textbf{y} \\sim Inv.Gamma\\left(a_0 + \\frac{n}{2}, b_0 + \\frac{\\sum_{i=1}^n (y_i - \\mu)^2}{2}\\right)\\).\n\n\nset.seed(42)\nfor(b in 2:B){\n  # Draw mu from the F.C.\n  mu.chain[b] &lt;-\n    rnorm(1, \n          (n*sigma2.0*ybar + sigma2.chain[b-1]*mu0)/(n*sigma2.0+sigma2.chain[b-1]),\n          sqrt((sigma2.chain[b-1]*sigma2.0)/(n*sigma2.0+sigma2.chain[b-1])))\n  \n  # Draw sigma2 from the F.C.\n  sigma2.chain[b] &lt;-\n    1/rgamma(1, alpha + n/2,\n             rate = beta + .5*sum((y-mu.chain[b])^2))\n  \n}\n\n\nplot(mu.chain, sigma2.chain, pch=20)\npoints(mu.true, sigma2.true, col=\"#D55E00\", pch=20)\n\n\n\n\n\n\n\n\nWe can now generate some diagnostic plots.\n\n# Traceplots:\npar(mfrow=c(2,1))\nplot(mu.chain[-1], pch=20, type=\"l\")\nplot(sigma2.chain[-1], pch=20, type=\"l\")\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\n\nmu.means &lt;- numeric(B)\nsigma2.means &lt;- numeric(B)\n\nmu.means[1] &lt;- mu.chain[1]\nsigma2.means[1] &lt;- sigma2.chain[1]\n\nfor(b in 2:B){\n  mu.means[b] &lt;- mean(mu.chain[2:b])\n  sigma2.means[b] &lt;- mean(sigma2.chain[2:b])\n}\n\nplot(mu.means[-1], pch=20, type=\"l\", ylim=c(9.2, 10.8))\nabline(h=mu.true, col=\"#D55E00\")\n\n\n\n\n\n\n\n\n\nplot(sigma2.means[-1], pch=20, type=\"l\", ylim=c(3,7))\nabline(h=sigma2.true, col=\"#D55E00\")\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,1))\nacf(mu.chain)\nacf(sigma2.chain)\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nOnce we have a chain for each parameter, we need to remove the warm-up (i.e., the part of the chains for which we cannot assume the convergence to the stationary distribution).\n\nwarm_perc &lt;- .5\n\nmu.new &lt;- mu.chain[round(B*warm_perc+1):B]\nsigma2.new &lt;- sigma2.chain[round(B*warm_perc+1):B]\n\npar(mfrow=c(2,1))\nplot(mu.new, pch=20, type=\"l\", ylim=c(9.2, 10.8))\nabline(h=mu.true, col=\"#D55E00\")\n\nplot(sigma2.new, pch=20, type=\"l\", ylim=c(3,7))\nabline(h=sigma2.true, col=\"#D55E00\")\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nThe new vectors can be used to compute estimates, CSs, and probabilities:\n\nmean(mu.new)\n\n[1] 10.07776\n\nquantile(mu.new, probs = c(.025, .975))\n\n     2.5%     97.5% \n 9.660935 10.497956 \n\n\n\nmean(sigma2.new)\n\n[1] 4.710458\n\nquantile(sigma2.new, probs = c(.025, .975))\n\n    2.5%    97.5% \n3.694683 6.045644 \n\n\n\nmean(mu.new &gt; 10)\n\n[1] 0.648\n\n\n\nhist(mu.new, prob=T, xlab=expression(mu),\n     main=\"Posterior distribution of mu\")\nlines(density(mu.new), col=\"#D55E00\")\nabline(v=mu.true, col=\"black\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nhist(sigma2.new, prob=T, xlab=expression(sigma2),\n     main=\"Posterior distribution of sigma2\")\nlines(density(sigma2.new), col=\"#D55E00\")\nabline(v=sigma2.true, col=\"black\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nplot(mu.new, sigma2.new, pch = 20)\npoints(mu.true, sigma2.true, pch = 19, col = \"#D55E00\")\n\n\n\n\n\n\n\n\nWe can define a function to fit this Gibbs sampler more easily on new data.\n\nnormal_GS &lt;- function(y, B = 5000, \n                      mu0 = 0, sigma2.0 = 1000, \n                      alpha = 10, beta = 10, \n                      warm_perc = .5, seed=42){\n  \n  mu.chain &lt;- numeric(B)\n  sigma2.chain &lt;- numeric(B)\n  ybar &lt;- mean(y)\n  n &lt;- length(y)\n  \n  # Initialization:\n  set.seed(seed)\n  mu.chain[1] &lt;- rnorm(1, mu0, sqrt(sigma2.0))\n  sigma2.chain[1] &lt;- 1/rgamma(1, alpha, rate = beta)\n  \n  for(b in 2:B){\n    # Draw mu from the F.C.\n    mu.chain[b] &lt;-\n      rnorm(1,\n            (n*sigma2.0*ybar + sigma2.chain[b-1]*mu0)/(n*sigma2.0+sigma2.chain[b-1]),\n            sqrt((sigma2.chain[b-1]*sigma2.0)/(n*sigma2.0+sigma2.chain[b-1])))\n    \n    # Draw sigma2 from the F.C.\n    sigma2.chain[b] &lt;-\n      1/rgamma(1, alpha + n/2,\n               rate = beta + .5*sum((y-mu.chain[b])^2))\n  }\n  \n  mu.new &lt;- mu.chain[round(B*warm_perc+1):B]\n  sigma2.new &lt;- sigma2.chain[round(B*warm_perc+1):B]\n  \n  return(cbind(mu.chain = mu.new, sigma2.chain = sigma2.new))\n}"
  },
  {
    "objectID": "Script3.html#gala-dataset-i",
    "href": "Script3.html#gala-dataset-i",
    "title": "Script 3 - MCMC - Gibbs sampling",
    "section": "Gala dataset (I)",
    "text": "Gala dataset (I)\n\nlibrary(faraway)\n\nWarning in check_dep_version(): ABI version mismatch: \nlme4 was built with Matrix ABI version 1\nCurrent Matrix ABI version is 0\nPlease re-install lme4 from source or restore original 'Matrix' package\n\ndata(gala)\nstr(gala)\n\n'data.frame':   30 obs. of  7 variables:\n $ Species  : num  58 31 3 25 2 18 24 10 8 2 ...\n $ Endemics : num  23 21 3 9 1 11 0 7 4 2 ...\n $ Area     : num  25.09 1.24 0.21 0.1 0.05 ...\n $ Elevation: num  346 109 114 46 77 119 93 168 71 112 ...\n $ Nearest  : num  0.6 0.6 2.8 1.9 1.9 8 6 34.1 0.4 2.6 ...\n $ Scruz    : num  0.6 26.3 58.7 47.4 1.9 ...\n $ Adjacent : num  1.84 572.33 0.78 0.18 903.82 ...\n\n\n\ny &lt;- gala$Species\ngala_GS &lt;- normal_GS(y, B = 10000)\n\nstr(gala_GS)\n\n num [1:5000, 1:2] 92.5 44.1 32.6 24.1 102.8 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr [1:2] \"mu.chain\" \"sigma2.chain\"\n\nhead(gala_GS)\n\n      mu.chain sigma2.chain\n[1,]  92.52505     9972.609\n[2,]  44.11655     8021.474\n[3,]  32.56115     9979.261\n[4,]  24.07501    10765.908\n[5,] 102.78028     7780.960\n[6,]  63.76942     7516.838\n\n\n\nplot(gala_GS, pch = 20)\n\n\n\n\n\n\n\nhist(gala_GS[,1], prob = T, \n     main=\"Posterior distribution of mu\")\nlines(density(gala_GS[,1]), col=\"#D55E00\")\n\n\n\n\n\n\n\nhist(gala_GS[,2], prob = T, \n     main=\"Posterior distribution of sigma2\")\nlines(density(gala_GS[,2]), col=\"#D55E00\")\n\n\n\n\n\n\n\ncolMeans(gala_GS)\n\n    mu.chain sigma2.chain \n    67.11751   8299.55326 \n\nt(apply(gala_GS, 2, function(x) quantile(x, probs=c(.025, .975))))\n\n                   2.5%       97.5%\nmu.chain       37.25436    96.47314\nsigma2.chain 5468.21124 12628.64946\n\n\n\nmean(gala_GS[,1] &gt; 70)\n\n[1] 0.4212\n\nmean(gala_GS[,1] &gt; 70 & gala_GS[,2] &lt; 5500)\n\n[1] 0.017"
  },
  {
    "objectID": "Script3.html#gala-dataset-ii",
    "href": "Script3.html#gala-dataset-ii",
    "title": "Script 3 - MCMC - Gibbs sampling",
    "section": "Gala dataset (II)",
    "text": "Gala dataset (II)\n\ngala_lm &lt;- lm(Species ~ ., data = gala[,-2])\nsumm &lt;- summary(gala_lm); summ\n\n\nCall:\nlm(formula = Species ~ ., data = gala[, -2])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-111.679  -34.898   -7.862   33.460  182.584 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.068221  19.154198   0.369 0.715351    \nArea        -0.023938   0.022422  -1.068 0.296318    \nElevation    0.319465   0.053663   5.953 3.82e-06 ***\nNearest      0.009144   1.054136   0.009 0.993151    \nScruz       -0.240524   0.215402  -1.117 0.275208    \nAdjacent    -0.074805   0.017700  -4.226 0.000297 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 60.98 on 24 degrees of freedom\nMultiple R-squared:  0.7658,    Adjusted R-squared:  0.7171 \nF-statistic:  15.7 on 5 and 24 DF,  p-value: 6.838e-07\n\n# Definition of y and X:\ny &lt;- gala$Species\nX &lt;- model.matrix(Species ~ ., data = gala[,-2])\n\n# Fitting the model:\nlm_gala &lt;- LM_GS(y, X)\nstr(lm_gala)\n\nList of 2\n $ beta  : num [1:2500, 1:6] -0.685 0.788 -0.497 0.317 -0.501 ...\n $ sigma2: num [1:2500] 1677 2015 1942 1978 2534 ...\n\n\n\n# Extracting the elements of the chain:\nbetas &lt;- lm_gala$beta\ncolnames(betas) &lt;- colnames(X)\nsigma2 &lt;- lm_gala$sigma2\n\ncolMeans(betas)\n\n(Intercept)        Area   Elevation     Nearest       Scruz    Adjacent \n 0.02312137 -0.02602818  0.32983671 -0.00566936 -0.20657747 -0.07616102 \n\n\n\nt(apply(betas, 2, function(x) quantile(x, probs=c(.025, .975))))\n\n                   2.5%        97.5%\n(Intercept) -1.90212462  1.943814977\nArea        -0.05552613  0.003312421\nElevation    0.26572467  0.392750020\nNearest     -1.14952611  1.231034257\nScruz       -0.48552725  0.070630919\nAdjacent    -0.09987118 -0.051690537\n\n\n\nmean(sigma2)\n\n[1] 2049.94\n\n\n\nround(cov(betas), 5)\n\n            (Intercept)     Area Elevation  Nearest    Scruz Adjacent\n(Intercept)     0.97945  0.00058  -0.00232  0.02294 -0.00695  0.00054\nArea            0.00058  0.00023  -0.00038  0.00136  0.00035  0.00007\nElevation      -0.00232 -0.00038   0.00104 -0.00467 -0.00106 -0.00025\nNearest         0.02294  0.00136  -0.00467  0.38000 -0.05012  0.00196\nScruz          -0.00695  0.00035  -0.00106 -0.05012  0.01880 -0.00008\nAdjacent        0.00054  0.00007  -0.00025  0.00196 -0.00008  0.00016"
  },
  {
    "objectID": "Script1.html",
    "href": "Script1.html",
    "title": "Script 1 - Bernoulli - Beta",
    "section": "",
    "text": "In this section, we will explore the Bernoulli-Beta model applied to the cardiac dataset. Specifically, we assume that \\(Y_i | \\theta \\sim Bernoulli(\\theta)\\) and that \\(\\theta \\sim Beta(a, b)\\). With these choices, we showed that the posterior distribution is \\[\\theta | \\textbf{y} \\sim Beta\\left(a + \\sum_{i=1}^n y_i, b + n - \\sum_{i=1}^n y_i\\right).\\]\nWe will examine four different scenarios, each corresponding to a distinct choice of hyperparameters for the Beta prior.\n\ncardiac &lt;- read.table(\"data/cardiac.csv\", header=T, sep=\";\")\nstr(cardiac)\n\n'data.frame':   100 obs. of  2 variables:\n $ Age: int  20 23 24 25 25 26 26 28 28 29 ...\n $ Chd: int  0 0 0 0 1 0 0 0 0 0 ...\n\ny &lt;- cardiac$Chd\ntable(y)\n\ny\n 0  1 \n57 43 \n\nsum(y)\n\n[1] 43\n\n\n\ncardiac &lt;- read.table(\"data/cardiac.csv\", header=T, sep=\";\")\nstr(cardiac)\n\n'data.frame':   100 obs. of  2 variables:\n $ Age: int  20 23 24 25 25 26 26 28 28 29 ...\n $ Chd: int  0 0 0 0 1 0 0 0 0 0 ...\n\ny &lt;- cardiac$Chd\ntable(y)\n\ny\n 0  1 \n57 43 \n\nsum(y)\n\n[1] 43\n\nlength(y)\n\n[1] 100\n\n\n\nsum(y)\n\n[1] 43\n\n\n\nlength(y)\n\n[1] 100\n\n\nThese are the four scenarios we consider:\n\nA: \\(\\theta \\sim Beta(1,1)\\);\nB: \\(\\theta \\sim Beta(10,10)\\);\nC: \\(\\theta \\sim Beta(10,5)\\);\nD: \\(\\theta \\sim Beta(5,10)\\).\n\n\npar(mfrow=c(2,2))\ncurve(dbeta(x, 1, 1), main=\"Scenario A\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 10, 10), main=\"Scenario B\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 10, 5), main=\"Scenario C\", xlab=expression(theta), ylab=expression(pi(theta)))\ncurve(dbeta(x, 5, 10), main=\"Scenario D\", xlab=expression(theta), ylab=expression(pi(theta)))\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\nWe further define the L_binom function, which returns the value of the likelihood function evaluated in a specific theta point.\n\nL_Binom &lt;- function(y, theta){\n  s &lt;- sum(y)\n  n &lt;- length(y)\n  L &lt;- theta^s * (1-theta)^(n-s)\n  return(L)\n}\n\nL_Binom(y, theta=0.001)\n\n[1] 9.445671e-130\n\n\nObviously, the likelihood function does not depend on the prior distribution we impose on \\(\\theta\\). In this case, the MLE corresponds to the sample mean (blue dashed line).\n\ncurve(L_Binom(y, theta=x), main=\"Likelihood\",\n      xlab=expression(theta), lwd=2)\nabline(v=mean(y), col=\"#0072B2\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\n\nIn this scenario, the prior distribution coincides with a uniform distribution on the \\((0,1)\\) interval. Thus, the prior mean (red dashed line) is equal to 0.5.\n\na1 &lt;- 1; b1 &lt;- 1\n\ncurve(dbeta(x, a1, b1), main=\"Prior A\", ylim=c(0,1.4),\n      xlab=expression(theta), lwd=2)\nabline(v=a1/(a1+b1), col=\"#D55E00\", lty=\"dashed\")\n\n\n\n\n\n\n\n\nThe posterior distribution is \\(\\theta | \\textbf{y} \\sim Beta(44, 58)\\).\n\nn &lt;- length(y)\na1.post &lt;- a1 + sum(y)\nb1.post &lt;- b1 + n - sum(y)\n\ncurve(dbeta(x, a1.post, b1.post), main=\"Scenario A\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a1, b1), lty=\"dashed\", add=T, lwd=2)\nabline(v=a1/(a1+b1), col=\"#D55E00\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"#009E73\", lty=\"dotted\", lwd=2)\nabline(v=a1.post/(a1.post+b1.post), col=\"#0072B2\", lty=\"dashed\", lwd=2)\nlegend(.6,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"#D55E00\", \"#009E73\", \"#0072B2\"), lty=c(2,1,2,3,2), bty=\"n\")\n\n\n\n\n\n\n\n\nThe chosen prior distribution does not favor any particular value of \\(\\theta\\). Thus, we selected a non-informative prior. As a result, the posterior distribution is centered around the maximum likelihood estimate (MLE).\n\n\n\nIn this scenario, we use a prior distribution for \\(\\theta\\) that is still symmetric (meaning it treats values below and above 0.5 in the same way). However, unlike the uniform prior, it is not flat: it expresses a preference for values closer to 0.5.\n\na2 &lt;- 10; b2 &lt;- 10\n\ncurve(dbeta(x, a2, b2), main=\"Prior B\",\n      xlab=expression(theta), lwd=2)\nabline(v=a2/(a2+b2), col=\"#D55E00\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nn &lt;- length(y)\na2.post &lt;- a2 + sum(y)\nb2.post &lt;- b2 + n - sum(y)\n\ncurve(dbeta(x, a2.post, b2.post), main=\"Scenario B\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a2, b2), lty=\"dashed\", add=T, lwd=2)\nabline(v=a2/(a2+b2), col=\"#D55E00\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"#009E73\", lty=\"dotted\", lwd=2)\nabline(v=a2.post/(a2.post+b2.post), col=\"#0072B2\", lty=\"dashed\", lwd=2)\nlegend(.6,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"#D55E00\", \"#009E73\", \"#0072B2\"), lty=c(2,1,2,3,2), bty=\"n\")\n\n\n\n\n\n\n\n\nSince we are still considering a symmetric prior, the prior mean is equal to 0.5. However, unlike the uniform prior, it does not assign the same probability density to every value or interval of \\(\\theta\\). As a consequence, the posterior mean becomes a weighted average between the prior mean and the MLE.\n\n\n\nScenarios C and D consider asymmetric prior distributions.\n\na3 &lt;- 10; b3 &lt;- 5\n\ncurve(dbeta(x, a3, b3), main=\"Prior C\",\n      xlab=expression(theta), lwd=2)\nabline(v=a3/(a3+b3), col=\"#D55E00\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nn &lt;- length(y)\na3.post &lt;- a3 + sum(y)\nb3.post &lt;- b3 + n - sum(y)\n\ncurve(dbeta(x, a3.post, b3.post), main=\"Scenario C\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a3, b3), lty=\"dashed\", add=T, lwd=2)\nabline(v=a3/(a3+b3), col=\"#D55E00\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"#009E73\", lty=\"dotted\", lwd=2)\nabline(v=a3.post/(a3.post+b3.post), col=\"#0072B2\", lty=\"dashed\", lwd=2)\nlegend(.7,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"#D55E00\", \"#009E73\", \"#0072B2\"), lty=c(2,1,2,3,2), bty=\"n\")\n\n\n\n\n\n\n\n\n\na4 &lt;- 5; b4 &lt;- 10\n\ncurve(dbeta(x, a4, b4), main=\"Prior D\",\n      xlab=expression(theta), lwd=2)\nabline(v=a4/(a4+b4), col=\"#D55E00\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nn &lt;- length(y)\na4.post &lt;- a4 + sum(y)\nb4.post &lt;- b4 + n - sum(y)\n\ncurve(dbeta(x, a4.post, b4.post), main=\"Scenario D\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a4, b4), lty=\"dashed\", add=T, lwd=2)\nabline(v=a4/(a4+b4), col=\"#D55E00\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"#009E73\", lty=\"dotted\", lwd=2)\nabline(v=a4.post/(a4.post+b4.post), col=\"#0072B2\", lty=\"dashed\", lwd=2)\nlegend(.6,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"#D55E00\", \"#009E73\", \"#0072B2\"), lty=c(2,1,2,3,2), bty=\"n\")"
  },
  {
    "objectID": "Script1.html#scenario-a",
    "href": "Script1.html#scenario-a",
    "title": "Script 1 - Bernoulli - Beta",
    "section": "",
    "text": "In this scenario, the prior distribution coincides with a uniform distribution on the \\((0,1)\\) interval. Thus, the prior mean (red dashed line) is equal to 0.5.\n\na1 &lt;- 1; b1 &lt;- 1\n\ncurve(dbeta(x, a1, b1), main=\"Prior A\", ylim=c(0,1.4),\n      xlab=expression(theta), lwd=2)\nabline(v=a1/(a1+b1), col=\"#D55E00\", lty=\"dashed\")\n\n\n\n\n\n\n\n\nThe posterior distribution is \\(\\theta | \\textbf{y} \\sim Beta(44, 58)\\).\n\nn &lt;- length(y)\na1.post &lt;- a1 + sum(y)\nb1.post &lt;- b1 + n - sum(y)\n\ncurve(dbeta(x, a1.post, b1.post), main=\"Scenario A\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a1, b1), lty=\"dashed\", add=T, lwd=2)\nabline(v=a1/(a1+b1), col=\"#D55E00\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"#009E73\", lty=\"dotted\", lwd=2)\nabline(v=a1.post/(a1.post+b1.post), col=\"#0072B2\", lty=\"dashed\", lwd=2)\nlegend(.6,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"#D55E00\", \"#009E73\", \"#0072B2\"), lty=c(2,1,2,3,2), bty=\"n\")\n\n\n\n\n\n\n\n\nThe chosen prior distribution does not favor any particular value of \\(\\theta\\). Thus, we selected a non-informative prior. As a result, the posterior distribution is centered around the maximum likelihood estimate (MLE)."
  },
  {
    "objectID": "Script1.html#scenario-b",
    "href": "Script1.html#scenario-b",
    "title": "Script 1 - Bernoulli - Beta",
    "section": "",
    "text": "In this scenario, we use a prior distribution for \\(\\theta\\) that is still symmetric (meaning it treats values below and above 0.5 in the same way). However, unlike the uniform prior, it is not flat: it expresses a preference for values closer to 0.5.\n\na2 &lt;- 10; b2 &lt;- 10\n\ncurve(dbeta(x, a2, b2), main=\"Prior B\",\n      xlab=expression(theta), lwd=2)\nabline(v=a2/(a2+b2), col=\"#D55E00\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nn &lt;- length(y)\na2.post &lt;- a2 + sum(y)\nb2.post &lt;- b2 + n - sum(y)\n\ncurve(dbeta(x, a2.post, b2.post), main=\"Scenario B\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a2, b2), lty=\"dashed\", add=T, lwd=2)\nabline(v=a2/(a2+b2), col=\"#D55E00\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"#009E73\", lty=\"dotted\", lwd=2)\nabline(v=a2.post/(a2.post+b2.post), col=\"#0072B2\", lty=\"dashed\", lwd=2)\nlegend(.6,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"#D55E00\", \"#009E73\", \"#0072B2\"), lty=c(2,1,2,3,2), bty=\"n\")\n\n\n\n\n\n\n\n\nSince we are still considering a symmetric prior, the prior mean is equal to 0.5. However, unlike the uniform prior, it does not assign the same probability density to every value or interval of \\(\\theta\\). As a consequence, the posterior mean becomes a weighted average between the prior mean and the MLE."
  },
  {
    "objectID": "Script1.html#scenario-c-d",
    "href": "Script1.html#scenario-c-d",
    "title": "Script 1 - Bernoulli - Beta",
    "section": "",
    "text": "Scenarios C and D consider asymmetric prior distributions.\n\na3 &lt;- 10; b3 &lt;- 5\n\ncurve(dbeta(x, a3, b3), main=\"Prior C\",\n      xlab=expression(theta), lwd=2)\nabline(v=a3/(a3+b3), col=\"#D55E00\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nn &lt;- length(y)\na3.post &lt;- a3 + sum(y)\nb3.post &lt;- b3 + n - sum(y)\n\ncurve(dbeta(x, a3.post, b3.post), main=\"Scenario C\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a3, b3), lty=\"dashed\", add=T, lwd=2)\nabline(v=a3/(a3+b3), col=\"#D55E00\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"#009E73\", lty=\"dotted\", lwd=2)\nabline(v=a3.post/(a3.post+b3.post), col=\"#0072B2\", lty=\"dashed\", lwd=2)\nlegend(.7,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"#D55E00\", \"#009E73\", \"#0072B2\"), lty=c(2,1,2,3,2), bty=\"n\")\n\n\n\n\n\n\n\n\n\na4 &lt;- 5; b4 &lt;- 10\n\ncurve(dbeta(x, a4, b4), main=\"Prior D\",\n      xlab=expression(theta), lwd=2)\nabline(v=a4/(a4+b4), col=\"#D55E00\", lty=\"dashed\")\n\n\n\n\n\n\n\n\n\nn &lt;- length(y)\na4.post &lt;- a4 + sum(y)\nb4.post &lt;- b4 + n - sum(y)\n\ncurve(dbeta(x, a4.post, b4.post), main=\"Scenario D\",\n      xlab=expression(theta), lwd=2)\ncurve(dbeta(x, a4, b4), lty=\"dashed\", add=T, lwd=2)\nabline(v=a4/(a4+b4), col=\"#D55E00\", lty=\"dashed\", lwd=2)\nabline(v=mean(y), col=\"#009E73\", lty=\"dotted\", lwd=2)\nabline(v=a4.post/(a4.post+b4.post), col=\"#0072B2\", lty=\"dashed\", lwd=2)\nlegend(.6,8, c(\"Prior\", \"Posterior\", \"Prior Mean\", \"MLE\", \"Post. Mean\"), lwd=2,\n       col=c(\"black\", \"black\", \"#D55E00\", \"#009E73\", \"#0072B2\"), lty=c(2,1,2,3,2), bty=\"n\")"
  },
  {
    "objectID": "Script1.html#credible-sets-cs.",
    "href": "Script1.html#credible-sets-cs.",
    "title": "Script 1 - Bernoulli - Beta",
    "section": "Credible Sets (CS).",
    "text": "Credible Sets (CS).\nThe CS are very easy to compute in this scenario. Indeed, it is sufficient to calculate the appropriate quantiles of the posterior distribution.\nLet us consider scenario D:\n\na &lt;- a4.post\nb &lt;- b4.post\n\ncurve(dbeta(x, a, b), main=\"Scenario D\",\n      xlab=expression(theta), lwd=2)\n\n# CS of level 0.95\nCS &lt;- qbeta(c(0.025,0.975), a, b); CS\n\n[1] 0.3291889 0.5083162\n\nabline(v=CS,lty=\"dashed\", col=\"#0072B2\")\nlegend(0.8,8,c(\"Posterior\", \"Credible Set\"),\n       lty=c(1,2), col=c(\"black\", \"#0072B2\"), bty=\"n\")"
  },
  {
    "objectID": "Script1.html#highest-posterior-density-hpd",
    "href": "Script1.html#highest-posterior-density-hpd",
    "title": "Script 1 - Bernoulli - Beta",
    "section": "Highest posterior density (HPD):",
    "text": "Highest posterior density (HPD):\nLet’s start by considering an initial value for \\(h\\).\n\nh &lt;- 2\ncurve(dbeta(x, a, b), \n      ylab=expression(paste(pi,\"(\",theta,\"|x)\")),\n      xlab=expression(theta))\nabline(h=h,lty=2)\n\n\n\n\n\n\n\n\nWe need to find the values of theta such that \\(P(\\theta|\\textbf{y}) = h\\).\nTo do this, we progressively lower the posterior density curve by an amount \\(h\\).\n\ncurve(dbeta(x, a, b), \n      ylab=expression(paste(pi,\"(\",theta,\"|x)\")),\n      xlab=expression(theta))\nabline(h=h,lty=2)\n\ntranslated &lt;- function(x, a, b) dbeta(x,a, b) - h \n\ncurve(translated(x, a, b), add = T, lty = 3, lwd = 2)\nabline(h = 0, lty = 3)\n\n\n\n\n\n\n\n\nThe values we are looking for are those where the translated curve —that is, the original posterior density minus \\(h\\)— becomes zero.\n\nhpd1 &lt;- uniroot(translated,c(.2, .4),a,b)$root; hpd1\n\n[1] 0.3384793\n\nhpd2 &lt;- uniroot(translated,c(.45, .5),a,b)$root; hpd2\n\n[1] 0.4962774\n\nintegrate(dbeta, lower=hpd1, upper=hpd2, shape1=a, shape2=b)\n\n0.9152466 with absolute error &lt; 2.5e-14\n\n\nWe have a probability equal to 0.9152. Thus, we have to select a smaller \\(h\\). Iteratively:\n\nh.grid &lt;- seq(1, 2, by = 0.01)   \n\nres &lt;- matrix(NA, ncol=4, nrow=length(h.grid))\ncolnames(res) &lt;- c(\"HPD1\", \"HPD2\", \"level\", \"h\")\n\nfor(i in 1:length(h.grid)){        \n  translated &lt;- function(x, a, b) dbeta(x,a, b) - h.grid[i] \n  \n  hpd1&lt;-uniroot(translated,c(.2,.4),a,b)$root\n  hpd2&lt;-uniroot(translated,c(.43,.55),a,b)$root\n  \n  I &lt;- integrate(dbeta, lower=hpd1, upper=hpd2, shape1=a, shape2=b)$value\n  \n  iter &lt;- i\n  res[i,] &lt;- c(hpd1, hpd2, I, h.grid[i])\n  if(I &lt;= 0.95) break\n}\n\nres[1:iter,]\n\n           HPD1      HPD2     level    h\n [1,] 0.3225742 0.5134903 0.9635447 1.00\n [2,] 0.3228413 0.5132645 0.9630493 1.01\n [3,] 0.3230458 0.5130403 0.9626140 1.02\n [4,] 0.3232488 0.5128178 0.9621776 1.03\n [5,] 0.3234504 0.5125970 0.9617401 1.04\n [6,] 0.3236505 0.5123778 0.9613016 1.05\n [7,] 0.3238493 0.5121602 0.9608621 1.06\n [8,] 0.3240466 0.5119441 0.9604215 1.07\n [9,] 0.3242426 0.5117296 0.9599800 1.08\n[10,] 0.3244373 0.5115165 0.9595373 1.09\n[11,] 0.3246306 0.5113050 0.9590937 1.10\n[12,] 0.3248227 0.5110948 0.9586491 1.11\n[13,] 0.3250135 0.5108865 0.9582038 1.12\n[14,] 0.3252030 0.5106809 0.9577589 1.13\n[15,] 0.3253914 0.5104765 0.9573130 1.14\n[16,] 0.3255785 0.5102735 0.9568661 1.15\n[17,] 0.3257644 0.5100717 0.9564182 1.16\n[18,] 0.3259492 0.5098712 0.9559692 1.17\n[19,] 0.3261329 0.5096720 0.9555191 1.18\n[20,] 0.3263154 0.5094740 0.9550681 1.19\n[21,] 0.3264968 0.5092772 0.9546160 1.20\n[22,] 0.3266772 0.5090816 0.9541629 1.21\n[23,] 0.3268564 0.5088872 0.9537088 1.22\n[24,] 0.3270346 0.5086939 0.9532537 1.23\n[25,] 0.3272118 0.5085017 0.9527976 1.24\n[26,] 0.3273880 0.5083107 0.9523404 1.25\n[27,] 0.3275631 0.5081208 0.9518822 1.26\n[28,] 0.3277373 0.5079320 0.9514231 1.27\n[29,] 0.3279104 0.5077442 0.9509629 1.28\n[30,] 0.3280827 0.5075575 0.9505017 1.29\n[31,] 0.3282539 0.5073718 0.9500395 1.30\n[32,] 0.3284243 0.5071872 0.9495763 1.31\n\nh.grid[iter]\n\n[1] 1.31\n\nHPD &lt;- res[iter-1,-c(3,4)]; HPD\n\n     HPD1      HPD2 \n0.3282539 0.5073718 \n\nCS\n\n[1] 0.3291889 0.5083162\n\n\nWe can compare the two interval estimates in a graphical way:\n\ncurve(dbeta(x, a, b), main=\"Scenario D\",\n      xlab=expression(theta), lwd=2)\n\nabline(v=CS,lty=\"dashed\", col=\"#0072B2\")\nabline(v=HPD,lty=\"dashed\", col=\"#D55E00\")\n\nlegend(0.6,8,c(\"Posterior\", \"Credible Set\", \"HPD\"),\n       lty=c(1,2,2), col=c(\"black\", \"#0072B2\", \"#D55E00\"), bty=\"n\")\n\n\n\n\n\n\n\n\nHPD and CS are very similar, since the posterior is unimodal and almost symmetrical."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Inference II",
    "section": "",
    "text": "Statistical Inference II provides an introduction to Bayesian data analysis, covering prior and posterior distributions, classical one-parameter models, prior elicitation, posterior-based inference, MCMC methods, and Bayesian approaches to linear and generalized linear models."
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Statistical Inference II",
    "section": "Syllabus",
    "text": "Syllabus\n\nIntroduction to Bayesian data analysis: prior and posterior distributions for inference.\nOne-parameter models: Binomial-Beta, Poisson-Gamma, Exponential-Gamma, and Normal-Normal.\nMethods for prior elicitation.\nInference based on the posterior distribution (point and interval estimates; hypotheses testing).\nSimulation-based inference: MCMC methods.\nLinear and generalized linear models from a Bayesian perspective."
  },
  {
    "objectID": "index.html#textbooks",
    "href": "index.html#textbooks",
    "title": "Statistical Inference II",
    "section": "Textbooks",
    "text": "Textbooks\n\nHoff, P. (2009). A first course in Bayesian Statistical Methods. Springer.\nGelman, A., Carlin, J., Stern, H., Dunson, D., Vehtari, A., and Rubin, D. (2013). Bayesian Data Analysis. Chapman & Hall/CRC Texts in Statistical Science.\nRobert, C. and Casella, G. (2004). Monte Carlo Statistical Methods. Springer."
  },
  {
    "objectID": "Script2.html",
    "href": "Script2.html",
    "title": "Script 2 - Normal & Monte Carlo Approximations",
    "section": "",
    "text": "Normal Approximation for the Bernoulli-Beta posterior:\nLet us consider results of a general election, where party A collected 150’000 votes out of the 240’000 total votes.\n\nsum_y &lt;- 150000\nn &lt;- 240000\n\nWe still consider the Bernoulli-Beta model, meaning that \\(Y_i | \\theta \\sim Bernoulli(\\theta)\\) and \\(\\theta \\sim Beta(a, b)\\). Thus, \\[\\theta | \\textbf{y} \\sim Beta\\left(a + \\sum_{i=1}^n y_i, b + n - \\sum_{i=1}^n y_i\\right).\\]\nAs for the choice of the hyperparameters \\(a\\) and \\(b\\), we consider results of a previous general election. In that election, party A collected 49’000 votes out of 270’000 total votes:\n\na &lt;- 49000\nb &lt;- 270000\n\nThe Normal approximation requires the evaluation of the posterior mode. Let’s definte the mode of a Beta distribution.\n\nBeta.Mode &lt;- function(a, b){\n  ifelse(a&gt;1 & b&gt;1, (a-1)/(a+b-2), NA)\n}\n\nThe updated hyperparameters and the posterior mode \\(\\tilde{\\theta}\\) are:\n\na.post &lt;- a + sum_y; a.post\n\n[1] 199000\n\nb.post &lt;- b + n - sum_y; b.post\n\n[1] 360000\n\n\n\npost.mode &lt;- Beta.Mode(a.post, b.post); post.mode\n\n[1] 0.3559923\n\n\nFinally, the variance of the approximated normal distribution, namely the inverse of \\[\\left. -\\frac{\\partial^2 \\log \\pi(\\theta | \\textbf{y})}{\\partial \\theta^2} \\right|_{\\theta = \\tilde{\\theta}}.\\]\n\nsigma_approx &lt;- ((a.post-1)/(post.mode^2) + (b.post-1)/(1-post.mode)^2)^(-1)\n\nsigma_approx\n\n[1] 4.101299e-07\n\n\nLet’s compare the true posterior distribution with its Normal approximation:\n\ncurve(dbeta(x, a.post, b.post), xlim=c(.35,.36), lty = \"dashed\")\ncurve(dnorm(x, post.mode, sqrt(sigma_approx)), add=T, col=\"#D55E00\", lty = \"dotted\")\n\n\n\n\n\n\n\n\nThe Normal approximation is quite useful in computing HPDs, since they coincide with CSs due to the simmetry of the Normal distribution.\nIterative way for computing the true HPD:\n\nh.grid &lt;- seq(100, 105, by = 0.001)\n\nres &lt;- matrix(NA, ncol=4, nrow=length(h.grid))\ncolnames(res) &lt;- c(\"HPD1\", \"HPD2\", \"level\", \"h\")\n\nfor(i in 1:length(h.grid)){\n  translated &lt;- function(x, a, b) dbeta(x,a, b)-h.grid[i]\n\n  hpd1&lt;-uniroot(translated,c(.34,post.mode-.0005),a.post,b.post)$root\n  hpd2&lt;-uniroot(translated,c(post.mode+.0005,.36),a.post,b.post)$root\n\n  I &lt;- integrate(dbeta, lower=hpd1, upper=hpd2, shape1=a.post, shape2=b.post)$value\n\n  iter &lt;- i\n  res[i,] &lt;- c(hpd1, hpd2, I, h.grid[i])\n  if(I &lt;= 0.95) break\n}\n\nres[iter,]\n\n       HPD1        HPD2       level           h \n  0.3547393   0.3572496   0.9499997 102.0680000 \n\nHPD &lt;- res[iter-1,-c(3,4)]; HPD\n\n     HPD1      HPD2 \n0.3547393 0.3572496 \n\n\nHPD by taking advantage of the Normal approximation:\n\nqnorm(c(.025,.975), post.mode, sqrt(sigma_approx))\n\n[1] 0.3547371 0.3572475\n\n\n\n\nMonte Carlo\nWhen it is not possible to compute some quantity of interest directly from the posterior distribution, but it is possible to generate (pseudo)random samples from it, we can use Monte Carlo (MC) approximation methods.\nLet us consider the following example.\nA biostatistician aims to compare two treatments: - 1) a new vaccine for COVID-19; - 2) a placebo.\nHe/She collects a sample of 200 patients and randomly assigns them into two groups of equal size.\n\nn1 &lt;- 100\nn2 &lt;- 100\n\nThe first group receives the new vaccine, while the second group receives the placebo.\nAfter the treatments, the biostatistician records the number of deaths in each group, denoted by \\(Y_1\\) and \\(Y_2\\), respectively.\n\\(Y_1|\\theta_1 \\sim Bernoulli(\\theta_1)\\)\n\\(Y_2| \\theta_2 \\sim Bernoulli(\\theta_2)\\)\n\\(\\theta_j\\) represents the probability of death in group \\(j\\).\nThe biostatistician chooses to use non-informative priors for the parameters.\n\na1 &lt;- 1\nb1 &lt;- 1\n\na2 &lt;- 1\nb2 &lt;- 1\n\nOnce the priors have been selected, he/she looks at the data…\n\ns1 &lt;- 20\ns2 &lt;- 80\n\n… and updates the prior hyperparameters to obtain the posterior distributions.\n\na1.post &lt;- a1 + s1\nb1.post &lt;- b1 + n1 - s1\n\na2.post &lt;- a2 + s2\nb2.post &lt;- b2 + n2 - s2\n\n\ncurve(dbeta(x ,a1.post,b1.post), 0,1, lty=1, ylim=c(0,10),\n      xlab=expression(theta), ylab=expression(pi), lwd=2)\ncurve(dbeta(x , a2.post, b2.post), add=T, lty=3, lwd=2)\nlegend(0.4,10,c(\"Posterior 1\",\"Posterior 2\"),lty=c(1,3),cex=0.8,lwd=2,bty=\"n\")\n\n\n\n\n\n\n\n\nThese posterior distributions suggest that \\(\\theta_1\\) and \\(\\theta_2\\) are concentrated around different values.\nThe biostatistician’s goal is to study the log odds ratio \\[\\eta = \\log \\frac{\\frac{\\theta_1}{1-\\theta_1}}{\\frac{\\theta_2}{1-\\theta_2}}.\\] It is important to note that while we have obtained the posterior distributions of \\(\\theta_1\\) and \\(\\theta_2\\), we have not directly computed the posterior distribution of \\(\\eta\\).\n\nB &lt;- 100000\n\nset.seed(42)\ntheta1 &lt;- rbeta(B, a1.post, b1.post)\ntheta2 &lt;- rbeta(B, a2.post, b2.post)\n\neta &lt;- log(theta1/(1-theta1)*(1-theta2)/theta2)\n\n# Posterior Mean:\nmean(eta)\n\n[1] -2.735187\n\n\n\n# Posterior Variance:\nvar(eta)\n\n[1] 0.1222466\n\n\n\n# CS\nCS &lt;- quantile(eta, p=c(.025,.975)); CS\n\n     2.5%     97.5% \n-3.433526 -2.066745 \n\n\n\n# Histogram and kernel estimate:\n\nhist(eta, prob=T, xlab=expression(eta), main=\"Posterior distribution of log OR\")\nlines(density(eta), col=\"#D55E00\")\n\n\n\n\n\n\n\n\nUsing Monte Carlo sampling, the biostatistician estimates that, with 95% probability, the log odds ratio falls within the interval (-3.433526, -2.0667447).\nThis indicates that the odds of death in the “Vaccine” group are significantly lower than those in the “Placebo” group."
  },
  {
    "objectID": "Script4.html",
    "href": "Script4.html",
    "title": "Script 4 - MCMC - Metropolis Hasting",
    "section": "",
    "text": "Let’s suppose that the posterior is a Gamma(alpha.post, beta.post) and that we cannot compute the normalizing constant (i.e., we know only the kernel).\nWe choose a an Exponential distribution with mean equal to the previous value of the chain as the proposal distribution.\nrm(list = ls())\n\nB &lt;- 1000\nwarmup &lt;- 0.6\n\n# Prior hyperparameters:\na &lt;- 10\nb &lt;- 3\n# Generating data:\nset.seed(42)\nn &lt;- 15\nsum_yi &lt;- sum(rpois(n, 3))\nWe initialize the MCMC with an initial value:\ntheta &lt;- numeric(B)\ntheta.mean &lt;- numeric(B)\n\ntheta[1] &lt;- 1\ntheta.mean[1] &lt;- theta[1]\nNow, let’s implement the Metropolis Hasting algorithm. The main step here is to compute the probability \\(\\alpha\\left(\\theta^{(b)}, \\theta^*\\right)\\), which has the following expression:\n\\[\\begin{equation*}\n    \\alpha\\left(\\theta^{(b)}, \\theta^*\\right) = \\begin{cases}\n  \\min\\left(1, \\frac{\\pi(\\theta^*|\\textbf{y})q(\\theta^{(b)}|\\theta^*)}{\\pi(\\theta^{(b)}|\\textbf{y})q(\\theta^{*}|\\theta^{(b)})} \\right)  & \\text{ if } {\\pi(\\theta^{(b)}|\\textbf{y})q(\\theta^{*}|\\theta^{(b)})} \\neq 0 \\\\\n  1 & \\text{ otherwise}\n          \\end{cases}.\n\\end{equation*}\\]\nIn this example, we have \\[\\frac{\\pi(\\theta^*|\\textbf{y})q(\\theta^{(b)}|\\theta^*)}{\\pi(\\theta^{(b)}|\\textbf{y})q(\\theta^{*}|\\theta^{(b)})}\n= \\left(\\frac{\\theta^*}{\\theta^{(b)}}\\right)^{\\alpha + \\sum_{i=1}^n y_i -2} e^{-(\\beta+n)\\left(\\theta^* - \\theta^{(b)}\\right)} e^{-\\frac{\\theta^{(b)}}{\\theta^*} + \\frac{\\theta^*}{\\theta^{(b)}}}.\\]\n# Counter:\nk &lt;- 0\n\nfor(bb in 2:B){\n  theta_star &lt;- rexp(1, rate=1/theta[bb-1]) \n  \n  alpha_prob &lt;- \n    (theta_star/theta[bb-1])^(a+sum_yi-2)*\n    exp(-(b+n)*(theta_star-theta[bb-1]))*\n    exp(-theta[bb-1]/theta_star + theta_star/theta[bb-1])\n  \n  alpha_prob &lt;- min(alpha_prob, 1)\n  \n  # Accept?\n  if(runif(1) &lt;= alpha_prob){\n    theta[bb] &lt;- theta_star\n    k &lt;- k + 1\n  } else {\n    theta[bb] &lt;- theta[bb-1]\n  }\n  \n  theta.mean[bb] &lt;- mean(theta[2:bb])\n}\n# Acceptance rate:\nk/B\n\n[1] 0.131\nTraceplots:\npar(mfrow=c(1,2))\nplot(theta, type=\"l\", col=\"gray\")\npoints(theta.mean, type=\"l\")\nacf(theta)\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\nWe remove the warm-up period…\nm &lt;- warmup*B\ntheta.new &lt;- theta[m:B]\n… and compare the real and simulated posterior:\nhist(theta.new,prob=T,xlab=expression(theta))\ncurve(dgamma(x, a+sum_yi, rate=b+n),add=T,lwd=2)\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\nThe comparison suggests that the simulated posterior is not a good approximation of the true theoretical one. As an additional red-flag, we note that the acceptance rate is quite small. Thus, to obtain a reliable sample from the stationary distribution, we have to increase the length of the chain and include a thin period:\nB &lt;- 50000\nwarmup &lt;- 0.6\n\nk &lt;- 0\ntheta &lt;- numeric(B)\ntheta.mean &lt;- numeric(B)\n\ntheta[1] &lt;- 1\ntheta.mean[1] &lt;- theta[1]\n\nfor(bb in 2:B){\n  theta_star &lt;- rexp(1, rate=1/theta[bb-1]) \n  \n  alpha_prob &lt;- \n    (theta_star/theta[bb-1])^(a+sum_yi-2)*\n    exp(-(b+n)*(theta_star-theta[bb-1]))*\n    exp(-theta[bb-1]/theta_star + theta_star/theta[bb-1])\n  \n  alpha_prob &lt;- min(alpha_prob,1)\n  \n  if(runif(1) &lt;= alpha_prob){\n    theta[bb] &lt;- theta_star\n    k &lt;- k + 1\n  } else {\n    theta[bb] &lt;- theta[bb-1]\n  }\n  \n  theta.mean[bb] &lt;- mean(theta[2:bb])\n}\nAcceptance rate:\nk/B\n\n[1] 0.1421\npar(mfrow=c(1,2))\nplot(theta, type=\"l\", col=\"gray\")\npoints(theta.mean, type=\"l\")\nacf(theta)\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n# Removing the warm up period:\nm &lt;- round(warmup*B)\nthin &lt;- 10\ntheta.new &lt;- theta[seq(m, B, thin)]\n\n# Comparing the real and simulated posterior:\nhist(theta.new,prob=T,xlab=expression(theta))\ncurve(dgamma(x, a+sum_yi, rate=b+n),add=T,lwd=2)\n\n\n\n\n\n\n\npar(mfrow=c(1,1))"
  },
  {
    "objectID": "Script4.html#homework",
    "href": "Script4.html#homework",
    "title": "Script 4 - MCMC - Metropolis Hasting",
    "section": "HOMEWORK:",
    "text": "HOMEWORK:\nConsider the same scenario of the latter example. Implement a Metropolis-Hasting algorithm by considering an independent MH generating the candidate theta_star from an exponential distribution with mean \\(X\\). Consider different values for \\(X\\) to investigate whether it affects the acceptance rate."
  }
]