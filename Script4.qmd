---
title: "Script 4 - MCMC - Metropolis Hasting"
author: "Roberto Ascari"
format: html
editor: visual
toc: true
---

# Metropolis Hasting for a Gamma posterior

Let's suppose that the posterior is a Gamma(alpha.post, beta.post) and that we cannot compute the normalizing constant (i.e., we know only the kernel).

We choose a an Exponential distribution with mean equal to the previous value of the chain as the **proposal distribution**.

```{r}
#| echo: true
#| eval: true

rm(list = ls())

B <- 1000
warmup <- 0.6

# Prior hyperparameters:
a <- 10
b <- 3
```

```{r}
#| echo: true
#| eval: true

# Generating data:
set.seed(42)
n <- 15
sum_yi <- sum(rpois(n, 3))
```

We initialize the MCMC with an initial value:

```{r}
#| echo: true
#| eval: true

theta <- numeric(B)
theta.mean <- numeric(B)

theta[1] <- 1
theta.mean[1] <- theta[1]
```

Now, let's implement the **Metropolis Hasting algorithm**. The main step here is to compute the probability $\alpha\left(\theta^{(b)}, \theta^*\right)$, which has the following expression:

\begin{equation*}
    \alpha\left(\theta^{(b)}, \theta^*\right) = \begin{cases}
  \min\left(1, \frac{\pi(\theta^*|\textbf{y})q(\theta^{(b)}|\theta^*)}{\pi(\theta^{(b)}|\textbf{y})q(\theta^{*}|\theta^{(b)})} \right)  & \text{ if } {\pi(\theta^{(b)}|\textbf{y})q(\theta^{*}|\theta^{(b)})} \neq 0 \\
  1 & \text{ otherwise}
          \end{cases}.
\end{equation*}

In this example, we have $$\frac{\pi(\theta^*|\textbf{y})q(\theta^{(b)}|\theta^*)}{\pi(\theta^{(b)}|\textbf{y})q(\theta^{*}|\theta^{(b)})} 
= \left(\frac{\theta^*}{\theta^{(b)}}\right)^{\alpha + \sum_{i=1}^n y_i -2} e^{-(\beta+n)\left(\theta^* - \theta^{(b)}\right)} e^{-\frac{\theta^{(b)}}{\theta^*} + \frac{\theta^*}{\theta^{(b)}}}.$$

```{r}
#| echo: true
#| eval: true

# Counter:
k <- 0

for(bb in 2:B){
  theta_star <- rexp(1, rate=1/theta[bb-1]) 
  
  alpha_prob <- 
    (theta_star/theta[bb-1])^(a+sum_yi-2)*
    exp(-(b+n)*(theta_star-theta[bb-1]))*
    exp(-theta[bb-1]/theta_star + theta_star/theta[bb-1])
  
  alpha_prob <- min(alpha_prob, 1)
  
  # Accept?
  if(runif(1) <= alpha_prob){
    theta[bb] <- theta_star
    k <- k + 1
  } else {
    theta[bb] <- theta[bb-1]
  }
  
  theta.mean[bb] <- mean(theta[2:bb])
}
```

```{r}
#| echo: true
#| eval: true

# Acceptance rate:
k/B
```

Traceplots:

```{r}
#| echo: true
#| eval: true

par(mfrow=c(1,2))
plot(theta, type="l", col="gray")
points(theta.mean, type="l")
acf(theta)
par(mfrow=c(1,1))
```

We remove the warm-up period...

```{r}
#| echo: true
#| eval: true

m <- warmup*B
theta.new <- theta[m:B]
```

... and compare the real and simulated posterior:

```{r}
#| echo: true
#| eval: true


hist(theta.new,prob=T,xlab=expression(theta))
curve(dgamma(x, a+sum_yi, rate=b+n),add=T,lwd=2)
par(mfrow=c(1,1))
```

The comparison suggests that the simulated posterior is not a good approximation of the true theoretical one. As an additional red-flag, we note that the acceptance rate is quite small. Thus, to obtain a reliable sample from the stationary distribution, we have to increase the length of the chain and include a thin period:

```{r}
#| echo: true
#| eval: true

B <- 50000
warmup <- 0.6

k <- 0
theta <- numeric(B)
theta.mean <- numeric(B)

theta[1] <- 1
theta.mean[1] <- theta[1]

for(bb in 2:B){
  theta_star <- rexp(1, rate=1/theta[bb-1]) 
  
  alpha_prob <- 
    (theta_star/theta[bb-1])^(a+sum_yi-2)*
    exp(-(b+n)*(theta_star-theta[bb-1]))*
    exp(-theta[bb-1]/theta_star + theta_star/theta[bb-1])
  
  alpha_prob <- min(alpha_prob,1)
  
  if(runif(1) <= alpha_prob){
    theta[bb] <- theta_star
    k <- k + 1
  } else {
    theta[bb] <- theta[bb-1]
  }
  
  theta.mean[bb] <- mean(theta[2:bb])
}
```

Acceptance rate:

```{r}
#| echo: true
#| eval: true


k/B
```

```{r}
#| echo: true
#| eval: true

par(mfrow=c(1,2))
plot(theta, type="l", col="gray")
points(theta.mean, type="l")
acf(theta)
par(mfrow=c(1,1))
```

```{r}
#| echo: true
#| eval: true

# Removing the warm up period:
m <- round(warmup*B)
thin <- 10
theta.new <- theta[seq(m, B, thin)]

# Comparing the real and simulated posterior:
hist(theta.new,prob=T,xlab=expression(theta))
curve(dgamma(x, a+sum_yi, rate=b+n),add=T,lwd=2)
par(mfrow=c(1,1))
```

## HOMEWORK

Consider the same scenario of the latter example. Implement a Metropolis-Hasting algorithm by considering an independent MH generating the candidate `theta_star` from an exponential distribution with mean $X$. Consider different values for $X$ to investigate whether it affects the acceptance rate.

# Metropolis Hasting for a Logistic Regression model

In this Section, we consider the [cardiac dataset](data/cardiac.csv). Let's consider $Y_i \sim Bernoulli (\theta_i)$, where the probability of success depends on a quantitative covariate $X$ by means of the logit link function: $$logit(\theta_i) = \log\left(\frac{\theta_i}{1-\theta_i}\right) = \alpha + \beta x_i.$$

```{r}
#| echo: true
#| eval: true

rm(list=ls())

cardiac <- read.csv("data/cardiac.csv", header=T, sep=";")
str(cardiac)

y <- cardiac$Chd
x <- cardiac$Age
n <- nrow(cardiac)

plot(x, y, pch=20)
```

We fit the classical (frequentist!) logistic regression through the `glm()` function. This is going to be useful to set hyperparameters by following Robert & Casella.

```{r}
#| echo: true
#| eval: true

summary(glm(y ~ x, family="binomial"))
```

```{r}
#| echo: true
#| eval: true

B <- 10000
warmup <- 0.6
```

For selecting the hyperparameters, we follow Robert & Casella:

```{r}
#| echo: true
#| eval: true

b <- exp(-5.30945+.577216)

# From GLM estimates:
m_norm <- 0.1109
v_norm <- .02406^2

```

```{r}
#| echo: true
#| eval: true

k <- 0
alpha <- numeric(B)
beta <- numeric(B)

alpha.mean <- numeric(B)
beta.mean <- numeric(B)

set.seed(42)
alpha[1] <- 0 #log(rexp(1, rate = 1/b))
alpha.mean[1] <- alpha[1]

beta[1] <- rnorm(1, m_norm, sqrt(v_norm))
beta.mean[1] <- beta[1]
```

The likelihood function is defined as $$L(\textbf{y} | \alpha, \beta) = \prod_{i=1}^n \left(\frac{\exp(\alpha+\beta x_i)}{1+\exp(\alpha+\beta x_i)}\right)^{y_i} \left(\frac{1}{1+\exp(\alpha+\beta x_i)}\right)^{1-y_i}.$$

```{r}
#| echo: true
#| eval: true

Likelihood <- function(alpha, beta, y, x){
  eta <- alpha + beta*x
  theta <- (exp(eta)/(1+exp(eta)))
  L <- prod((theta^y)*((1-theta)^(1-y)))
  return(L)
}
```

Metropolis Hasting:

```{r}
#| echo: true
#| eval: true

set.seed(42)
for(bb in 2:B){
  alpha_star <- log(rexp(1, rate = 1/b))
  beta_star <- rnorm(1, m_norm, sqrt(v_norm))
  
  num <- Likelihood(alpha_star, beta_star, y, x)*dnorm(beta[bb-1], m_norm, sqrt(v_norm))
  den <- Likelihood(alpha[bb-1], beta[bb-1], y, x)*dnorm(beta_star, m_norm, sqrt(v_norm))
  
  alpha_prob <- min(num/den, 1)
  
  if(runif(1) <= alpha_prob){
    alpha[bb] <- alpha_star
    beta[bb] <- beta_star
    
    k <- k + 1
  } else {
    alpha[bb] <- alpha[bb-1]
    beta[bb] <- beta[bb-1]
  }
  
  alpha.mean[bb] <- mean(alpha[2:bb])
  beta.mean[bb] <- mean(beta[2:bb])
}
```

Acceptance rate:

```{r}
#| echo: true
#| eval: true

# Acceptance rate:
k/B
```

```{r}
#| echo: true
#| eval: true

par(mfrow=c(1,2))
plot(alpha, type="l", col="gray")
points(alpha.mean, type="l")
plot(beta, type="l", col="gray")
points(beta.mean, type="l")
par(mfrow=c(1,1))

par(mfrow=c(1,2))
acf(alpha)
acf(beta)
par(mfrow=c(1,1))
```

Removing the warm-up period:

```{r}
#| echo: true
#| eval: true

m <- round(warmup*B)
thin <- 15
alpha.new <- alpha[seq(m, B, thin)]
beta.new <- beta[seq(m, B, thin)]
```

```{r}
#| echo: true
#| eval: true

mean(alpha)
mean(beta)
```

```{r}
#| echo: true
#| eval: true

quantile(beta, probs = c(.025, .975))
```

```{r}
#| echo: true
#| eval: true

par(mfrow=c(1,2))
hist(alpha.new,prob=T,xlab=expression(theta))
lines(density(alpha.new),lwd=2)

hist(beta.new,prob=T,xlab=expression(theta))
lines(density(beta.new),lwd=2)
par(mfrow=c(1,1))
```

```{r}
#| echo: true
#| eval: true

range(x)
theta_i <- matrix(NA, ncol=length(unique(x)), nrow=length(alpha.new))

for(i in 1:length(unique(x))){
  eta <- alpha.new + beta.new*unique(x)[i]
  theta_i[,i] <- exp(eta)/(1+exp(eta))
}

head(theta_i)
```

```{r}
#| echo: true
#| eval: true

# Empty plot:
plot(0, 1, xlab = "Age", ylab = "P(Y = 1 | Age)",
     ylim=c(0,1), xlim=range(x), col="white")

for(i in 1:length(unique(x))){
  xx <- unique(x)[i]
  points(rep(xx, length(theta_i[,i])), 
         theta_i[,i], col="gray", pch=20)
}

points(x, y, pch=20)
abline(h = .5, col = "#D55E00", lty = "dashed")

x_grid <- seq(min(x)-1, max(x)+1, by = .5)
theta_grid <- numeric(length(x_grid))

for(i in 1:length(x_grid)){
  eta <- alpha.new + beta.new*x_grid[i]
  theta_grid[i] <- mean(exp(eta)/(1+exp(eta)))
}

points(x_grid, theta_grid, type = "l", lwd = 1.5)
```
